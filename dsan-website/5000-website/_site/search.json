[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nLithium, Sustainability and Batteries:  A new Era in Resource Management\n",
    "section": "",
    "text": "M.S. in Data Science and Analytics\n\n\n\nData Science & Analytics\n\n\nFinal Project\n\n\n\nLithium, Sustainability and Batteries:  A new Era in Resource Management\n\n\n\nBy: Maria Agustina Zuckerberg\n\n\nGU Net ID: maz53"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Build out your website tab for exploratory data analysis"
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]"
  },
  {
    "objectID": "eda/eda.html#basic-visualization",
    "href": "eda/eda.html#basic-visualization",
    "title": "Data Exploration",
    "section": "Basic visualization",
    "text": "Basic visualization\n\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nplt.show()"
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Build out your website tab for “clustering”"
  },
  {
    "objectID": "index.html#m.s.-in-data-science-and-analytics",
    "href": "index.html#m.s.-in-data-science-and-analytics",
    "title": "Data Science & Analytics",
    "section": "M.S. in Data Science and Analytics",
    "text": "M.S. in Data Science and Analytics"
  },
  {
    "objectID": "1_introduction.html",
    "href": "1_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "::: {.justify} Credit: Paz Olivares Droguett for NPR"
  },
  {
    "objectID": "2_data_gathering.html#global-lithium-production",
    "href": "2_data_gathering.html#global-lithium-production",
    "title": "Data Gathering",
    "section": "Global Lithium Production",
    "text": "Global Lithium Production\n\n\n\n\nThe Global Lithium Production dataset was extracted from the Our World in Data website. The Energy Institute Statistical Review of World Energy analyzes data on world energy markets from the previous year, and Our World in Data consolidates data originally derived from the reports produced. The dataset provides information on global lithium production, including data for major producing countries and aggregations at the regional, continental, and global levels. Lithium production is measured in metric tons. The dataset covers the period from 1995 to 2022.\nWhile the dataset includes global data, this analysis will focus specifically on the evolution of lithium production, with a strong emphasis on the U.S. This approach allows for a more in-depth analysis of the growth and fluctuations of this resource, while assessing the impact on the economy.\nSource: Lithium Production"
  },
  {
    "objectID": "2_data_gathering.html#chinese-yuan-renminbi-to-u.s.-dollar-spot-exchange-rate",
    "href": "2_data_gathering.html#chinese-yuan-renminbi-to-u.s.-dollar-spot-exchange-rate",
    "title": "Data Gathering",
    "section": "Chinese Yuan Renminbi to U.S. Dollar Spot Exchange Rate",
    "text": "Chinese Yuan Renminbi to U.S. Dollar Spot Exchange Rate\n\nQUANTMOD API | R\nConsidering that the price of lithium is calculated in Yuan currency, and the analysis will be done in US dollars, the Quantmod API is used to obtain the daily exchange rates since 2020."
  },
  {
    "objectID": "2_data_gathering.html#lithium-news",
    "href": "2_data_gathering.html#lithium-news",
    "title": "Data Gathering",
    "section": "Lithium News",
    "text": "Lithium News\n\n\n\nThe Lithium News dataset is an essential resource for the project to understand the media perspective on the evolution of the lithium market. The News API allows the collection of text data from news articles related to lithium. This platform aggregates information from a variety of media sources. Given the focus of the project, the text data collection is primarily focused on U.S. media resources that have published articles in English. This approach ensures that the dataset provides relevant insights to address the project’s questions.\nThe date range for the API is variable, depending on the amount of data available at the time the scraping is done. To optimize data collection and maximize the date range for article collection, a special function has been developed to adjust the number of days. This approach allows for the most current and broadest data collection.\nSource: News API | Python"
  },
  {
    "objectID": "2_data_gathering.html#lithium-news---sentiment-analysis",
    "href": "2_data_gathering.html#lithium-news---sentiment-analysis",
    "title": "Data Gathering",
    "section": "Lithium News - Sentiment Analysis",
    "text": "Lithium News - Sentiment Analysis\n\n\n\nThe Sentiment Analysis Dataset for the Lithium News is a crucial step in the analytical process of the project. It enriches the Lithium News dataset collected in the previous section. To improve our understanding of the lithium market perspective, we perform a sentiment analysis on the content of the news articles.\nTo do this, we will use the IBM Watson API, a tool designed to perform sentiment analysis on text data. The API allows us to process and analyze the text data and provide insights into the sentiment and tone of the articles.\nThe main objective of this analysis is to understand how the perspective on the lithium market has evolved. By evaluating the results obtained on the media news, we aim to determine whether the news articles have shown a bias towards a more positive or negative perspective. This allows us to identify potential shifts in sentiment that could impact the industry in the future.\nSource: IBM Watson API | Python"
  },
  {
    "objectID": "3_data_cleaning.html",
    "href": "3_data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Below are the libraries used in this section:\nR Libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(gridExtra)\nlibrary(readxl)\nlibrary(imputeTS)\nlibrary(zoo)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(vars)\nlibrary(dplyr)\nPython Libraries\nfrom datetime import date\nfrom datetime import date, timedelta\nfrom newsapi.newsapi_client import NewsApiClient\nimport pandas as pd\n\nimport json\nfrom ibm_watson import NaturalLanguageUnderstandingV1\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\nfrom ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions\n\nimport os\nFunction stocks_cleaning()\nstocks_cleaning &lt;- function(data_frame){\n\n    symbol &lt;- max(data_frame$symbol)\n\n    # Select relevant columns\n    data_frame &lt;- data.frame(data_frame$date, data_frame$adjusted)\n\n    # Rename columsn\n    names(data_frame) &lt;- c(\"date\", \"adjusted\")\n\n    # Create a sequence of dates from start_date to end_date\n    start_date &lt;- as.Date(min(data_frame$date))  \n    end_date &lt;- as.Date(max(data_frame$date))\n\n    # Create data range\n    date_range &lt;- seq(start_date, end_date, by = \"1 day\")\n\n    # Create a dataset with the date range\n    date_dataset &lt;- data.frame(Date = date_range)\n\n    # Merge dataframes\n    data_frame &lt;- merge(data_frame, date_dataset, by.x = \"date\", by.y = \"Date\", all = TRUE)\n\n    # Extract rows with missing values\n    df_na_rows &lt;- data_frame[which(rowSums(is.na(data_frame)) &gt; 0),]\n\n    # Extract columns with missing values\n    df_na_cols &lt;- data_frame[, which(colSums(is.na(data_frame)) &gt; 0)]\n\n    # Modify data\n    imputed_time_series &lt;- na_ma(data_frame, k = 4, weighting = \"exponential\")\n\n    # Add modified data\n    data_frame &lt;- data.frame(imputed_time_series)\n\n    # Change data type\n    data_frame$date &lt;- as.Date(data_frame$date,format = \"%m/%d/%y\")\n\n    names(data_frame) &lt;- c(\"date\", symbol)\n\n    return(data_frame)\n}\nFunction merge_dataframes()\nmerge_dataframes &lt;- function(dataframes_list) {\n  # Perform inner join using the first data frame as the base\n  df_complete &lt;- dataframes_list[[1]]\n  \n  # Iterate over the remaining data frames and merge\n  for (i in 2:length(dataframes_list)) {\n    df_complete &lt;- merge(df_complete, dataframes_list[[i]], by = \"date\", all.x = TRUE)\n  }\n  \n  return(df_complete)\n}\nFunction merge_dataframes_2()\nmerge_dataframes_2 &lt;- function(dataframes_list) {\n  # Perform inner join using the first data frame as the base\n  df_complete &lt;- dataframes_list[[1]]\n  \n  # Iterate over the remaining data frames and merge\n  for (i in 2:length(dataframes_list)) {\n    df_complete &lt;- merge(df_complete, dataframes_list[[i]], by = \"Quarter Ended\", all.x = TRUE)\n  }\n  \n  return(df_complete)\n}"
  },
  {
    "objectID": "2_data_gathering.html",
    "href": "2_data_gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Source\n\nThe Global Lithium Production dataset was extracted from the Our World in Data website. The Energy Institute Statistical Review of World Energy analyzes data on world energy markets from the previous year, and Our World in Data consolidates data originally derived from the reports produced. The dataset provides information on global lithium production, including data for major producing countries and aggregations at the regional, continental, and global levels. Lithium production is measured in metric tons. The dataset covers the period from 1995 to 2022.\nWhile the dataset includes global data, this analysis will focus specifically on the evolution of lithium production, with a strong emphasis on the U.S. This approach allows for a more in-depth analysis of the growth and fluctuations of this resource, while assessing the impact on the economy.\n\n\n\n\n\n\nLithium Companies\nSource\n\nDescription…\n\n\n\n\n\nResources Prices\nSource\n\nDescription… \n\n\n\n\n\n\n\n\nLithium News\nSource:  News API | Python\nThe Lithium News dataset is an essential resource for the project to understand the media perspective on the evolution of the lithium market. The News API allows the collection of text data from news articles related to lithium. This platform aggregates information from a variety of media sources. Given the focus of the project, the text data collection is primarily focused on U.S. media resources that have published articles in English. This approach ensures that the dataset provides relevant insights to address the project’s questions.\nThe date range for the API is variable, depending on the amount of data available at the time the scraping is done. To optimize data collection and maximize the date range for article collection, a special function has been developed to adjust the number of days. This approach allows for the most current and broadest data collection.\n\n\n\n\n\n\n\nLithium News - Sentiment Analysis\nSource:  IBM Watson API | Python\nThe Sentiment Analysis Dataset for the Lithium News is a crucial step in the analytical process of the project. It enriches the Lithium News dataset collected in the previous section. To improve our understanding of the lithium market perspective, we perform a sentiment analysis on the content of the news articles.\nTo do this, we will use the IBM Watson API, a tool designed to perform sentiment analysis on text data. The API allows us to process and analyze the text data and provide insights into the sentiment and tone of the articles.\nThe main objective of this analysis is to understand how the perspective on the lithium market has evolved. By evaluating the results obtained on the media news, we aim to determine whether the news articles have shown a bias towards a more positive or negative perspective. This allows us to identify potential shifts in sentiment that could impact the industry in the future."
  },
  {
    "objectID": "3_data_cleaning Python.html",
    "href": "3_data_cleaning Python.html",
    "title": "Data Gathering Python",
    "section": "",
    "text": "from datetime import date\nfrom datetime import date, timedelta"
  },
  {
    "objectID": "3_data_cleaning Python.html#lithium-news",
    "href": "3_data_cleaning Python.html#lithium-news",
    "title": "Data Gathering Python",
    "section": "Lithium News",
    "text": "Lithium News\n\nfrom newsapi.newsapi_client import NewsApiClient\nimport pandas as pd\n\ndate = date.today()\n\nprint(date)\n\ndate_past = date.today() - timedelta(days=15)\n\nprint(date_past)\n\nf = open('auth.k','r', encoding=\"utf-8\")\nak = f.readlines()\nf.close()\n\n#newsapi = NewsApiClient(api_key=ak[0])\n\nnewsapi = NewsApiClient(api_key='6c4d77d5f3ab45749cf47b85c1032fe6')\n\nsources = newsapi.get_sources()\n\nsources = pd.DataFrame(sources['sources'])\n\nsources = sources[(sources['language'] == 'en') & (sources['country'] == 'us') & ~sources['category'].isin(['sports', 'entertainment', 'health'])]\n\n\ndf_sources = ', '.join(sources['id'].astype(str))\n\ndf_domains = ', '.join(sources['url'].astype(str))\n\nall_articles = newsapi.get_everything(q='lithium',\n                                      sources=str(df_sources),\n                                      domains=str(df_domains),\n                                      from_param=date_past,\n                                      to=date,\n                                      language='en',\n                                      sort_by='relevancy')\n\ndf_articles = pd.DataFrame(all_articles['articles'])\n\ndf_articles\n\n2023-11-02\n2023-10-18\n\n\n\n\n\n\n\n\n\nsource\nauthor\ntitle\ndescription\nurl\nurlToImage\npublishedAt\ncontent\n\n\n\n\n0\n{'id': 'wired', 'name': 'Wired'}\nGrace Browne, Matt Reynolds\nHere’s the Truth Behind the Biggest (and Dumbe...\nYes, charging your phone overnight is bad for ...\nhttps://www.wired.com/story/how-to-improve-bat...\nhttps://media.wired.com/photos/653b8f898ed7be7...\n2023-10-27T11:00:00Z\nIn lithium-ion batteries, thats no longer the ...\n\n\n1\n{'id': 'the-verge', 'name': 'The Verge'}\nAndrew J. Hawkins\nFord hits the brakes on $12 billion in EV spen...\nFord is pausing about $12 billion in spending ...\nhttps://www.theverge.com/2023/10/26/23934172/f...\nhttps://cdn.vox-cdn.com/thumbor/LpKbkjEO0XFAxX...\n2023-10-27T00:16:44Z\nFord hits the brakes on $12 billion in EV spen...\n\n\n2\n{'id': 'ars-technica', 'name': 'Ars Technica'}\nJonathan M. Gitlin\nA giant battery gives this new school bus a 30...\nThe Type-D school bus uses a 387 kWh lithium i...\nhttps://arstechnica.com/cars/2023/10/this-elec...\nhttps://cdn.arstechnica.net/wp-content/uploads...\n2023-10-31T11:00:53Z\nEnlarge/ GreenPower has given its class-D elec...\n\n\n3\n{'id': 'business-insider', 'name': 'Business I...\nNathan Rennolds\nChina's led the EV race – but it may be runnin...\nThe world has woken up to China's control over...\nhttps://www.businessinsider.com/electric-cars-...\nhttps://i.insider.com/652eba9496f7540cd05e9155...\n2023-10-22T09:43:12Z\nWuling Hongguang Mini EVs on display at the Sh...\n\n\n4\n{'id': 'the-next-web', 'name': 'The Next Web'}\nIoanna Lykiardopoulou\nStartup bags €8.5M to bolster Europe’s EV batt...\nIn a big boost to sustainable mobility, 130 mi...\nhttps://thenextweb.com/news/startup-to-bolster...\nhttps://img-cdn.tnwcdn.com/image/tnw-blurple?f...\n2023-10-24T04:03:53Z\nIn a big boost to sustainable mobility, 130 mi...\n\n\n5\n{'id': 'the-next-web', 'name': 'The Next Web'}\nSiôn Geschwindt\nNorway’s AutoStore unveils next-gen electric w...\nNorwegian tech company AutoStore today unveile...\nhttps://thenextweb.com/news/norway-autostore-u...\nhttps://img-cdn.tnwcdn.com/image/tnw-blurple?f...\n2023-10-23T04:00:52Z\nNorwegian tech company AutoStore today unveile...\n\n\n6\n{'id': 'business-insider', 'name': 'Business I...\nSonam Sheth,Jessica Orwig\nHumans have launched so much to space that it'...\nResearchers traced the metals back to rockets ...\nhttps://www.businessinsider.com/rocket-satelli...\nhttps://i.insider.com/6530534796f7540cd05fff1c...\n2023-10-18T23:13:29Z\nWhen something enters Earth's atmosphere, it's...\n\n\n7\n{'id': 'engadget', 'name': 'Engadget'}\nAndrew Tarantola\nNASA's John Mather keeps redefining our unders...\nSpace isn't hard only on account of the rocket...\nhttps://www.engadget.com/inside-the-star-facto...\nhttps://s.yimg.com/ny/api/res/1.2/pcPe4lUVbQVQ...\n2023-10-22T14:30:46Z\nSpace isn't hard only on account of the rocket...\n\n\n8\n{'id': 'business-insider', 'name': 'Business I...\nJenny McGrath\nWhy we don't have fusion power plants yet, and...\nTrying to create a fusion reaction on Earth is...\nhttps://www.businessinsider.com/why-no-fusion-...\nhttps://i.insider.com/6531858096f7540cd060d9a2...\n2023-10-29T12:19:01Z\nThe National Spherical Torus Experiment-Upgrad...\n\n\n9\n{'id': 'time', 'name': 'Time'}\nEliana Dockterman\nBritney Spears Feared Her Family Would Kill He...\nThe pop star writes in her new memoir, The Wom...\nhttps://time.com/6326001/britney-spears-book-f...\nhttps://api.time.com/wp-content/uploads/2023/1...\n2023-10-19T20:32:08Z\nIn her new book, The Woman in Me, Britney Spea...\n\n\n10\n{'id': 'time', 'name': 'Time'}\nMoises Mendez II\n‘She Should Have Taken My Side.’ Britney Spear...\nIn 'The Woman in Me,' Britney Spears writes ca...\nhttps://time.com/6326006/britney-spears-jamie-...\nhttps://api.time.com/wp-content/uploads/2023/1...\n2023-10-19T21:13:59Z\nThe rocky relationship between Britney Spears ...\n\n\n11\n{'id': 'cbs-news', 'name': 'CBS News'}\nKate Gibson\nFeds warn against using Toos electric scooters...\nThe U.S. Consumer Product Safety Commission sa...\nhttps://www.cbsnews.com/news/product-recall-el...\nhttps://assets2.cbsnewsstatic.com/hub/i/r/2023...\n2023-10-23T19:46:00Z\nRiders of Toos Elite 60-volt electric scooters...\n\n\n12\n{'id': 'next-big-future', 'name': 'Next Big Fu...\nBrian Wang\nBetter EV Batteries and a Glut of Batteries Sh...\nCATL’s new Shenxing ultra-fast charging iron L...\nhttps://www.nextbigfuture.com/2023/10/better-e...\nhttps://nextbigfuture.s3.amazonaws.com/uploads...\n2023-10-25T15:47:05Z\nBrian Wang is a Futurist Thought Leader and a ...\n\n\n13\n{'id': 'new-scientist', 'name': 'New Scientist'}\nJeremy Hsu\nCheap salty solution cools computers and boost...\nWater containing a cheap lithium bromide salt ...\nhttps://www.newscientist.com/article/2400457-c...\nhttps://images.newscientist.com/wp-content/upl...\n2023-10-31T15:00:23Z\nPassive cooling could be more efficient using ...\n\n\n14\n{'id': 'fortune', 'name': 'Fortune'}\nMatthew Perrone, The Associated Press\nE-cigarettes pose a new environmental dilemma—...\nCommunities are experimenting with new ways to...\nhttps://fortune.com/2023/10/19/e-cigarettes-po...\nhttps://content.fortune.com/wp-content/uploads...\n2023-10-19T14:46:55Z\nWith the growing popularity of disposable e-ci...\n\n\n15\n{'id': 'fortune', 'name': 'Fortune'}\nHannah Schoenbaum, The Associated Press\nToyota literally doubles down on EVs in North ...\nToyota's first U.S. automotive battery plant, ...\nhttps://fortune.com/2023/10/31/toyota-electric...\nhttps://content.fortune.com/wp-content/uploads...\n2023-10-31T20:37:05Z\nToyota will invest an additional $8 billion in...\n\n\n16\n{'id': 'new-scientist', 'name': 'New Scientist'}\nMatthew Sparkes\nWhat are solid-state batteries and why do we n...\nBatteries containing solid electrolytes have m...\nhttps://www.newscientist.com/article/2398896-w...\nhttps://images.newscientist.com/wp-content/upl...\n2023-10-24T11:54:42Z\nSolid-state batteries could be lighter and mor...\n\n\n17\n{'id': 'the-verge', 'name': 'The Verge'}\nAndrew J. Hawkins\nRedwood Materials is recycling its first stati...\nRedwood Materials will decommission and recycl...\nhttps://www.theverge.com/2023/11/2/23943267/re...\nhttps://cdn.vox-cdn.com/thumbor/UUXd9_9UhChk9E...\n2023-11-02T13:00:00Z\nRedwood Materials is recycling its first stati...\n\n\n18\n{'id': 'newsweek', 'name': 'Newsweek'}\nThomas Kika\n'Urgent' Warning Issued for Electric Scooter A...\nA battery malfunction in one of the scooters w...\nhttps://www.newsweek.com/urgent-warning-issued...\nhttps://d.newsweek.com/en/full/2298354/electri...\n2023-10-23T20:49:19Z\nU.S. consumers are being strongly warned again...\n\n\n19\n{'id': 'abc-news', 'name': 'ABC News'}\nHANNAH SCHOENBAUM /REPORT FOR AMERICA Associat...\nToyota more than doubles investment and job cr...\nToyota will invest an additional $8 billion in...\nhttps://abcnews.go.com/Technology/wireStory/to...\nhttps://i.abcnewsfe.com/a/5ace85e3-724d-46db-8...\n2023-10-31T13:42:59Z\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n20\n{'id': 'fortune', 'name': 'Fortune'}\nJames K. Glassman\nHow the West can help Ukraine win its economic...\n'Right now, Ukraine’s allies can help most by ...\nhttps://fortune.com/2023/10/31/west-ukraine-ec...\nhttps://content.fortune.com/wp-content/uploads...\n2023-10-31T11:15:22Z\nSince Russian President Vladimir Putin invaded...\n\n\n21\n{'id': 'breitbart-news', 'name': 'Breitbart Ne...\nJohn Hayward, John Hayward\nTaliban to Formally Join China’s Belt and Road...\nThe Taliban plans to formally join China’s Bel...\nhttps://www.breitbart.com/asia/2023/10/19/tali...\nhttps://media.breitbart.com/media/2023/10/Tali...\n2023-10-19T15:47:29Z\nHaji Nooruddin Azizi, the acting commerce mini...\n\n\n22\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nJapanese automaker Toyota's profits zoom on ch...\nToyota’s profit in the latest quarter jumped n...\nhttps://abcnews.go.com/International/wireStory...\nhttps://i.abcnewsfe.com/a/0b9f4a7b-034e-4653-9...\n2023-11-01T06:22:34Z\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n23\n{'id': 'abc-news', 'name': 'ABC News'}\nMATTHEW PERRONE AP health writer\nCommunities can't recycle or trash disposable ...\nCommunities across the U.S. are confronting a ...\nhttps://abcnews.go.com/Health/wireStory/commun...\nhttps://i.abcnewsfe.com/a/1e51aec8-b5f5-4031-b...\n2023-10-19T12:22:30Z\nWASHINGTON -- With the growing popularity of d...\n\n\n24\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nG-7 nations back strong supply chains for ener...\nTrade and economy officials from the Group of ...\nhttps://abcnews.go.com/International/wireStory...\nhttps://i.abcnewsfe.com/a/1ad16d9a-8471-4838-9...\n2023-10-29T08:06:27Z\nTOKYO -- Trade and economy officials from the ...\n\n\n25\n{'id': 'new-scientist', 'name': 'New Scientist'}\nJames Dinneen\nDesert plant collects water from air by excret...\nAn evergreen desert shrub common in the Middle...\nhttps://www.newscientist.com/article/2400273-d...\nhttps://images.newscientist.com/wp-content/upl...\n2023-10-30T19:00:25Z\nThe Athel tamarisk has ingenious ways of survi...\n\n\n26\n{'id': 'newsweek', 'name': 'Newsweek'}\nMeghan Gunn\nFive Tech Innovations That Could Help Save the...\nFrom a paint that cools buildings to a new car...\nhttps://www.newsweek.com/five-tech-innovations...\nhttps://d.newsweek.com/en/full/2294547/dr-xiul...\n2023-10-30T16:13:19Z\nAround the world, entrepreneurs are trying to ...\n\n\n27\n{'id': 'abc-news', 'name': 'ABC News'}\nHANNAH SCHOENBAUM /REPORT FOR AMERICA Associat...\nToyota more than doubles investment and job cr...\nToyota will invest an additional $8 billion in...\nhttps://abcnews.go.com/Business/wireStory/toyo...\nhttps://i.abcnewsfe.com/a/5ace85e3-724d-46db-8...\n2023-10-31T13:42:54Z\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n28\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nJapan's automakers unveil EVs galore at Tokyo ...\nToyota, Honda, Nissan and other Japanese autom...\nhttps://abcnews.go.com/Technology/wireStory/ja...\nhttps://i.abcnewsfe.com/a/946cd347-6904-4e14-9...\n2023-10-25T03:51:22Z\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n29\n{'id': 'abc-news', 'name': 'ABC News'}\nMATTHEW PERRONE AP health writer\nCommunities can't recycle or trash disposable ...\nCommunities across the U.S. are confronting a ...\nhttps://abcnews.go.com/Business/wireStory/comm...\nhttps://i.abcnewsfe.com/a/1e51aec8-b5f5-4031-b...\n2023-10-19T21:26:36Z\nWASHINGTON -- With the growing popularity of d...\n\n\n30\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nJapan's automakers unveil EVs galore at Tokyo ...\nToyota, Honda, Nissan and other Japanese autom...\nhttps://abcnews.go.com/International/wireStory...\nhttps://i.abcnewsfe.com/a/946cd347-6904-4e14-9...\n2023-10-25T03:50:32Z\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n31\n{'id': 'abc-news', 'name': 'ABC News'}\nFATIMA HUSSEIN Associated Press\nYellen calls for more US-Latin America trade, ...\nTreasury Secretary Janet Yellen wants Latin Am...\nhttps://abcnews.go.com/US/wireStory/treasury-s...\nhttps://i.abcnewsfe.com/a/7d503fb7-17af-4773-b...\n2023-11-02T12:22:28Z\nWASHINGTON -- Treasury Secretary Janet Yellen ...\n\n\n32\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nJapanese automaker Toyota's profits zoom on ch...\nToyota’s profit in the latest quarter jumped n...\nhttps://abcnews.go.com/Business/wireStory/japa...\nhttps://i.abcnewsfe.com/a/0b9f4a7b-034e-4653-9...\n2023-11-01T06:22:38Z\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n33\n{'id': 'abc-news', 'name': 'ABC News'}\nFATIMA HUSSEIN Associated Press\nTreasury Secretary Yellen calls for more US-La...\nTreasury Secretary Janet Yellen wants Latin Am...\nhttps://abcnews.go.com/Business/wireStory/trea...\nhttps://i.abcnewsfe.com/a/7d503fb7-17af-4773-b...\n2023-11-02T12:22:34Z\nWASHINGTON -- Treasury Secretary Janet Yellen ...\n\n\n34\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nG-7 nations back strong supply chains for ener...\nTrade and economy officials from the Group of ...\nhttps://abcnews.go.com/Business/wireStory/7-na...\nhttps://i.abcnewsfe.com/a/1ad16d9a-8471-4838-9...\n2023-10-29T08:22:22Z\nTOKYO -- Trade and economy officials from the ...\n\n\n35\n{'id': 'time', 'name': 'Time'}\nEliana Dockterman\nBritney Spears Is Suspended Between Girlhood a...\nIn her highly anticipated memoir, the pop star...\nhttps://time.com/6326344/britney-spears-the-wo...\nhttps://api.time.com/wp-content/uploads/2023/1...\n2023-10-20T21:39:30Z\nBritney Spears knows what it means to be depri...\n\n\n36\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nJapan's automakers unveil EVs galore at Tokyo ...\nToyota, Honda, Nissan and other Japanese autom...\nhttps://abcnews.go.com/Business/wireStory/japa...\nhttps://i.abcnewsfe.com/a/946cd347-6904-4e14-9...\n2023-10-25T03:50:36Z\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n37\n{'id': 'ars-technica', 'name': 'Ars Technica'}\nJonathan M. Gitlin\nHonda says making cheap electric vehicles is t...\nThe platform was to use GM's Ultium batteries.\nhttps://arstechnica.com/cars/2023/10/honda-can...\nhttps://cdn.arstechnica.net/wp-content/uploads...\n2023-10-25T15:25:55Z\nEnlarge/ A GM Ultium battery pack. \\r\\n75 with...\n\n\n38\n{'id': 'breitbart-news', 'name': 'Breitbart Ne...\nJohn Binder, John Binder\nGM Backtracks on Joe Biden's Green Energy Agen...\nAfter investing billions to adhere to Presiden...\nhttps://www.breitbart.com/economy/2023/10/31/g...\nhttps://media.breitbart.com/media/2023/10/Gett...\n2023-10-31T18:55:02Z\nAfter investing billions to adhere to Presiden...\n\n\n39\n{'id': 'breitbart-news', 'name': 'Breitbart Ne...\nJohn Binder, John Binder\nUAW Strike: 6.8K Auto Workers Shut Down Stella...\nNearly 7,000 auto workers at Stellantis' Sterl...\nhttps://www.breitbart.com/politics/2023/10/23/...\nhttps://media.breitbart.com/media/2023/10/Gett...\n2023-10-23T22:15:07Z\nNearly 7,000 auto workers at Stellantis’ Sterl...\n\n\n40\n{'id': 'fox-news', 'name': 'Fox News'}\nAssociated Press\nUS struggles to discard hazardous materials fr...\nE-cigarettes are considered hazardous waste th...\nhttps://www.foxnews.com/us/us-struggles-discar...\nhttps://static.foxnews.com/foxnews.com/content...\n2023-10-19T17:55:51Z\nWith the growing popularity of disposable e-ci...\n\n\n41\n{'id': 'techradar', 'name': 'TechRadar'}\nspace2occupy@gmail.com (James Holland)\nYeedi Cube robot vacuum and mop review: lots o...\nWith its 2-in-1 vacuuming and mopping cleaning...\nhttps://www.techradar.com/home/robot-vacuums/y...\nhttps://cdn.mos.cms.futurecdn.net/gnPf25Xz3Ui3...\n2023-10-30T13:00:29Z\nYeedi Cube: One-minute review\\r\\nThe Yeedi Cub...\n\n\n42\n{'id': 'nbc-news', 'name': 'NBC News'}\nChloe Melas\nBritney Spears reflects on her 'traumatizing' ...\nIn her upcoming memoir, Britney Spears is shed...\nhttps://www.nbcnews.com/pop-culture/pop-cultur...\nhttps://media-cldnry.s-nbcnews.com/image/uploa...\n2023-10-20T04:06:21Z\nBritney Spears is shedding new light onto what...\n\n\n43\n{'id': 'fox-news', 'name': 'Fox News'}\nPeter Aitken\nBiden hosts anti-Israel world leader to talk c...\nChile, Bolivia and Colombia all recalled their...\nhttps://www.foxnews.com/world/biden-hosts-anti...\nhttps://static.foxnews.com/foxnews.com/content...\n2023-11-02T08:00:19Z\nPresident Biden will meet with Chilean Preside...\n\n\n44\n{'id': 'breitbart-news', 'name': 'Breitbart Ne...\nJohn Binder, John Binder\nJoe Biden's Green Energy Flop: Automakers Real...\nAmerican automakers are quickly learning that ...\nhttps://www.breitbart.com/economy/2023/10/27/j...\nhttps://media.breitbart.com/media/2023/10/Gett...\n2023-10-27T22:28:20Z\nAfter investing billions into a green energy a...\n\n\n45\n{'id': 'fox-news', 'name': 'Fox News'}\nKristen Walker\nThe latest attempt to take away your gas-power...\nAnother government agency has proposed a rule ...\nhttps://www.foxnews.com/opinion/latest-attempt...\nhttps://static.foxnews.com/foxnews.com/content...\n2023-10-18T18:00:10Z\nIn yet another attempt to regulate the car mar...\n\n\n46\n{'id': 'engadget', 'name': 'Engadget'}\nSteve Dent\nThis 90-passenger school bus has nearly 300 mi...\nGreenPower has launched the Mega Beast electri...\nhttps://www.engadget.com/this-90-passenger-sch...\nhttps://s.yimg.com/ny/api/res/1.2/MZj6YDQcHvJg...\n2023-11-01T04:57:10Z\nYour EV may go a long way between charges, but...\n\n\n47\n{'id': 'reuters', 'name': 'Reuters'}\nJoseph White\nGM withdraws 2023 guidance as UAW strike costs...\nGeneral Motors &lt;a href=\"https://www.reuters.co...\nhttps://www.reuters.com/business/autos-transpo...\nhttps://www.reuters.com/resizer/2eWtjHsHlJ4x8o...\n2023-10-24T10:40:38Z\nDETROIT, Oct 24 (Reuters) - General Motors (GM...\n\n\n48\n{'id': 'usa-today', 'name': 'USA Today'}\nNada Hassanein\nAbortion ban medical exceptions often exclude ...\nPregnant women were more likely to die from me...\nhttps://www.usatoday.com/story/news/nation/202...\nhttps://www.usatoday.com/gcdn/authoring/author...\n2023-10-26T11:35:39Z\nMore than a dozen states now have near-total a...\n\n\n\n\n\n\n\n\nsources\n\n\n\n\n\n\n\n\nid\nname\ndescription\nurl\ncategory\nlanguage\ncountry\n\n\n\n\n0\nabc-news\nABC News\nYour trusted source for breaking news, analysi...\nhttps://abcnews.go.com\ngeneral\nen\nus\n\n\n3\nal-jazeera-english\nAl Jazeera English\nNews, analysis from the Middle East and worldw...\nhttp://www.aljazeera.com\ngeneral\nen\nus\n\n\n6\nars-technica\nArs Technica\nThe PC enthusiast's resource. Power users and ...\nhttp://arstechnica.com\ntechnology\nen\nus\n\n\n8\nassociated-press\nAssociated Press\nThe AP delivers in-depth coverage on the inter...\nhttps://apnews.com/\ngeneral\nen\nus\n\n\n10\naxios\nAxios\nAxios are a new media company delivering vital...\nhttps://www.axios.com\ngeneral\nen\nus\n\n\n16\nbloomberg\nBloomberg\nBloomberg delivers business and markets news, ...\nhttp://www.bloomberg.com\nbusiness\nen\nus\n\n\n17\nbreitbart-news\nBreitbart News\nSyndicated news and opinion website providing ...\nhttp://www.breitbart.com\ngeneral\nen\nus\n\n\n18\nbusiness-insider\nBusiness Insider\nBusiness Insider is a fast-growing business si...\nhttp://www.businessinsider.com\nbusiness\nen\nus\n\n\n22\ncbs-news\nCBS News\nCBS News: dedicated to providing the best in j...\nhttp://www.cbsnews.com\ngeneral\nen\nus\n\n\n23\ncnn\nCNN\nView the latest news and breaking news today f...\nhttp://us.cnn.com\ngeneral\nen\nus\n\n\n25\ncrypto-coins-news\nCrypto Coins News\nProviding breaking cryptocurrency news - focus...\nhttps://www.ccn.com\ntechnology\nen\nus\n\n\n29\nengadget\nEngadget\nEngadget is a web magazine with obsessive dail...\nhttps://www.engadget.com\ntechnology\nen\nus\n\n\n36\nfortune\nFortune\nFortune 500 Daily and Breaking Business News\nhttp://fortune.com\nbusiness\nen\nus\n\n\n38\nfox-news\nFox News\nBreaking News, Latest News and Current News fr...\nhttp://www.foxnews.com\ngeneral\nen\nus\n\n\n41\ngoogle-news\nGoogle News\nComprehensive, up-to-date news coverage, aggre...\nhttps://news.google.com\ngeneral\nen\nus\n\n\n55\nhacker-news\nHacker News\nHacker News is a social news website focusing ...\nhttps://news.ycombinator.com\ntechnology\nen\nus\n\n\n73\nmsnbc\nMSNBC\nBreaking news and in-depth analysis of the hea...\nhttp://www.msnbc.com\ngeneral\nen\nus\n\n\n76\nnational-geographic\nNational Geographic\nReporting our world daily: original nature and...\nhttp://news.nationalgeographic.com\nscience\nen\nus\n\n\n77\nnational-review\nNational Review\nNational Review: Conservative News, Opinion, P...\nhttps://www.nationalreview.com/\ngeneral\nen\nus\n\n\n78\nnbc-news\nNBC News\nBreaking news, videos, and the latest top stor...\nhttp://www.nbcnews.com\ngeneral\nen\nus\n\n\n80\nnew-scientist\nNew Scientist\nBreaking science and technology news from arou...\nhttps://www.newscientist.com/section/news\nscience\nen\nus\n\n\n82\nnewsweek\nNewsweek\nNewsweek provides in-depth analysis, news and ...\nhttps://www.newsweek.com\ngeneral\nen\nus\n\n\n83\nnew-york-magazine\nNew York Magazine\nNYMAG and New York magazine cover the new, the...\nhttp://nymag.com\ngeneral\nen\nus\n\n\n84\nnext-big-future\nNext Big Future\nCoverage of science and technology that have t...\nhttps://www.nextbigfuture.com\nscience\nen\nus\n\n\n88\npolitico\nPolitico\nPolitical news about Congress, the White House...\nhttps://www.politico.com\ngeneral\nen\nus\n\n\n91\nrecode\nRecode\nGet the latest independent tech news, reviews ...\nhttp://www.recode.net\ntechnology\nen\nus\n\n\n92\nreddit-r-all\nReddit /r/all\nReddit is an entertainment, social news networ...\nhttps://www.reddit.com/r/all\ngeneral\nen\nus\n\n\n93\nreuters\nReuters\nReuters.com brings you the latest news from ar...\nhttp://www.reuters.com\ngeneral\nen\nus\n\n\n102\ntechcrunch\nTechCrunch\nTechCrunch is a leading technology media prope...\nhttps://techcrunch.com\ntechnology\nen\nus\n\n\n104\ntechradar\nTechRadar\nThe latest technology news and reviews, coveri...\nhttp://www.techradar.com\ntechnology\nen\nus\n\n\n105\nthe-american-conservative\nThe American Conservative\nRealism and reform. A new voice for a new gene...\nhttp://www.theamericanconservative.com/\ngeneral\nen\nus\n\n\n107\nthe-hill\nThe Hill\nThe Hill is a top US political website, read b...\nhttp://thehill.com\ngeneral\nen\nus\n\n\n109\nthe-huffington-post\nThe Huffington Post\nThe Huffington Post is a politically liberal A...\nhttp://www.huffingtonpost.com\ngeneral\nen\nus\n\n\n113\nthe-next-web\nThe Next Web\nThe Next Web is one of the world’s largest onl...\nhttp://thenextweb.com\ntechnology\nen\nus\n\n\n116\nthe-verge\nThe Verge\nThe Verge covers the intersection of technolog...\nhttp://www.theverge.com\ntechnology\nen\nus\n\n\n117\nthe-wall-street-journal\nThe Wall Street Journal\nWSJ online coverage of breaking news and curre...\nhttp://www.wsj.com\nbusiness\nen\nus\n\n\n118\nthe-washington-post\nThe Washington Post\nBreaking news and analysis on politics, busine...\nhttps://www.washingtonpost.com\ngeneral\nen\nus\n\n\n119\nthe-washington-times\nThe Washington Times\nThe Washington Times delivers breaking news an...\nhttps://www.washingtontimes.com/\ngeneral\nen\nus\n\n\n120\ntime\nTime\nBreaking news and analysis from TIME.com. Poli...\nhttp://time.com\ngeneral\nen\nus\n\n\n121\nusa-today\nUSA Today\nGet the latest national, international, and po...\nhttp://www.usatoday.com/news\ngeneral\nen\nus\n\n\n122\nvice-news\nVice News\nVice News is Vice Media, Inc.'s current affair...\nhttps://news.vice.com\ngeneral\nen\nus\n\n\n123\nwired\nWired\nWired is a monthly American magazine, publishe...\nhttps://www.wired.com\ntechnology\nen\nus"
  },
  {
    "objectID": "3_data_cleaning Python.html#ibm-watson---sentiment-analysis",
    "href": "3_data_cleaning Python.html#ibm-watson---sentiment-analysis",
    "title": "Data Gathering Python",
    "section": "IBM Watson - Sentiment Analysis",
    "text": "IBM Watson - Sentiment Analysis\n\n# pip install xlsxwriter\n#pip install xlrd\n#pip install openpyxl\n\nimport json\nfrom ibm_watson import NaturalLanguageUnderstandingV1\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\nfrom ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions\n\nimport os\n\n\ndf_content = pd.DataFrame(df_articles, columns=['source', 'content', 'publishedAt'])\ndf_content['source'] = df_content['source'].apply(lambda x: x['id'])\n\n# Vieja\n#_4YE1Qj6PFjke1zYsp7Kapgfu5laaaBE1E_ZUw1IiUPa\n\n# Nueva\n#DHHnv30sML61Hn2pt6iHBdMKtj6bxV1PLvzg7j0OSQf3\n\nauthenticator = IAMAuthenticator('_4YE1Qj6PFjke1zYsp7Kapgfu5laaaBE1E_ZUw1IiUPa')\nnatural_language_understanding = NaturalLanguageUnderstandingV1(\n    version='2020-08-01',\n    authenticator=authenticator\n)\n\nnatural_language_understanding.set_service_url('https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/8b0909d1-3768-4c54-b80d-b9817610e36d')\n\n\n#IBM Watson\ni = 0\nibm_source = []\nibm_date = []\nibm_score = []\nibm_label = []\nibm_content = []\n\nfor index, row in df_content.iterrows():\n    response = natural_language_understanding.analyze(\n        text=row['content'], \n        language = 'en', \n        features=Features(sentiment=SentimentOptions())).get_result()\n\n    s = row['source']\n    ibm_source.append(s)\n    d = row['publishedAt']\n    ibm_date.append(d)\n    c = row['content']\n    ibm_content.append(c)\n    x = response['sentiment']['document']['score']\n    x = round(x, 4)\n    ibm_score.append(x)\n    y = response['sentiment']['document']['label']\n    ibm_label.append(y)\n    # print(response['sentiment']['document']['score'])\n    # print(response['sentiment']['document']['label'])\n    # print(json.dumps(response, indent=2))\n\n    i=i+1   \n\n\nresults = {\"id\": ibm_source, \"ibm_date\": ibm_date, \"ibm_score\": ibm_score, \"ibm_label\": ibm_label, \"ibm_content\": ibm_content}\n\nresults = pd.DataFrame(results)\n\n\nresults = results.merge(sources, how='left')\n\nresults = pd.DataFrame(results, columns=['name', 'category', 'ibm_score', 'ibm_label', 'ibm_date', 'ibm_content'])\n\nresults = results.rename(columns={'ibm_date': 'date'})\n\nresults['date'] = pd.to_datetime(results['date'])\n\nresults['date'] = results['date'].dt.date\n\nresults\n\n\n\n\n\n\n\n\nname\ncategory\nibm_score\nibm_label\ndate\nibm_content\n\n\n\n\n0\nWired\ntechnology\n-0.5961\nnegative\n2023-10-27\nIn lithium-ion batteries, thats no longer the ...\n\n\n1\nThe Verge\ntechnology\n-0.8783\nnegative\n2023-10-27\nFord hits the brakes on $12 billion in EV spen...\n\n\n2\nArs Technica\ntechnology\n0.2886\npositive\n2023-10-31\nEnlarge/ GreenPower has given its class-D elec...\n\n\n3\nBusiness Insider\nbusiness\n0.6452\npositive\n2023-10-22\nWuling Hongguang Mini EVs on display at the Sh...\n\n\n4\nThe Next Web\ntechnology\n0.6209\npositive\n2023-10-24\nIn a big boost to sustainable mobility, 130 mi...\n\n\n5\nThe Next Web\ntechnology\n0.9080\npositive\n2023-10-23\nNorwegian tech company AutoStore today unveile...\n\n\n6\nBusiness Insider\nbusiness\n-0.6760\nnegative\n2023-10-18\nWhen something enters Earth's atmosphere, it's...\n\n\n7\nEngadget\ntechnology\n0.0000\nneutral\n2023-10-22\nSpace isn't hard only on account of the rocket...\n\n\n8\nBusiness Insider\nbusiness\n0.6035\npositive\n2023-10-29\nThe National Spherical Torus Experiment-Upgrad...\n\n\n9\nTime\ngeneral\n0.0000\nneutral\n2023-10-19\nIn her new book, The Woman in Me, Britney Spea...\n\n\n10\nTime\ngeneral\n0.0000\nneutral\n2023-10-19\nThe rocky relationship between Britney Spears ...\n\n\n11\nCBS News\ngeneral\n-0.6290\nnegative\n2023-10-23\nRiders of Toos Elite 60-volt electric scooters...\n\n\n12\nNext Big Future\nscience\n0.0000\nneutral\n2023-10-25\nBrian Wang is a Futurist Thought Leader and a ...\n\n\n13\nNew Scientist\nscience\n0.6051\npositive\n2023-10-31\nPassive cooling could be more efficient using ...\n\n\n14\nFortune\nbusiness\n-0.8252\nnegative\n2023-10-19\nWith the growing popularity of disposable e-ci...\n\n\n15\nFortune\nbusiness\n0.0000\nneutral\n2023-10-31\nToyota will invest an additional $8 billion in...\n\n\n16\nNew Scientist\nscience\n0.8984\npositive\n2023-10-24\nSolid-state batteries could be lighter and mor...\n\n\n17\nThe Verge\ntechnology\n0.0000\nneutral\n2023-11-02\nRedwood Materials is recycling its first stati...\n\n\n18\nNewsweek\ngeneral\n-0.9520\nnegative\n2023-10-23\nU.S. consumers are being strongly warned again...\n\n\n19\nABC News\ngeneral\n0.0000\nneutral\n2023-10-31\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n20\nFortune\nbusiness\n-0.6701\nnegative\n2023-10-31\nSince Russian President Vladimir Putin invaded...\n\n\n21\nBreitbart News\ngeneral\n0.0000\nneutral\n2023-10-19\nHaji Nooruddin Azizi, the acting commerce mini...\n\n\n22\nABC News\ngeneral\n0.6487\npositive\n2023-11-01\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n23\nABC News\ngeneral\n-0.8907\nnegative\n2023-10-19\nWASHINGTON -- With the growing popularity of d...\n\n\n24\nABC News\ngeneral\n0.6301\npositive\n2023-10-29\nTOKYO -- Trade and economy officials from the ...\n\n\n25\nNew Scientist\nscience\n0.0000\nneutral\n2023-10-30\nThe Athel tamarisk has ingenious ways of survi...\n\n\n26\nNewsweek\ngeneral\n0.4480\npositive\n2023-10-30\nAround the world, entrepreneurs are trying to ...\n\n\n27\nABC News\ngeneral\n0.0000\nneutral\n2023-10-31\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n28\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n29\nABC News\ngeneral\n-0.8907\nnegative\n2023-10-19\nWASHINGTON -- With the growing popularity of d...\n\n\n30\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n31\nABC News\ngeneral\n-0.6795\nnegative\n2023-11-02\nWASHINGTON -- Treasury Secretary Janet Yellen ...\n\n\n32\nABC News\ngeneral\n0.6487\npositive\n2023-11-01\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n33\nABC News\ngeneral\n-0.6795\nnegative\n2023-11-02\nWASHINGTON -- Treasury Secretary Janet Yellen ...\n\n\n34\nABC News\ngeneral\n0.6301\npositive\n2023-10-29\nTOKYO -- Trade and economy officials from the ...\n\n\n35\nTime\ngeneral\n-0.3817\nnegative\n2023-10-20\nBritney Spears knows what it means to be depri...\n\n\n36\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n37\nArs Technica\ntechnology\n-0.7175\nnegative\n2023-10-25\nEnlarge/ A GM Ultium battery pack. \\r\\n75 with...\n\n\n38\nBreitbart News\ngeneral\n0.3935\npositive\n2023-10-31\nAfter investing billions to adhere to Presiden...\n\n\n39\nBreitbart News\ngeneral\n0.0000\nneutral\n2023-10-23\nNearly 7,000 auto workers at Stellantis’ Sterl...\n\n\n40\nFox News\ngeneral\n-0.8252\nnegative\n2023-10-19\nWith the growing popularity of disposable e-ci...\n\n\n41\nTechRadar\ntechnology\n-0.5295\nnegative\n2023-10-30\nYeedi Cube: One-minute review\\r\\nThe Yeedi Cub...\n\n\n42\nNBC News\ngeneral\n-0.5513\nnegative\n2023-10-20\nBritney Spears is shedding new light onto what...\n\n\n43\nFox News\ngeneral\n-0.4456\nnegative\n2023-11-02\nPresident Biden will meet with Chilean Preside...\n\n\n44\nBreitbart News\ngeneral\n0.3316\npositive\n2023-10-27\nAfter investing billions into a green energy a...\n\n\n45\nFox News\ngeneral\n0.0000\nneutral\n2023-10-18\nIn yet another attempt to regulate the car mar...\n\n\n46\nEngadget\ntechnology\n-0.6099\nnegative\n2023-11-01\nYour EV may go a long way between charges, but...\n\n\n47\nReuters\ngeneral\n-0.3458\nnegative\n2023-10-24\nDETROIT, Oct 24 (Reuters) - General Motors (GM...\n\n\n48\nUSA Today\ngeneral\n-0.6586\nnegative\n2023-10-26\nMore than a dozen states now have near-total a..."
  },
  {
    "objectID": "3_data_cleaning R.html",
    "href": "3_data_cleaning R.html",
    "title": "Data Gathering R",
    "section": "",
    "text": "Libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(readxl)\nlibrary(zoo)"
  },
  {
    "objectID": "3_data_cleaning/3_5_news_api.html",
    "href": "3_data_cleaning/3_5_news_api.html",
    "title": "Lithium News",
    "section": "",
    "text": "Libraries\nfrom datetime import date\nfrom datetime import date, timedelta\nfrom newsapi.newsapi_client import NewsApiClient\nimport pandas as pd\n\nimport json\nfrom ibm_watson import NaturalLanguageUnderstandingV1\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\nfrom ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions\n\nimport os\n\n\n\nNews API\n\n\nCode\ndate = date.today()\ndate_past = date - timedelta(days=20)\n\nf = open('../auth.k','r', encoding=\"utf-8\")\nak = f.readlines()\nf.close()\n\nnewsapi = NewsApiClient(api_key=ak[0])\n\nsources = newsapi.get_sources()\n\nsources = pd.DataFrame(sources['sources'])\n\nsources = sources[(sources['language'] == 'en') & (sources['country'] == 'us') & ~sources['category'].isin(['sports', 'entertainment', 'health'])]\n\nsources.head(5)\n\n\n\n\n\n\n\n\n\nid\nname\ndescription\nurl\ncategory\nlanguage\ncountry\n\n\n\n\n0\nabc-news\nABC News\nYour trusted source for breaking news, analysi...\nhttps://abcnews.go.com\ngeneral\nen\nus\n\n\n3\nal-jazeera-english\nAl Jazeera English\nNews, analysis from the Middle East and worldw...\nhttp://www.aljazeera.com\ngeneral\nen\nus\n\n\n6\nars-technica\nArs Technica\nThe PC enthusiast's resource. Power users and ...\nhttp://arstechnica.com\ntechnology\nen\nus\n\n\n8\nassociated-press\nAssociated Press\nThe AP delivers in-depth coverage on the inter...\nhttps://apnews.com/\ngeneral\nen\nus\n\n\n10\naxios\nAxios\nAxios are a new media company delivering vital...\nhttps://www.axios.com\ngeneral\nen\nus\n\n\n\n\n\n\n\n\n\nCode\ndf_sources = ', '.join(sources['id'].astype(str))\n\ndf_domains = ', '.join(sources['url'].astype(str))\n\nall_articles_1 = newsapi.get_everything(q='lithium',\n                                      sources=str(df_sources),\n                                      domains=str(df_domains),\n                                      from_param=date_past,\n                                      to=date,\n                                      language='en',\n                                      sort_by='relevancy')\n\nall_articles_2 = newsapi.get_everything(q='\"lithium batteries\"',\n                                      sources=str(df_sources),\n                                      domains=str(df_domains),\n                                      from_param=date_past,\n                                      to=date,\n                                      language='en',\n                                      sort_by='relevancy')\n\nall_articles_3 = newsapi.get_everything(q='\"electric vehicles\"',\n                                      sources=str(df_sources),\n                                      domains=str(df_domains),\n                                      from_param=date_past,\n                                      to=date,\n                                      language='en',\n                                      sort_by='relevancy')\n\ndf_articles_1 = pd.DataFrame(all_articles_1['articles'])\n\ndf_articles_2 = pd.DataFrame(all_articles_2['articles'])\n\ndf_articles_3 = pd.DataFrame(all_articles_3['articles'])\n\n\ndf_articles = pd.concat([df_articles_1, df_articles_2, df_articles_3], ignore_index=True)\n\ndf_articles[['id', 'name']] = df_articles['source'].apply(lambda x: pd.Series([x['id'], x['name']]))\n\ndf_articles_save = pd.DataFrame(df_articles, columns=[\n                                                'id', \n                                                'name', \n                                                'author', \n                                                'title',\n                                                'description',\n                                                'url',\n                                                'urlToImage', \n                                                'content', \n                                                'publishedAt'])\n\ndf_articles.head(5)\n\n\n\n\n\n\n\n\n\nsource\nauthor\ntitle\ndescription\nurl\nurlToImage\npublishedAt\ncontent\nid\nname\n\n\n\n\n0\n{'id': 'ars-technica', 'name': 'Ars Technica'}\nJonathan M. Gitlin\nA lithium mine for EV batteries is coming to A...\nWith EV incentives tied to domestic battery co...\nhttps://arstechnica.com/cars/2023/11/exxon-wil...\nhttps://cdn.arstechnica.net/wp-content/uploads...\n2023-11-13T18:59:10Z\nEnlarge/ These are piles of lithium harvested ...\nars-technica\nArs Technica\n\n\n1\n{'id': 'abc-news', 'name': 'ABC News'}\nKAREN MATTHEWS Associated Press\nAfter fatal fire, officials push retailers, de...\nNew York City officials say retailers and food...\nhttps://abcnews.go.com/US/wireStory/after-fire...\nhttps://i.abcnewsfe.com/a/d19c885d-b4ca-4f4d-8...\n2023-11-14T20:30:35Z\nNEW YORK -- New York City officials say retail...\nabc-news\nABC News\n\n\n2\n{'id': 'business-insider', 'name': 'Business I...\nHaley Tenore\nCheck out how different the inside of fake Air...\nThere are a lot of differences between real Ai...\nhttps://www.businessinsider.com/difference-bet...\nhttps://i.insider.com/654e90983cc84b4dfa0066bf...\n2023-11-13T09:00:02Z\nYour fake AirPods may be able to fool others a...\nbusiness-insider\nBusiness Insider\n\n\n3\n{'id': 'ars-technica', 'name': 'Ars Technica'}\nJonathan M. Gitlin\nThe 2024 Alfa Romeo Tonale is a confoundingly ...\nFaults that should have been frustrating just ...\nhttps://arstechnica.com/cars/2023/11/the-2024-...\nhttps://cdn.arstechnica.net/wp-content/uploads...\n2023-11-27T17:16:29Z\nEnlarge/ Alfa Romeo has a new crossover called...\nars-technica\nArs Technica\n\n\n4\n{'id': 'ars-technica', 'name': 'Ars Technica'}\nJonathan M. Gitlin\nThe return of GTP racing to IMSA gets a big th...\nHybrid prototypes from Acura, BMW, Cadillac, a...\nhttps://arstechnica.com/cars/2023/11/the-retur...\nhttps://cdn.arstechnica.net/wp-content/uploads...\n2023-11-21T12:00:43Z\nEnlarge/ A Porsche 963, one of the four differ...\nars-technica\nArs Technica\n\n\n\n\n\n\n\n\n\nShape 'Lithium': \n(80, 8)\nShape 'Lithium Batteries': \n(6, 8)\nShape 'Electric Vehicles': \n(100, 8)\nShape Total: \n(186, 10)\n\n\n\n\n\nIBM Watson - Sentiment Analysis\n\n\nCode\ndf_content = pd.DataFrame(df_articles, columns=['source', 'content', 'publishedAt'])\ndf_content['source'] = df_content['source'].apply(lambda x: x['id'])\n\nauthenticator = IAMAuthenticator('_4YE1Qj6PFjke1zYsp7Kapgfu5laaaBE1E_ZUw1IiUPa')\nnatural_language_understanding = NaturalLanguageUnderstandingV1(\n    version='2020-08-01',\n    authenticator=authenticator\n)\n\nnatural_language_understanding.set_service_url('https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/8b0909d1-3768-4c54-b80d-b9817610e36d')\n\n#IBM Watson\ni = 0\nibm_source = []\nibm_date = []\nibm_score = []\nibm_label = []\nibm_content = []\n\nfor index, row in df_content.iterrows():\n    response = natural_language_understanding.analyze(\n    text=row['content'], language = 'en',\n    features=Features(sentiment=SentimentOptions())).get_result()\n\n    s = row['source']\n    ibm_source.append(s)\n    d = row['publishedAt']\n    ibm_date.append(d)\n    c = row['content']\n    ibm_content.append(c)\n    x = response['sentiment']['document']['score']\n    x = round(x, 4)\n    ibm_score.append(x)\n    y = response['sentiment']['document']['label']\n    ibm_label.append(y)\n    # print(response['sentiment']['document']['score'])\n    # print(response['sentiment']['document']['label'])\n    # print(json.dumps(response, indent=2))\n\n    i=i+1   \n\nresults = {\"id\": ibm_source, \"ibm_date\": ibm_date, \"ibm_score\": ibm_score, \"ibm_label\": ibm_label, \"ibm_content\": ibm_content}\n\nresults = pd.DataFrame(results)\n\n\n\n\nCode\ndate = date.today()\n\nname = '../' + str(date) + '_results.csv'\n\nresults.to_csv(name, index=False)\n\nresults = results.merge(sources, how='left')\n\nresults = pd.DataFrame(results, columns=['name', 'category', 'ibm_score', 'ibm_label', 'ibm_date', 'ibm_content'])\n\nresults = results.rename(columns={'ibm_date': 'date'})\n\nresults['date'] = pd.to_datetime(results['date'])\n\nresults['date'] = results['date'].dt.date\n\nresults.head(5)\n\n\n\n\n\n\n\n\n\nname\ncategory\nibm_score\nibm_label\ndate\nibm_content\n\n\n\n\n0\nArs Technica\ntechnology\n0.0000\nneutral\n2023-11-13\nEnlarge/ These are piles of lithium harvested ...\n\n\n1\nABC News\ngeneral\n-0.7716\nnegative\n2023-11-14\nNEW YORK -- New York City officials say retail...\n\n\n2\nBusiness Insider\nbusiness\n-0.3920\nnegative\n2023-11-13\nYour fake AirPods may be able to fool others a...\n\n\n3\nArs Technica\ntechnology\n-0.5907\nnegative\n2023-11-27\nEnlarge/ Alfa Romeo has a new crossover called...\n\n\n4\nArs Technica\ntechnology\n0.0000\nneutral\n2023-11-21\nEnlarge/ A Porsche 963, one of the four differ..."
  },
  {
    "objectID": "3_5_news_api.html",
    "href": "3_5_news_api.html",
    "title": "Lithium News",
    "section": "",
    "text": "Libraries\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\nfrom newsapi.newsapi_client import NewsApiClient\nimport pandas as pd\n\nimport json\nfrom ibm_watson import NaturalLanguageUnderstandingV1\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\nfrom ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions\n\nimport os\n\n\n\nNews API\n\n\nCode\ndate = date.today()\ndate_past = date - relativedelta(days=12)\n\nf = open('auth.k','r', encoding=\"utf-8\")\nak = f.readlines()\nf.close()\n\nnewsapi = NewsApiClient(api_key=ak[0])\n\nsources = newsapi.get_sources()\n\nsources = pd.DataFrame(sources['sources'])\n\nsources = sources[(sources['language'] == 'en') & (sources['country'] == 'us') & ~sources['category'].isin(['sports', 'entertainment', 'health'])]\n\nprint(sources.head(5))\n\n\n                    id                name  \\\n0             abc-news            ABC News   \n3   al-jazeera-english  Al Jazeera English   \n6         ars-technica        Ars Technica   \n8     associated-press    Associated Press   \n10               axios               Axios   \n\n                                          description  \\\n0   Your trusted source for breaking news, analysi...   \n3   News, analysis from the Middle East and worldw...   \n6   The PC enthusiast's resource. Power users and ...   \n8   The AP delivers in-depth coverage on the inter...   \n10  Axios are a new media company delivering vital...   \n\n                         url    category language country  \n0     https://abcnews.go.com     general       en      us  \n3   http://www.aljazeera.com     general       en      us  \n6     http://arstechnica.com  technology       en      us  \n8        https://apnews.com/     general       en      us  \n10     https://www.axios.com     general       en      us  \n\n\n\n\nCode\ndf_sources = ', '.join(sources['id'].astype(str))\n\ndf_domains = ', '.join(sources['url'].astype(str))\n\nall_articles = newsapi.get_everything(q='lithium',\n                                      sources=str(df_sources),\n                                      domains=str(df_domains),\n                                      from_param=date_past,\n                                      to=date,\n                                      language='en',\n                                      sort_by='relevancy')\n\ndf_articles = pd.DataFrame(all_articles['articles'])\n\ndf_articles.head(5)\n\n\n\n\n\n\n\n\n\nsource\nauthor\ntitle\ndescription\nurl\nurlToImage\npublishedAt\ncontent\n\n\n\n\n0\n{'id': 'abc-news', 'name': 'ABC News'}\nABC News\nWATCH: CCTV captures terrifying moment E-bike ...\nCCTV captures the moment an e-bike's lithium-i...\nhttps://abcnews.go.com/US/video/cctv-captures-...\nhttps://i.abcnewsfe.com/a/7d89adcc-4f6c-4a61-9...\n2023-10-04T11:03:13Z\n&lt;ul&gt;&lt;li&gt;Whats next for Russia? \\r\\n&lt;/li&gt;&lt;li&gt;Wh...\n\n\n1\n{'id': 'the-next-web', 'name': 'The Next Web'}\nIoanna Lykiardopoulou\nFrench startup bags €40M for unique CO2 batter...\nLyon-based Mecaware has raised €40mn in fundin...\nhttps://thenextweb.com/news/french-startup-40m...\nhttps://img-cdn.tnwcdn.com/image/tnw-blurple?f...\n2023-10-09T14:05:22Z\nLyon-based Mecaware has raised 40mn in funding...\n\n\n2\n{'id': 'ars-technica', 'name': 'Ars Technica'}\nRyan Waniata\n25 of the most popular Amazon Prime Day deals ...\nAirPods, docking stations, Switch games, and m...\nhttps://arstechnica.com/shopping/2023/10/25-of...\nhttps://cdn.arstechnica.net/wp-content/uploads...\n2023-10-11T22:03:33Z\nEnlarge/ The case for AirPods Pro.\\r\\n5 with \\...\n\n\n3\n{'id': 'abc-news', 'name': 'ABC News'}\nFARAI MUTSAKA Associated Press\nDeath toll in collapsed Zimbabwe gold mine exp...\nZimbabwe's vice president says the death toll ...\nhttps://abcnews.go.com/Business/wireStory/deat...\nhttps://i.abcnewsfe.com/a/9b5f9dbd-61d8-4bf8-8...\n2023-10-01T12:32:04Z\nHARARE, Zimbabwe -- The death toll from a shaf...\n\n\n4\n{'id': 'ars-technica', 'name': 'Ars Technica'}\nJonathan M. Gitlin\nThe 2023 Toyota Prius Prime is a mostly pleasa...\nToyota has helpfully increased the size of the...\nhttps://arstechnica.com/cars/2023/10/the-2023-...\nhttps://cdn.arstechnica.net/wp-content/uploads...\n2023-10-11T16:47:27Z\nEnlarge/ After a confusing mess for the last g...\n\n\n\n\n\n\n\n\n\n\nIBM Watson - Sentiment Analysis\n\n\nCode\ndf_content = pd.DataFrame(df_articles, columns=['source', 'content', 'publishedAt'])\ndf_content['source'] = df_content['source'].apply(lambda x: x['id'])\n\nauthenticator = IAMAuthenticator('_4YE1Qj6PFjke1zYsp7Kapgfu5laaaBE1E_ZUw1IiUPa')\nnatural_language_understanding = NaturalLanguageUnderstandingV1(\n    version='2020-08-01',\n    authenticator=authenticator\n)\n\nnatural_language_understanding.set_service_url('https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/8b0909d1-3768-4c54-b80d-b9817610e36d')\n\n#IBM Watson\ni = 0\nibm_source = []\nibm_date = []\nibm_score = []\nibm_label = []\nfor index, row in df_content.iterrows():\n    response = natural_language_understanding.analyze(\n    text=row['content'], language = 'en',\n    features=Features(sentiment=SentimentOptions())).get_result()\n\n    s = row['source']\n    ibm_source.append(s)\n    d = row['publishedAt']\n    ibm_date.append(d)\n    x = response['sentiment']['document']['score']\n    x = round(x, 4)\n    ibm_score.append(x)\n    y = response['sentiment']['document']['label']\n    ibm_label.append(y)\n    # print(response['sentiment']['document']['score'])\n    # print(response['sentiment']['document']['label'])\n    # print(json.dumps(response, indent=2))\n\n    i=i+1   \n\nresults = {\"id\": ibm_source, \"ibm_date\": ibm_date, \"ibm_score\": ibm_score, \"ibm_label\": ibm_label}\n\nresults = pd.DataFrame(data = results)\n\n\n\n\nCode\n# date = date.today()\n# name = str(date) + '_results.csv'\n\n# results.to_csv(name, index=False)\n\nresults = results.merge(sources, how='left')\n\nresults = pd.DataFrame(results, columns=['name', 'category', 'ibm_score', 'ibm_label', 'ibm_date'])\n\nresults = results.rename(columns={'ibm_date': 'date'})\n\nresults['date'] = pd.to_datetime(results['date'])\n\nresults['date'] = results['date'].dt.date\n\nresults\n\n\n\n\n\n\n\n\n\nname\ncategory\nibm_score\nibm_label\ndate\n\n\n\n\n0\nABC News\ngeneral\n-0.6553\nnegative\n2023-10-04\n\n\n1\nThe Next Web\ntechnology\n0.5855\npositive\n2023-10-09\n\n\n2\nArs Technica\ntechnology\n-0.8725\nnegative\n2023-10-11\n\n\n3\nABC News\ngeneral\n-0.7182\nnegative\n2023-10-01\n\n\n4\nArs Technica\ntechnology\n0.0000\nneutral\n2023-10-11\n\n\n5\nNext Big Future\nscience\n0.0000\nneutral\n2023-10-05\n\n\n6\nTechRadar\ntechnology\n-0.9441\nnegative\n2023-10-06\n\n\n7\nNewsweek\ngeneral\n0.8858\npositive\n2023-10-11\n\n\n8\nTechRadar\ntechnology\n0.0000\nneutral\n2023-10-05\n\n\n9\nCBS News\ngeneral\n-0.6365\nnegative\n2023-10-04\n\n\n10\nMSNBC\ngeneral\n0.0000\nneutral\n2023-10-06\n\n\n11\nABC News\ngeneral\n0.0000\nneutral\n2023-10-05\n\n\n12\nBreitbart News\ngeneral\n0.0000\nneutral\n2023-10-10\n\n\n13\nThe American Conservative\ngeneral\n-0.8355\nnegative\n2023-10-05\n\n\n14\nCBS News\ngeneral\n-0.6413\nnegative\n2023-10-10\n\n\n15\nCBS News\ngeneral\n0.6513\npositive\n2023-10-06\n\n\n16\nCBS News\ngeneral\n-0.9248\nnegative\n2023-10-11\n\n\n17\nCBS News\ngeneral\n0.0000\nneutral\n2023-10-11\n\n\n18\nABC News\ngeneral\n-0.7182\nnegative\n2023-10-01\n\n\n19\nNewsweek\ngeneral\n-0.3899\nnegative\n2023-10-03\n\n\n20\nNewsweek\ngeneral\n-0.4266\nnegative\n2023-10-11\n\n\n21\nEngadget\ntechnology\n0.8375\npositive\n2023-10-03\n\n\n22\nCBS News\ngeneral\n0.0000\nneutral\n2023-10-11\n\n\n23\nCBS News\ngeneral\n0.7472\npositive\n2023-10-10\n\n\n24\nNewsweek\ngeneral\n0.5167\npositive\n2023-10-03\n\n\n25\nThe Verge\ntechnology\n0.0000\nneutral\n2023-10-03\n\n\n26\nNew Scientist\nscience\n0.0000\nneutral\n2023-10-04\n\n\n27\nABC News\ngeneral\n0.0000\nneutral\n2023-10-02\n\n\n28\nNBC News\ngeneral\n0.0000\nneutral\n2023-10-04\n\n\n29\nThe Verge\ntechnology\n0.0000\nneutral\n2023-10-10\n\n\n30\nABC News\ngeneral\n0.0000\nneutral\n2023-10-02\n\n\n31\nFox News\ngeneral\n0.6282\npositive\n2023-10-05\n\n\n32\nBreitbart News\ngeneral\n-0.9107\nnegative\n2023-10-04\n\n\n33\nThe Verge\ntechnology\n0.0000\nneutral\n2023-10-10\n\n\n34\nReuters\ngeneral\n0.0000\nneutral\n2023-10-12\n\n\n35\nThe Verge\ntechnology\n-0.9079\nnegative\n2023-10-11\n\n\n36\nPolitico\ngeneral\n-0.8570\nnegative\n2023-10-02\n\n\n37\nTechCrunch\ntechnology\n0.8370\npositive\n2023-10-02\n\n\n38\nThe Verge\ntechnology\n0.3791\npositive\n2023-10-11"
  },
  {
    "objectID": "3_data_cleaning R.html#global-lithium-production",
    "href": "3_data_cleaning R.html#global-lithium-production",
    "title": "Data Gathering R",
    "section": "Global Lithium Production",
    "text": "Global Lithium Production\n\ndf_production &lt;- read.csv(\"../../data/00-raw-data/lithium-production.csv\")\n\ndf_production &lt;- df_production %&gt;% filter(nchar(Code) == 3)\n\nhead(df_production, n = 10)\n\n#summary(df_production)\n\n\nA data.frame: 10 × 4\n\n\n\nEntity\nCode\nYear\nLithium.production...kt\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\nArgentina\nARG\n1995\n8\n\n\n2\nArgentina\nARG\n1996\n8\n\n\n3\nArgentina\nARG\n1997\n8\n\n\n4\nArgentina\nARG\n1998\n1130\n\n\n5\nArgentina\nARG\n1999\n200\n\n\n6\nArgentina\nARG\n2000\n200\n\n\n7\nArgentina\nARG\n2001\n200\n\n\n8\nArgentina\nARG\n2002\n946\n\n\n9\nArgentina\nARG\n2003\n960\n\n\n10\nArgentina\nARG\n2004\n1970"
  },
  {
    "objectID": "3_data_cleaning R.html#chinese-yuan-renminbi-to-u.s.-dollar-spot-exchange-rate",
    "href": "3_data_cleaning R.html#chinese-yuan-renminbi-to-u.s.-dollar-spot-exchange-rate",
    "title": "Data Gathering R",
    "section": "Chinese Yuan Renminbi to U.S. Dollar Spot Exchange Rate",
    "text": "Chinese Yuan Renminbi to U.S. Dollar Spot Exchange Rate\n\n\nLibraries\n# Set the start and end dates\nstart_date &lt;- \"2010-01-01\"\nend_date &lt;- \"2022-12-31\"\n\n# Define the symbol for CNY to USD exchange rate\nsymbol &lt;- \"DEXCHUS\"\n\n# Use getSymbols() to fetch the data\ngetSymbols(symbol, from = start_date, to = end_date, src = \"FRED\")\n\n# Access the data as a data frame\ndf_exchange_rate &lt;- as.data.frame(DEXCHUS)\n\ndf_exchange_rate &lt;- rownames_to_column(df_exchange_rate, var = \"DATE\")\n\ndf_exchange_rate$DATE &lt;- as.Date(df_exchange_rate$DATE)\n\n# Print the first few rows of the data\nhead(df_exchange_rate)\n\nsummary(df_exchange_rate)\n\n\n'DEXCHUS'\n\n\n\nA data.frame: 6 × 2\n\n\n\nDATE\nDEXCHUS\n\n\n\n&lt;date&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n2010-01-01\nNA\n\n\n2\n2010-01-04\n6.8273\n\n\n3\n2010-01-05\n6.8258\n\n\n4\n2010-01-06\n6.8272\n\n\n5\n2010-01-07\n6.8280\n\n\n6\n2010-01-08\n6.8274\n\n\n\n\n\n      DATE               DEXCHUS     \n Min.   :2010-01-01   Min.   :6.040  \n 1st Qu.:2013-04-02   1st Qu.:6.309  \n Median :2016-07-01   Median :6.504  \n Mean   :2016-07-01   Mean   :6.548  \n 3rd Qu.:2019-10-01   3rd Qu.:6.805  \n Max.   :2022-12-30   Max.   :7.305  \n                      NA's   :140"
  },
  {
    "objectID": "3_data_cleaning R.html#global-lithium-demand",
    "href": "3_data_cleaning R.html#global-lithium-demand",
    "title": "Data Gathering R",
    "section": "Global Lithium Demand",
    "text": "Global Lithium Demand\n\n#df &lt;- read_excel(\"./data/00-raw-data/lithium_price.xlsx\")"
  },
  {
    "objectID": "3_data_cleaning R.html#commodity-price",
    "href": "3_data_cleaning R.html#commodity-price",
    "title": "Data Gathering R",
    "section": "Commodity Price",
    "text": "Commodity Price\n\nUranium\n\n\nData Cleaning Code\ndf_commodity_price &lt;- read_excel(\"../../data/00-raw-data/commodity_price.xlsx\")\n\ndf_commodity_price &lt;- df_commodity_price %&gt;%\n  pivot_longer(cols = -c('...1'), \n               names_to = \"Month_Year\",\n               values_to = \"Price\")\n\ndf_commodity_price &lt;- df_commodity_price %&gt;% filter(!is.na(Price) & Price != \"\")\n\ndf_commodity_price$Month_Year &lt;- as.yearmon(df_commodity_price$Month_Year, format = \"%b %Y\")\n\ndf_commodity_price$Month_Year &lt;- format(df_commodity_price$Month_Year, \"%m-%Y\")\n\ndf_commodity_price$Month_Year &lt;- paste(\"01-\", df_commodity_price$Month_Year, sep = \"\")\n\ndf_commodity_price$Month_Year &lt;- as.Date(df_commodity_price$Month_Year, format = \"%d-%m-%Y\")\n\nnames(df_commodity_price) &lt;- c('Commodity', 'DATE', 'Price')\n\ndf_uranium_price &lt;- df_commodity_price %&gt;%\n  filter(Commodity == \"Uranium\")\n\n#df_uranium_price &lt;- head(df_uranium_price, n = 10)\n\n\n\nhead(df_uranium_price,20)\n\n\nA tibble: 20 × 3\n\n\nCommodity\nDATE\nPrice\n\n\n&lt;chr&gt;\n&lt;date&gt;\n&lt;dbl&gt;\n\n\n\n\nUranium\n2012-01-01\n52.31250\n\n\nUranium\n2012-02-01\n52.05556\n\n\nUranium\n2012-03-01\n51.28889\n\n\nUranium\n2012-04-01\n51.30000\n\n\nUranium\n2012-05-01\n51.88889\n\n\nUranium\n2012-06-01\n50.83333\n\n\nUranium\n2012-07-01\n50.35556\n\n\nUranium\n2012-08-01\n49.25000\n\n\nUranium\n2012-09-01\n47.72500\n\n\nUranium\n2012-10-01\n44.61111\n\n\nUranium\n2012-11-01\n41.50000\n\n\nUranium\n2012-12-01\n43.66667\n\n\nUranium\n2013-01-01\n42.75000\n\n\nUranium\n2013-02-01\n43.40625\n\n\nUranium\n2013-03-01\n42.28125\n\n\nUranium\n2013-04-01\n41.41250\n\n\nUranium\n2013-05-01\n40.60500\n\n\nUranium\n2013-06-01\n39.93750\n\n\nUranium\n2013-07-01\n38.02222\n\n\nUranium\n2013-08-01\n34.99416\n\n\n\n\n\n\n\nNatural Gas\n\n# Set the start and end dates\nstart_date &lt;- \"2012-01-01\"\nend_date &lt;- \"2022-12-31\"\n\n# Define the symbol for Gas Price to USD exchange rate\nsymbol &lt;- \"GASREGCOVW\"\n\n# Use getSymbols() to fetch the data\ngetSymbols(symbol, from = start_date, to = end_date, src = \"FRED\")\n\n# Access the data as a data frame\ndf_gas_price &lt;- as.data.frame(GASREGCOVW)\n\ndf_gas_price &lt;- rownames_to_column(df_gas_price, var = \"DATE\")\n\ndf_gas_price$DATE &lt;- as.Date(df_gas_price$DATE)\n\n# Print the first few rows of the data\nhead(df_gas_price, 10)\n\n'GASREGCOVW'\n\n\n\nA data.frame: 10 × 2\n\n\n\nDATE\nGASREGCOVW\n\n\n\n&lt;date&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n2012-01-02\n3.254\n\n\n2\n2012-01-09\n3.333\n\n\n3\n2012-01-16\n3.342\n\n\n4\n2012-01-23\n3.333\n\n\n5\n2012-01-30\n3.386\n\n\n6\n2012-02-06\n3.436\n\n\n7\n2012-02-13\n3.466\n\n\n8\n2012-02-20\n3.523\n\n\n9\n2012-02-27\n3.641\n\n\n10\n2012-03-05\n3.717\n\n\n\n\n\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    mutate(Year = year(DATE)) %&gt;%\n    mutate(Month = month(DATE)) %&gt;%\n    mutate(Day = day(DATE))\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    group_by(Year, Month) %&gt;%\n    filter(Day == min(Day)) %&gt;%\n    ungroup()\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    select(GASREGCOVW, Year, Month, Day)\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    mutate(DATE = paste(Year, Month, '01', sep = \"-\"))\n\ndf_gas_price$DATE &lt;- as.Date(df_gas_price$DATE)\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    select(GASREGCOVW, DATE)\n\nhead(df_gas_price, 10)\n\n\nA tibble: 10 × 2\n\n\nGASREGCOVW\nDATE\n\n\n&lt;dbl&gt;\n&lt;date&gt;\n\n\n\n\n3.254\n2012-01-01\n\n\n3.436\n2012-02-01\n\n\n3.717\n2012-03-01\n\n\n3.874\n2012-04-01\n\n\n3.718\n2012-05-01\n\n\n3.518\n2012-06-01\n\n\n3.291\n2012-07-01\n\n\n3.606\n2012-08-01\n\n\n3.797\n2012-09-01\n\n\n3.750\n2012-10-01\n\n\n\n\n\n\n\nResources Prices\n\ndf_uranium_price &lt;- df_uranium_price %&gt;% select(DATE, Price)\n\nnames(df_uranium_price) &lt;- c('DATE', 'Uranium')\n\nnames(df_gas_price) &lt;- c('Natural Gas', 'DATE')\n\ndf_resource_price &lt;- merge(df_uranium_price, df_gas_price, by.x = 'DATE', by.y = 'DATE', all = TRUE)\n\ndf_resource_price &lt;- na.omit(df_resource_price)\n\n# head(df_resource_price, 10)\n\ndf_resource_price\n\n\nA data.frame: 132 × 3\n\n\n\nDATE\nUranium\nNatural Gas\n\n\n\n&lt;date&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n2012-01-01\n52.31250\n3.254\n\n\n2\n2012-02-01\n52.05556\n3.436\n\n\n3\n2012-03-01\n51.28889\n3.717\n\n\n4\n2012-04-01\n51.30000\n3.874\n\n\n5\n2012-05-01\n51.88889\n3.718\n\n\n6\n2012-06-01\n50.83333\n3.518\n\n\n7\n2012-07-01\n50.35556\n3.291\n\n\n8\n2012-08-01\n49.25000\n3.606\n\n\n9\n2012-09-01\n47.72500\n3.797\n\n\n10\n2012-10-01\n44.61111\n3.750\n\n\n11\n2012-11-01\n41.50000\n3.406\n\n\n12\n2012-12-01\n43.66667\n3.337\n\n\n13\n2013-01-01\n42.75000\n3.233\n\n\n14\n2013-02-01\n43.40625\n3.471\n\n\n15\n2013-03-01\n42.28125\n3.698\n\n\n16\n2013-04-01\n41.41250\n3.572\n\n\n17\n2013-05-01\n40.60500\n3.478\n\n\n18\n2013-06-01\n39.93750\n3.610\n\n\n19\n2013-07-01\n38.02222\n3.410\n\n\n20\n2013-08-01\n34.99416\n3.566\n\n\n21\n2013-09-01\n34.45329\n3.575\n\n\n22\n2013-10-01\n34.84850\n3.298\n\n\n23\n2013-11-01\n35.59200\n3.209\n\n\n24\n2013-12-01\n34.59330\n3.209\n\n\n25\n2014-01-01\n35.21499\n3.261\n\n\n26\n2014-02-01\n35.54674\n3.243\n\n\n27\n2014-03-01\n34.70286\n3.421\n\n\n28\n2014-04-01\n32.74457\n3.533\n\n\n29\n2014-05-01\n28.53986\n3.606\n\n\n30\n2014-06-01\n28.22546\n3.624\n\n\n...\n...\n...\n...\n\n\n103\n2020-07-01\n32.36087\n2.100\n\n\n104\n2020-08-01\n31.37619\n2.085\n\n\n105\n2020-09-01\n29.98182\n2.122\n\n\n106\n2020-10-01\n29.63636\n2.091\n\n\n107\n2020-11-01\n29.48571\n2.021\n\n\n108\n2020-12-01\n29.77391\n2.063\n\n\n109\n2021-01-01\n29.85714\n2.160\n\n\n110\n2021-02-01\n28.66250\n2.316\n\n\n111\n2021-03-01\n28.33478\n2.625\n\n\n112\n2021-04-01\n29.75227\n2.777\n\n\n113\n2021-05-01\n30.25714\n2.790\n\n\n114\n2021-06-01\n32.14211\n2.935\n\n\n115\n2021-07-01\n32.33864\n3.032\n\n\n116\n2021-08-01\n32.14773\n3.059\n\n\n117\n2021-09-01\n45.07812\n3.080\n\n\n118\n2021-10-01\n38.47619\n3.093\n\n\n119\n2021-11-01\n31.09958\n3.280\n\n\n120\n2021-12-01\n36.12930\n3.204\n\n\n121\n2022-01-01\n36.87292\n3.141\n\n\n122\n2022-02-01\n35.83191\n3.330\n\n\n123\n2022-03-01\n45.51470\n3.963\n\n\n124\n2022-04-01\n48.70473\n4.021\n\n\n125\n2022-05-01\n40.89163\n4.031\n\n\n126\n2022-06-01\n40.33152\n4.702\n\n\n127\n2022-07-01\n38.93916\n4.619\n\n\n128\n2022-08-01\n39.80231\n4.034\n\n\n129\n2022-09-01\n40.95039\n3.617\n\n\n130\n2022-10-01\n41.30360\n3.592\n\n\n131\n2022-11-01\n40.95406\n3.628\n\n\n132\n2022-12-01\n39.17824\n3.260"
  },
  {
    "objectID": "3_data_cleaning/3_2_exchange_rate.html",
    "href": "3_data_cleaning/3_2_exchange_rate.html",
    "title": "Gas Price",
    "section": "",
    "text": "QUANTMOD API | R"
  },
  {
    "objectID": "3_data_cleaning/3_3_lithium_demand.html",
    "href": "3_data_cleaning/3_3_lithium_demand.html",
    "title": "Global Lithium Demand",
    "section": "",
    "text": "Data Cleaning Code\ndf_demand &lt;- read_excel(\"../../../data/00-raw-data/global-lithium-demand.xlsx\", sheet=\"Data\")\n\nnames(df_demand) &lt;- c('Year', 'Demand')\n\ndf_demand &lt;- df_demand %&gt;% filter(!is.na(Demand) & Demand != \"\")\n\ndf_demand$Year &lt;- gsub(\"\\\\*\", \"\", df_demand$Year)\n\ndf_demand$Year &lt;- as.integer(df_demand$Year)\n\nhead(df_demand, n = 10)\n\n\n# A tibble: 10 × 2\n    Year Demand\n   &lt;int&gt;  &lt;dbl&gt;\n 1  2020    310\n 2  2021    508\n 3  2022    690\n 4  2023    917\n 5  2024   1072\n 6  2025   1257\n 7  2026   1433\n 8  2027   1628\n 9  2028   1861\n10  2029   2130\n\n\n\n\n      Year          Demand    \n Min.   :2020   Min.   : 310  \n 1st Qu.:2024   1st Qu.:1033  \n Median :2028   Median :1744  \n Mean   :2028   Mean   :1898  \n 3rd Qu.:2031   3rd Qu.:2749  \n Max.   :2035   Max.   :3829"
  },
  {
    "objectID": "3_data_cleaning/3_4_commodity_price.html",
    "href": "3_data_cleaning/3_4_commodity_price.html",
    "title": "Commodity Price",
    "section": "",
    "text": "Resources Price\n\nBefore cleaning the dataset, the dataset had ### rows and # columns:\n\n\n\n\n\n\n\n\n\nData Gathering\n# Read csv file\ndf_commodity_price &lt;- read_excel(\"../../../data/00-raw-data/commodity_price.xlsx\")\n\n# Original dataset\nknitr::kable(head(df_commodity_price, n = 10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n…1\nJan 2012\n…3\nFeb 2012\n…5\nMar 2012\n…7\nApr 2012\n…9\nMay 2012\n…11\nJun 2012\n…13\nJul 2012\n…15\nAug 2012\n…17\nSep 2012\n…19\nOct 2012\n…21\nNov 2012\n…23\nDec 2012\n…25\nJan 2013\n…27\nFeb 2013\n…29\nMar 2013\n…31\nApr 2013\n…33\nMay 2013\n…35\nJun 2013\n…37\nJul 2013\n…39\nAug 2013\n…41\nSep 2013\n…43\nOct 2013\n…45\nNov 2013\n…47\nDec 2013\n…49\nJan 2014\n…51\nFeb 2014\n…53\nMar 2014\n…55\nApr 2014\n…57\nMay 2014\n…59\nJun 2014\n…61\nJul 2014\n…63\nAug 2014\n…65\nSep 2014\n…67\nOct 2014\n…69\nNov 2014\n…71\nDec 2014\n…73\nJan 2015\n…75\nFeb 2015\n…77\nMar 2015\n…79\nApr 2015\n…81\nMay 2015\n…83\nJun 2015\n…85\nJul 2015\n…87\nAug 2015\n…89\nSep 2015\n…91\nOct 2015\n…93\nNov 2015\n…95\nDec 2015\n…97\nJan 2016\n…99\nFeb 2016\n…101\nMar 2016\n…103\nApr 2016\n…105\nMay 2016\n…107\nJun 2016\n…109\nJul 2016\n…111\nAug 2016\n…113\nSep 2016\n…115\nOct 2016\n…117\nNov 2016\n…119\nDec 2016\n…121\nJan 2017\n…123\nFeb 2017\n…125\nMar 2017\n…127\nApr 2017\n…129\nMay 2017\n…131\nJun 2017\n…133\nJul 2017\n…135\nAug 2017\n…137\nSep 2017\n…139\nOct 2017\n…141\nNov 2017\n…143\nDec 2017\n…145\nJan 2018\n…147\nFeb 2018\n…149\nMar 2018\n…151\nApr 2018\n…153\nMay 2018\n…155\nJun 2018\n…157\nJul 2018\n…159\nAug 2018\n…161\nSep 2018\n…163\nOct 2018\n…165\nNov 2018\n…167\nDec 2018\n…169\nJan 2019\n…171\nFeb 2019\n…173\nMar 2019\n…175\nApr 2019\n…177\nMay 2019\n…179\nJun 2019\n…181\nJul 2019\n…183\nAug 2019\n…185\nSep 2019\n…187\nOct 2019\n…189\nNov 2019\n…191\nDec 2019\n…193\nJan 2020\n…195\nFeb 2020\n…197\nMar 2020\n…199\nApr 2020\n…201\nMay 2020\n…203\nJun 2020\n…205\nJul 2020\n…207\nAug 2020\n…209\nSep 2020\n…211\nOct 2020\n…213\nNov 2020\n…215\nDec 2020\n…217\nJan 2021\n…219\nFeb 2021\n…221\nMar 2021\n…223\nApr 2021\n…225\nMay 2021\n…227\nJun 2021\n…229\nJul 2021\n…231\nAug 2021\n…233\nSep 2021\n…235\nOct 2021\n…237\nNov 2021\n…239\nDec 2021\n…241\nJan 2022\n…243\nFeb 2022\n…245\nMar 2022\n…247\nApr 2022\n…249\nMay 2022\n…251\nJun 2022\n…253\nJul 2022\n…255\nAug 2022\n…257\nSep 2022\n…259\nOct 2022\n…261\nNov 2022\n…263\nDec 2022\n…265\n\n\n\n\nAustralian, export markets\n123.08571\nNA\n124.96684\nNA\n114.150974\nNA\n110.35376\nNA\n103.819480\nNA\n92.754699\nNA\n91.364123\nNA\n93.456331\nNA\n93.181607\nNA\n86.88540\nNA\n87.21964\nNA\n96.076128\nNA\n96.952110\nNA\n100.3473\nNA\n97.316786\nNA\n93.233163\nNA\n93.126020\nNA\n89.64268\nNA\n82.222360\nNA\n82.221429\nNA\n83.295918\nNA\n85.443634\nNA\n88.363265\nNA\n89.444464\nNA\n87.973539\nNA\n83.02768\nNA\n79.753061\nNA\n78.520714\nNA\n78.834643\nNA\n77.348469\nNA\n74.038044\nNA\n74.248393\nNA\n71.525649\nNA\n68.984627\nNA\n67.33071\nNA\n67.162245\nNA\n64.716326\nNA\n70.65911\nNA\n68.344967\nNA\n61.197857\nNA\n65.671241\nNA\n63.104708\nNA\n64.48323\nNA\n63.393214\nNA\n61.765422\nNA\n57.330682\nNA\n55.778061\nNA\n55.865816\nNA\n53.428929\nNA\n53.781633\nNA\n55.435204\nNA\n54.658673\nNA\n54.527143\nNA\n56.228571\nNA\n64.418367\nNA\n71.326948\nNA\n76.373864\nNA\n96.239796\nNA\n111.428084\nNA\n93.134464\nNA\n91.424490\nNA\n85.969286\nNA\n86.333851\nNA\n90.110714\nNA\n80.118367\nNA\n85.673377\nNA\n90.773980\nNA\n102.772403\nNA\n104.775000\nNA\n103.249675\nNA\n102.487013\nNA\n105.968797\nNA\n111.932630\nNA\n111.477321\nNA\n105.728061\nNA\n99.223393\nNA\n109.375000\nNA\n121.524490\nNA\n125.085877\nNA\n123.734903\nNA\n122.22911\nNA\n114.762112\nNA\n107.552922\nNA\n106.143609\nNA\n105.401299\nNA\n101.357143\nNA\n99.669898\nNA\n88.764643\nNA\n89.564286\nNA\n77.629821\nNA\n77.845808\nNA\n69.739286\nNA\n66.958674\nNA\n69.194255\nNA\n69.729082\nNA\n70.464643\nNA\n72.106169\nNA\n71.050179\nNA\n70.204870\nNA\n64.620000\nNA\n54.148308\nNA\n55.002760\nNA\n51.672205\nNA\n51.382500\nNA\n52.005682\nNA\n58.069480\nNA\n62.946429\nNA\n80.589796\nNA\n90.995357\nNA\n92.485714\nNA\n95.230901\nNA\n97.847679\nNA\n104.207143\nNA\n132.152922\nNA\n153.198214\nNA\n173.568367\nNA\n188.122890\nNA\n240.732143\nNA\n177.801623\nNA\n181.473980\nNA\n231.760179\nNA\n272.48679\nNA\n353.440528\nNA\n334.718233\nNA\n418.49388\nNA\n427.425000\nNA\n433.626531\nNA\n447.447565\nNA\n467.78367\nNA\n426.90918\nNA\n368.258279\nNA\n427.686964\nNA\n\n\nSouth African, export markets\n106.04524\nNA\n105.28333\nNA\n103.428182\nNA\n101.15421\nNA\n93.850454\nNA\n84.998421\nNA\n87.164091\nNA\n89.160455\nNA\n86.084500\nNA\n82.71087\nNA\n86.10455\nNA\n89.108947\nNA\n86.392727\nNA\n85.1960\nNA\n82.474500\nNA\n81.907619\nNA\n81.726191\nNA\n77.50100\nNA\n72.790000\nNA\n72.996190\nNA\n73.248095\nNA\n81.746956\nNA\n83.245238\nNA\n84.661000\nNA\n82.825455\nNA\n77.59800\nNA\n74.577143\nNA\n75.442000\nNA\n75.876500\nNA\n74.137143\nNA\n71.463043\nNA\n71.082000\nNA\n67.589545\nNA\n65.782174\nNA\n65.66000\nNA\n65.602857\nNA\n60.187143\nNA\n63.10550\nNA\n60.829091\nNA\n59.641000\nNA\n63.228421\nNA\n60.915455\nNA\n57.02826\nNA\n54.185500\nNA\n51.482273\nNA\n49.825000\nNA\n54.868571\nNA\n49.863810\nNA\n50.668000\nNA\n52.365714\nNA\n53.567143\nNA\n52.868095\nNA\n53.449000\nNA\n58.070000\nNA\n62.680476\nNA\n65.950000\nNA\n67.491818\nNA\n83.600000\nNA\n89.594545\nNA\n82.719000\nNA\n86.087143\nNA\n83.033500\nNA\n78.302609\nNA\n77.305000\nNA\n72.502857\nNA\n78.063182\nNA\n81.690952\nNA\n87.174091\nNA\n91.869048\nNA\n91.262727\nNA\n91.639545\nNA\n94.904737\nNA\n97.381818\nNA\n94.102000\nNA\n91.027143\nNA\n94.398000\nNA\n102.740952\nNA\n104.565714\nNA\n107.569546\nNA\n98.450909\nNA\n98.26700\nNA\n99.337391\nNA\n91.533182\nNA\n95.724737\nNA\n90.365454\nNA\n80.911500\nNA\n74.088571\nNA\n68.701000\nNA\n65.757619\nNA\n63.141000\nNA\n65.379565\nNA\n58.813809\nNA\n59.720476\nNA\n67.588696\nNA\n77.570476\nNA\n83.141500\nNA\n87.266364\nNA\n82.515000\nNA\n65.795455\nNA\n53.397500\nNA\n53.901053\nNA\n54.532273\nNA\n53.459130\nNA\n55.126500\nNA\n57.656818\nNA\n58.776818\nNA\n69.616191\nNA\n90.627143\nNA\n90.285000\nNA\n87.258000\nNA\n94.999565\nNA\n92.479500\nNA\n105.719474\nNA\n115.443182\nNA\n123.615909\nNA\n138.505238\nNA\n162.838182\nNA\n214.269524\nNA\n140.071364\nNA\n137.473333\nNA\n174.546500\nNA\n210.71050\nNA\n326.108696\nNA\n298.866316\nNA\n325.03619\nNA\n327.821000\nNA\n328.681429\nNA\n332.844091\nNA\n291.36810\nNA\n230.95619\nNA\n199.325455\nNA\n206.895500\nNA\n\n\nIndonesian in Japan\n17.46000\nNA\n16.71000\nNA\n18.420000\nNA\n19.57000\nNA\n18.330000\nNA\n19.380000\nNA\n19.100000\nNA\n18.390000\nNA\n18.760000\nNA\n17.27000\nNA\n16.83000\nNA\n17.520000\nNA\n17.790000\nNA\n17.7200\nNA\n18.310000\nNA\n17.710000\nNA\n16.940000\nNA\n17.69000\nNA\n16.980000\nNA\n17.000000\nNA\n17.010000\nNA\n16.490000\nNA\n16.710000\nNA\n17.720000\nNA\n17.760000\nNA\n17.96000\nNA\n17.810000\nNA\n17.670000\nNA\n17.680000\nNA\n17.510000\nNA\n17.430000\nNA\n16.230000\nNA\n15.700000\nNA\n15.230000\nNA\n16.41000\nNA\n16.580000\nNA\n15.500000\nNA\n14.69000\nNA\n13.050000\nNA\n11.020000\nNA\n9.040000\nNA\n8.790000\nNA\n9.32000\nNA\n10.160000\nNA\n10.010000\nNA\n10.370000\nNA\n9.400000\nNA\n10.150000\nNA\n8.460000\nNA\n7.990000\nNA\n8.010000\nNA\n6.680000\nNA\n6.810000\nNA\n7.080000\nNA\n6.850000\nNA\n7.420000\nNA\n7.790000\nNA\n7.930000\nNA\n7.160000\nNA\n7.090000\nNA\n9.174091\nNA\n7.128000\nNA\n5.794783\nNA\n5.709500\nNA\n5.719062\nNA\n5.523636\nNA\n5.631905\nNA\n6.130870\nNA\n7.084286\nNA\n8.792727\nNA\n9.571429\nNA\n10.709048\nNA\n11.669565\nNA\n10.471000\nNA\n8.460909\nNA\n7.475238\nNA\n8.612609\nNA\n10.437619\nNA\n9.972273\nNA\n10.590435\nNA\n11.09050\nNA\n10.035652\nNA\n9.777273\nNA\n8.951905\nNA\n8.285652\nNA\n6.501000\nNA\n5.245238\nNA\n5.079762\nNA\n5.033261\nNA\n4.389250\nNA\n4.419348\nNA\n4.280227\nNA\n5.054524\nNA\n6.017391\nNA\n5.456429\nNA\n5.573409\nNA\n4.913696\nNA\n2.920250\nNA\n3.278864\nNA\n2.126364\nNA\n2.032143\nNA\n2.050454\nNA\n2.286957\nNA\n3.625714\nNA\n4.609773\nNA\n6.121818\nNA\n6.822143\nNA\n11.608913\nNA\n20.415238\nNA\n7.168750\nNA\n6.443478\nNA\n7.894130\nNA\n10.014500\nNA\n12.017273\nNA\n14.096818\nNA\n16.643864\nNA\n24.062955\nNA\n35.065238\nNA\n31.963571\nNA\n37.419783\nNA\n26.828333\nNA\n27.82325\nNA\n38.735870\nNA\n29.014762\nNA\n21.93773\nNA\n29.538182\nNA\n41.169500\nNA\n54.157500\nNA\n44.68227\nNA\n29.74200\nNA\n25.217727\nNA\n30.717143\nNA\n\n\nNetherlands TFF\n12.33000\nNA\n12.22000\nNA\n12.510000\nNA\n12.57000\nNA\n12.570000\nNA\n12.560000\nNA\n11.380000\nNA\n11.400000\nNA\n11.380000\nNA\n11.57000\nNA\n11.64000\nNA\n11.640000\nNA\n11.390000\nNA\n11.3600\nNA\n11.360000\nNA\n11.640000\nNA\n11.410000\nNA\n11.32000\nNA\n10.980000\nNA\n10.970000\nNA\n10.960000\nNA\n10.930000\nNA\n10.960000\nNA\n10.990000\nNA\n10.900000\nNA\n10.83000\nNA\n10.690000\nNA\n10.790000\nNA\n10.640000\nNA\n10.520000\nNA\n9.400000\nNA\n10.380000\nNA\n10.400000\nNA\n10.400000\nNA\n10.16000\nNA\n10.450000\nNA\n9.500000\nNA\n9.29000\nNA\n9.290000\nNA\n7.390000\nNA\n7.370000\nNA\n7.300000\nNA\n6.68000\nNA\n6.660000\nNA\n6.490000\nNA\n6.010000\nNA\n5.870000\nNA\n5.810000\nNA\n5.090000\nNA\n4.790000\nNA\n4.090000\nNA\n4.020000\nNA\n3.990000\nNA\n4.040000\nNA\n4.300000\nNA\n4.250000\nNA\n3.960000\nNA\n4.010000\nNA\n4.540000\nNA\n5.160000\nNA\n6.278140\nNA\n6.100298\nNA\n4.945020\nNA\n5.035127\nNA\n5.077171\nNA\n4.983818\nNA\n5.096028\nNA\n5.536059\nNA\n6.040606\nNA\n5.892519\nNA\n6.734038\nNA\n7.209792\nNA\n6.648303\nNA\n7.841036\nNA\n8.536361\nNA\n7.064733\nNA\n7.460863\nNA\n7.504423\nNA\n7.621063\nNA\n8.104114\nNA\n9.51888\nNA\n8.595915\nNA\n8.204490\nNA\n7.892946\nNA\n7.161065\nNA\n5.986339\nNA\n5.197180\nNA\n4.953250\nNA\n4.360183\nNA\n3.483036\nNA\n3.595944\nNA\n3.270197\nNA\n3.086805\nNA\n3.339724\nNA\n4.781020\nNA\n4.243549\nNA\n3.626270\nNA\n2.995477\nNA\n2.789824\nNA\n2.083882\nNA\n1.462612\nNA\n1.647300\nNA\n1.640863\nNA\n2.611566\nNA\n3.854996\nNA\n4.813822\nNA\n4.783508\nNA\n5.828639\nNA\n7.302821\nNA\n6.171341\nNA\n6.205197\nNA\n7.282724\nNA\n9.004704\nNA\n10.324139\nNA\n12.524247\nNA\n15.279029\nNA\n22.232827\nNA\n29.814143\nNA\n27.383826\nNA\n37.363363\nNA\n27.890945\nNA\n26.98428\nNA\n41.727688\nNA\n31.989874\nNA\n27.46417\nNA\n32.912607\nNA\n51.145913\nNA\n69.977239\nNA\n55.17913\nNA\n20.80664\nNA\n28.790805\nNA\n35.368731\nNA\n\n\nUS, domestic market\n2.67727\nNA\n2.50486\nNA\n2.163636\nNA\n1.94875\nNA\n2.432727\nNA\n2.458571\nNA\n2.954005\nNA\n2.840548\nNA\n2.852105\nNA\n3.31913\nNA\n3.53950\nNA\n3.338235\nNA\n3.338571\nNA\n3.3055\nNA\n3.784286\nNA\n4.161364\nNA\n4.074348\nNA\n3.80600\nNA\n3.643044\nNA\n3.412273\nNA\n3.615714\nNA\n3.655217\nNA\n3.651429\nNA\n4.283636\nNA\n4.528261\nNA\n5.16350\nNA\n4.484762\nNA\n4.614091\nNA\n4.528636\nNA\n4.593333\nNA\n4.041739\nNA\n3.899048\nNA\n3.926364\nNA\n3.801304\nNA\n4.23950\nNA\n3.486522\nNA\n2.936364\nNA\n2.75650\nNA\n2.746364\nNA\n2.596364\nNA\n2.857619\nNA\n2.768636\nNA\n2.80913\nNA\n2.752857\nNA\n2.638182\nNA\n2.384546\nNA\n2.275714\nNA\n2.042174\nNA\n2.232381\nNA\n1.930476\nNA\n1.812609\nNA\n2.014286\nNA\n2.087273\nNA\n2.623636\nNA\n2.772857\nNA\n2.722174\nNA\n2.898636\nNA\n3.072381\nNA\n2.879091\nNA\n3.585454\nNA\n3.315909\nNA\n2.901500\nNA\n2.991739\nNA\n3.191500\nNA\n3.235652\nNA\n2.993636\nNA\n2.954286\nNA\n2.903913\nNA\n3.009048\nNA\n2.911818\nNA\n3.053636\nNA\n2.769524\nNA\n3.151739\nNA\n2.658000\nNA\n2.700909\nNA\n2.723810\nNA\n2.833913\nNA\n2.941429\nNA\n2.793182\nNA\n2.908261\nNA\n2.89750\nNA\n3.207391\nNA\n4.108182\nNA\n3.904286\nNA\n3.108696\nNA\n2.679500\nNA\n2.803333\nNA\n2.597273\nNA\n2.591739\nNA\n2.330000\nNA\n2.302174\nNA\n2.173636\nNA\n2.507143\nNA\n2.340870\nNA\n2.625238\nNA\n2.282727\nNA\n2.033913\nNA\n1.845000\nNA\n1.733182\nNA\n1.763636\nNA\n1.809048\nNA\n1.699545\nNA\n1.764783\nNA\n2.340476\nNA\n2.285000\nNA\n2.829091\nNA\n2.870476\nNA\n2.584783\nNA\n2.649048\nNA\n2.917000\nNA\n2.622174\nNA\n2.683636\nNA\n2.961429\nNA\n3.272727\nNA\n3.809546\nNA\n4.032273\nNA\n5.096818\nNA\n5.571429\nNA\n5.119091\nNA\n3.858261\nNA\n4.233333\nNA\n4.47500\nNA\n4.971304\nNA\n6.744762\nNA\n8.16000\nNA\n7.570909\nNA\n7.109048\nNA\n8.777391\nNA\n7.83500\nNA\n6.09381\nNA\n6.478182\nNA\n5.746818\nNA\n\n\nDubai\n108.60455\nNA\n116.67619\nNA\n122.598636\nNA\n117.49714\nNA\n107.650870\nNA\n94.438095\nNA\n99.964545\nNA\n108.954783\nNA\n110.853500\nNA\n108.42696\nNA\n107.10545\nNA\n106.397619\nNA\n108.186522\nNA\n111.1215\nNA\n105.704286\nNA\n101.495000\nNA\n100.521739\nNA\n100.38550\nNA\n103.828696\nNA\n107.071364\nNA\n108.073809\nNA\n106.866087\nNA\n106.060000\nNA\n107.932273\nNA\n103.767826\nNA\n105.25850\nNA\n104.344286\nNA\n104.820909\nNA\n105.534091\nNA\n108.168095\nNA\n105.957826\nNA\n101.608095\nNA\n96.380000\nNA\n86.377826\nNA\n76.11950\nNA\n59.825652\nNA\n46.492273\nNA\n55.92000\nNA\n54.316818\nNA\n58.433636\nNA\n63.178095\nNA\n61.797273\nNA\n55.98739\nNA\n47.980952\nNA\n45.191364\nNA\n45.859546\nNA\n41.597143\nNA\n34.138261\nNA\n27.458095\nNA\n29.672857\nNA\n35.450000\nNA\n39.323810\nNA\n44.350000\nNA\n46.315909\nNA\n42.444762\nNA\n43.969130\nNA\n43.676364\nNA\n48.802381\nNA\n44.039546\nNA\n52.136818\nNA\n53.569091\nNA\n54.295000\nNA\n51.021304\nNA\n52.554500\nNA\n50.134348\nNA\n46.356364\nNA\n47.792381\nNA\n50.269565\nNA\n53.922857\nNA\n55.774546\nNA\n60.586818\nNA\n61.384286\nNA\n66.024783\nNA\n62.631000\nNA\n63.917273\nNA\n68.667143\nNA\n74.077826\nNA\n73.311429\nNA\n72.769546\nNA\n72.594783\nNA\n76.92500\nNA\n78.762609\nNA\n64.685909\nNA\n55.669048\nNA\n59.159130\nNA\n64.548000\nNA\n66.952381\nNA\n70.939545\nNA\n68.667391\nNA\n60.915000\nNA\n62.860870\nNA\n58.504091\nNA\n60.856190\nNA\n58.863478\nNA\n61.220000\nNA\n64.453182\nNA\n63.125217\nNA\n54.048500\nNA\n33.509091\nNA\n26.192727\nNA\n32.602857\nNA\n40.068636\nNA\n42.533913\nNA\n43.917143\nNA\n41.373182\nNA\n40.814546\nNA\n43.473810\nNA\n49.626956\nNA\n54.646667\nNA\n61.250500\nNA\n64.287826\nNA\n63.128636\nNA\n66.350000\nNA\n71.303636\nNA\n72.136818\nNA\n68.731364\nNA\n72.758182\nNA\n81.437143\nNA\n78.657727\nNA\n72.847826\nNA\n83.450952\nNA\n91.52500\nNA\n107.587826\nNA\n102.755238\nNA\n107.86136\nNA\n111.859545\nNA\n100.318571\nNA\n95.693044\nNA\n89.74273\nNA\n91.21476\nNA\n84.599091\nNA\n76.855909\nNA\n\n\nU.K. Brent\n111.41500\nNA\n119.16571\nNA\n124.703182\nNA\n120.47000\nNA\n110.822174\nNA\n95.889048\nNA\n102.979546\nNA\n113.780435\nNA\n113.641500\nNA\n111.79261\nNA\n109.91182\nNA\n109.569048\nNA\n112.527826\nNA\n116.4880\nNA\n109.585714\nNA\n102.960000\nNA\n102.980435\nNA\n103.35850\nNA\n107.779565\nNA\n111.058182\nNA\n111.962857\nNA\n109.618696\nNA\n108.150476\nNA\n111.070454\nNA\n107.926957\nNA\n108.83800\nNA\n107.868095\nNA\n108.003636\nNA\n109.832273\nNA\n112.240952\nNA\n107.462609\nNA\n102.401905\nNA\n97.852727\nNA\n87.584783\nNA\n79.22950\nNA\n62.903913\nNA\n48.933182\nNA\n58.25400\nNA\n56.524546\nNA\n59.826364\nNA\n65.193810\nNA\n62.620000\nNA\n56.82957\nNA\n47.533809\nNA\n48.030454\nNA\n48.906364\nNA\n45.095238\nNA\n38.525217\nNA\n32.045238\nNA\n33.762381\nNA\n39.792174\nNA\n43.330476\nNA\n47.735000\nNA\n49.877273\nNA\n46.598571\nNA\n47.052174\nNA\n47.376818\nNA\n51.413809\nNA\n46.942273\nNA\n55.022727\nNA\n55.710909\nNA\n56.096000\nNA\n52.655217\nNA\n53.978000\nNA\n51.374348\nNA\n47.652727\nNA\n49.253333\nNA\n51.927826\nNA\n55.557619\nNA\n57.536364\nNA\n62.836364\nNA\n64.064286\nNA\n68.889130\nNA\n65.696500\nNA\n66.891818\nNA\n71.932857\nNA\n76.934348\nNA\n75.802857\nNA\n75.229091\nNA\n73.851304\nNA\n79.16300\nNA\n80.783478\nNA\n66.228636\nNA\n57.948095\nNA\n59.944783\nNA\n64.384000\nNA\n66.941905\nNA\n71.475909\nNA\n70.389130\nNA\n63.208500\nNA\n64.322174\nNA\n59.618182\nNA\n62.430952\nNA\n59.631304\nNA\n62.686667\nNA\n65.348182\nNA\n63.898696\nNA\n55.691000\nNA\n33.947273\nNA\n26.848636\nNA\n32.422381\nNA\n40.860455\nNA\n43.295652\nNA\n45.081429\nNA\n41.926818\nNA\n41.606818\nNA\n44.050000\nNA\n50.376956\nNA\n55.224762\nNA\n62.362500\nNA\n65.796087\nNA\n65.531818\nNA\n68.375238\nNA\n73.512727\nNA\n74.403182\nNA\n70.588182\nNA\n74.749545\nNA\n83.865238\nNA\n80.890455\nNA\n74.678261\nNA\n85.622381\nNA\n94.26700\nNA\n112.440000\nNA\n106.155714\nNA\n112.11364\nNA\n117.692727\nNA\n105.252857\nNA\n97.643044\nNA\n90.60818\nNA\n93.71857\nNA\n90.938636\nNA\n81.503182\nNA\n\n\nWest Texas Intermediate\n100.10227\nNA\n102.29714\nNA\n106.187273\nNA\n103.32571\nNA\n94.534783\nNA\n82.400476\nNA\n87.919091\nNA\n94.118261\nNA\n94.704500\nNA\n89.59652\nNA\n86.72091\nNA\n88.262857\nNA\n94.729565\nNA\n95.3095\nNA\n93.199048\nNA\n92.067727\nNA\n94.766957\nNA\n95.76950\nNA\n104.426522\nNA\n106.435455\nNA\n106.395238\nNA\n100.556956\nNA\n93.850952\nNA\n97.890455\nNA\n94.963043\nNA\n100.70500\nNA\n100.568095\nNA\n102.175000\nNA\n101.996364\nNA\n105.234762\nNA\n102.948261\nNA\n96.317143\nNA\n93.271818\nNA\n84.406087\nNA\n75.70750\nNA\n59.139565\nNA\n47.557273\nNA\n50.85700\nNA\n47.782273\nNA\n54.378636\nNA\n59.388571\nNA\n59.828636\nNA\n51.19870\nNA\n42.912857\nNA\n45.510000\nNA\n46.268636\nNA\n42.585238\nNA\n37.396087\nNA\n31.784286\nNA\n30.377619\nNA\n37.901739\nNA\n41.031905\nNA\n46.841818\nNA\n48.791818\nNA\n44.896191\nNA\n44.751739\nNA\n45.200000\nNA\n49.809524\nNA\n45.470909\nNA\n52.052727\nNA\n52.561818\nNA\n53.450000\nNA\n49.363044\nNA\n51.174000\nNA\n48.559565\nNA\n45.185454\nNA\n46.581905\nNA\n48.047826\nNA\n49.735714\nNA\n51.573182\nNA\n56.738182\nNA\n57.920476\nNA\n63.584783\nNA\n62.227000\nNA\n62.830455\nNA\n66.313810\nNA\n69.898696\nNA\n67.876191\nNA\n71.074545\nNA\n67.927391\nNA\n70.19050\nNA\n70.753044\nNA\n56.188182\nNA\n48.919048\nNA\n51.227391\nNA\n54.996000\nNA\n58.170476\nNA\n63.870455\nNA\n60.740000\nNA\n54.666000\nNA\n57.376087\nNA\n54.830455\nNA\n56.860000\nNA\n53.960870\nNA\n56.677619\nNA\n59.867727\nNA\n57.716957\nNA\n50.608000\nNA\n29.320455\nNA\n16.975000\nNA\n28.781429\nNA\n38.314546\nNA\n40.715217\nNA\n42.370952\nNA\n39.598182\nNA\n39.405000\nNA\n41.389048\nNA\n47.070435\nNA\n51.849524\nNA\n59.234500\nNA\n62.183913\nNA\n61.417727\nNA\n65.160952\nNA\n71.377273\nNA\n72.591818\nNA\n67.872273\nNA\n71.535000\nNA\n81.364286\nNA\n79.095455\nNA\n71.802174\nNA\n83.277619\nNA\n91.60800\nNA\n108.399130\nNA\n101.985238\nNA\n109.71273\nNA\n114.675909\nNA\n101.918095\nNA\n93.692609\nNA\n84.40000\nNA\n87.28667\nNA\n84.076364\nNA\n76.581818\nNA\n\n\nAgricultural raw materials\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nCotton\n101.11429\nNA\n100.74524\nNA\n99.504546\nNA\n100.09737\nNA\n88.534091\nNA\n82.181579\nNA\n83.968182\nNA\n84.397727\nNA\n84.150000\nNA\n81.95217\nNA\n80.87273\nNA\n83.373684\nNA\n85.506818\nNA\n89.7100\nNA\n94.447500\nNA\n92.535000\nNA\n92.622727\nNA\n93.08000\nNA\n92.615217\nNA\n92.714286\nNA\n90.092857\nNA\n89.347826\nNA\n84.647619\nNA\n87.487500\nNA\n90.963636\nNA\n94.05000\nNA\n96.947619\nNA\n94.202500\nNA\n92.712500\nNA\n90.897619\nNA\n83.836956\nNA\n73.995000\nNA\n73.381818\nNA\n70.343478\nNA\n67.52500\nNA\n68.304762\nNA\n67.350000\nNA\n69.84250\nNA\n69.352273\nNA\n71.702500\nNA\n72.863158\nNA\n72.352273\nNA\n72.34783\nNA\n71.822500\nNA\n68.736364\nNA\n69.027273\nNA\n69.221429\nNA\n70.388095\nNA\n68.750000\nNA\n66.571429\nNA\n65.457143\nNA\n69.278571\nNA\n70.277500\nNA\n74.102273\nNA\n81.064286\nNA\n80.261364\nNA\n77.861364\nNA\n78.516667\nNA\n78.922727\nNA\n79.502500\nNA\n82.330952\nNA\n85.155000\nNA\n86.782609\nNA\n87.036111\nNA\n88.638095\nNA\n84.763636\nNA\n84.088095\nNA\n79.340909\nNA\n80.604762\nNA\n78.604546\nNA\n80.411364\nNA\n85.423684\nNA\n91.056818\nNA\n88.267500\nNA\n92.135714\nNA\n92.237500\nNA\n94.478571\nNA\n97.707143\nNA\n96.179546\nNA\n94.552273\nNA\n90.35500\nNA\n86.800000\nNA\n86.775000\nNA\n85.997368\nNA\n82.354546\nNA\n81.150000\nNA\n83.809524\nNA\n87.247500\nNA\n80.138095\nNA\n77.650000\nNA\n75.539130\nNA\n70.776190\nNA\n71.311905\nNA\n73.884783\nNA\n74.857143\nNA\n75.830000\nNA\n79.068182\nNA\n76.567500\nNA\n67.686364\nNA\n63.532500\nNA\n65.702632\nNA\n67.800000\nNA\n68.523913\nNA\n70.000000\nNA\n70.813636\nNA\n74.815909\nNA\n77.723810\nNA\n81.021429\nNA\n87.235000\nNA\n92.760000\nNA\n91.452174\nNA\n90.730000\nNA\n90.886842\nNA\n94.504546\nNA\n97.700000\nNA\n101.304762\nNA\n103.681818\nNA\n117.383333\nNA\n126.543182\nNA\n120.040476\nNA\n132.332500\nNA\n138.40500\nNA\n141.126087\nNA\n155.315789\nNA\n163.97955\nNA\n154.445455\nNA\n131.035714\nNA\n124.758696\nNA\n117.94091\nNA\n100.27619\nNA\n101.165909\nNA\n100.886364\nNA\n\n\n\n\n\nDimensions:\n\n\n[1]  85 265\n\n\n\nData Cleaning Description.\n\n\n\nData Cleaning Code\ndf_commodity_price &lt;- read_excel(\"../../../data/00-raw-data/commodity_price.xlsx\")\n\ndf_commodity_price &lt;- df_commodity_price %&gt;%\n  pivot_longer(cols = -c('...1'), \n               names_to = \"Month_Year\",\n               values_to = \"Price\")\n\ndf_commodity_price &lt;- df_commodity_price %&gt;% filter(!is.na(Price) & Price != \"\")\n\ndf_commodity_price$Month_Year &lt;- as.yearmon(df_commodity_price$Month_Year, format = \"%b %Y\")\n\ndf_commodity_price$Month_Year &lt;- format(df_commodity_price$Month_Year, \"%m-%Y\")\n\ndf_commodity_price$Month_Year &lt;- paste(\"01-\", df_commodity_price$Month_Year, sep = \"\")\n\ndf_commodity_price$Month_Year &lt;- as.Date(df_commodity_price$Month_Year, format = \"%d-%m-%Y\")\n\nnames(df_commodity_price) &lt;- c('Commodity', 'DATE', 'Price')\n\n\nCleaned Data:\n\nData Cleaning Description.\n\n\n\nData Cleaning\ndf_commodity_price &lt;- df_commodity_price %&gt;% filter(Commodity %in% c(\"Lithium\", \"Aluminum\", \"Cobalt\", \"Copper\", \"Nickel\", \"Zinc\"))\n\ndf_commodity_price &lt;- pivot_wider(df_commodity_price, id_cols = DATE, names_from = Commodity, values_from = Price)\n\n#df_commodity_price &lt;- df_commodity_price %&gt;% select(Commodity, DATE, Price)\n\n#names(df_commodity_price) &lt;- c('DATE', 'Price')\n\ndf_commodity_price &lt;- na.omit(df_commodity_price)\n\n\nCleaned Data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDATE\nAluminum\nCobalt\nCopper\nNickel\nZinc\nLithium\n\n\n\n\n2022-03-01\n3498.373\n80457.43\n10230.894\n33924.18\n3962.207\n473185.9\n\n\n2022-04-01\n3246.990\n81790.38\n10174.348\n33133.81\n4371.343\n490008.2\n\n\n2022-05-01\n2839.505\n77721.36\n9395.027\n28228.89\n3769.652\n453228.9\n\n\n2022-06-01\n2575.670\n72057.73\n9067.552\n25877.07\n3652.891\n443656.2\n\n\n2022-07-01\n2408.423\n55713.24\n7544.810\n21481.89\n3105.362\n441052.3\n\n\n2022-08-01\n2433.916\n49308.83\n7990.812\n22034.89\n3590.131\n435305.3\n\n\n2022-09-01\n2224.756\n51515.41\n7746.011\n22773.97\n3124.968\n418813.5\n\n\n2022-10-01\n2255.535\n51509.24\n7651.083\n22032.89\n2967.210\n409005.5\n\n\n2022-11-01\n2350.716\n51504.59\n8049.861\n25562.70\n2938.920\n409954.0\n\n\n2022-12-01\n2398.207\n51497.77\n8371.091\n28985.92\n3116.523\n421882.5\n\n\n\n\n\nSummary:\n\n\n      DATE               Aluminum        Cobalt          Copper     \n Min.   :2012-06-01   Min.   :1460   Min.   :22276   Min.   : 4472  \n 1st Qu.:2015-01-16   1st Qu.:1742   1st Qu.:28745   1st Qu.: 5828  \n Median :2017-09-01   Median :1871   Median :32100   Median : 6713  \n Mean   :2017-08-31   Mean   :1973   Mean   :41195   Mean   : 6816  \n 3rd Qu.:2020-04-16   3rd Qu.:2081   3rd Qu.:52590   3rd Qu.: 7565  \n Max.   :2022-12-01   Max.   :3498   Max.   :90782   Max.   :10231  \n     Nickel           Zinc         Lithium      \n Min.   : 8298   Min.   :1520   Min.   : 62377  \n 1st Qu.:11664   1st Qu.:2026   1st Qu.: 66030  \n Median :14101   Median :2414   Median : 87823  \n Mean   :14992   Mean   :2488   Mean   :122254  \n 3rd Qu.:17331   3rd Qu.:2892   3rd Qu.:120041  \n Max.   :33924   Max.   :4371   Max.   :490008  \n\n\nDimensions:\n\n\n[1] 127   7"
  },
  {
    "objectID": "3_data_cleaning/3_1_lithium_production.html",
    "href": "3_data_cleaning/3_1_lithium_production.html",
    "title": "Global Lithium Production",
    "section": "",
    "text": "Before cleaning the dataset, the dataset had 532 rows and 4 columns:\n\nEntity that represents the region/country of the information\nCode of the Entity\nYear the information was collected\nLithium production measured in tons.\n\n\n\n\nData Gathering\n# Read csv file\ndf_production &lt;- read.csv(\"../../../data/00-raw-data/lithium-production.csv\")\n\n# Original dataset\nknitr::kable(head(df_production, n = 10))\n\n\n\n\n\nEntity\nCode\nYear\nLithium.production…kt\n\n\n\n\nAfrica\n\n1995\n520\n\n\nAfrica\n\n1996\n500\n\n\nAfrica\n\n1997\n700\n\n\nAfrica\n\n1998\n1000\n\n\nAfrica\n\n1999\n700\n\n\nAfrica\n\n2000\n740\n\n\nAfrica\n\n2001\n700\n\n\nAfrica\n\n2002\n640\n\n\nAfrica\n\n2003\n480\n\n\nAfrica\n\n2004\n240\n\n\n\n\n\nDimensions:\n\n\n[1] 532   4\n\n\n\nThe data cleansing step performed on this dataset was to filter the rows to remove the nan from the Code column. This allows us to keep only the lithium production records for the countries in the dataset, removing the global and regional totals. In the next section we will plot the information and compare lithium production between countries over time.\nThe cleaned dataset has 224 rows and 4 columns. The available information was collected from 1995 to 2022.\n\n\n\nData Cleaning\n# Filter code column \ndf_production &lt;- df_production %&gt;% filter(nchar(Code) == 3)\n\n# Change column names\nnames(df_production) &lt;- c('Country', 'Code', 'Year', 'Production')\n\n\nCleaned Data:\n\n\n\n\n\nCountry\nCode\nYear\nProduction\n\n\n\n\nArgentina\nARG\n1995\n8\n\n\nArgentina\nARG\n1996\n8\n\n\nArgentina\nARG\n1997\n8\n\n\nArgentina\nARG\n1998\n1130\n\n\nArgentina\nARG\n1999\n200\n\n\nArgentina\nARG\n2000\n200\n\n\nArgentina\nARG\n2001\n200\n\n\nArgentina\nARG\n2002\n946\n\n\nArgentina\nARG\n2003\n960\n\n\nArgentina\nARG\n2004\n1970\n\n\n\n\n\nSummary:\n\n\n   Country              Code                Year        Production   \n Length:224         Length:224         Min.   :1995   Min.   :    8  \n Class :character   Class :character   1st Qu.:2002   1st Qu.:  495  \n Mode  :character   Mode  :character   Median :2008   Median : 1500  \n                                       Mean   :2008   Mean   : 4591  \n                                       3rd Qu.:2015   3rd Qu.: 4735  \n                                       Max.   :2022   Max.   :61000  \n\n\nDimensions:\n\n\n[1] 224   4"
  },
  {
    "objectID": "4_eda/4_1_lithium_production.html",
    "href": "4_eda/4_1_lithium_production.html",
    "title": "Global Lithium Production",
    "section": "",
    "text": "Before cleaning the dataset, the dataset had 532 rows and 4 columns:\n\nEntity that represents the region/country of the information\nCode of the Entity\nYear the information was collected\nLithium production measured in tons.\n\n\n\n\nData Gathering\n# Read csv file\ndf_production &lt;- read.csv(\"../../../data/00-raw-data/lithium-production.csv\")\n\n# Original dataset\nhead(df_production, n = 10)\n\n\n   Entity Code Year Lithium.production...kt\n1  Africa      1995                     520\n2  Africa      1996                     500\n3  Africa      1997                     700\n4  Africa      1998                    1000\n5  Africa      1999                     700\n6  Africa      2000                     740\n7  Africa      2001                     700\n8  Africa      2002                     640\n9  Africa      2003                     480\n10 Africa      2004                     240\n\n\nDimensions:\n\n\n[1] 532   4\n\n\n\nThe data cleansing step performed on this dataset was to filter the rows to remove the nan from the Code column. This allows us to keep only the lithium production records for the countries in the dataset, removing the global and regional totals. In the next section we will plot the information and compare lithium production between countries over time.\nThe cleaned dataset has 224 rows and 4 columns. The available information was collected from 1995 to 2022.\n\n\n\nData Cleaning\n# Filter Code Column \ndf_production &lt;- df_production %&gt;% filter(nchar(Code) == 3)\n\n\nCleaned Data:\n\n\n      Entity Code Year Lithium.production...kt\n1  Argentina  ARG 1995                       8\n2  Argentina  ARG 1996                       8\n3  Argentina  ARG 1997                       8\n4  Argentina  ARG 1998                    1130\n5  Argentina  ARG 1999                     200\n6  Argentina  ARG 2000                     200\n7  Argentina  ARG 2001                     200\n8  Argentina  ARG 2002                     946\n9  Argentina  ARG 2003                     960\n10 Argentina  ARG 2004                    1970\n\n\nSummary:\n\n\n    Entity              Code                Year      Lithium.production...kt\n Length:224         Length:224         Min.   :1995   Min.   :    8          \n Class :character   Class :character   1st Qu.:2002   1st Qu.:  495          \n Mode  :character   Mode  :character   Median :2008   Median : 1500          \n                                       Mean   :2008   Mean   : 4591          \n                                       3rd Qu.:2015   3rd Qu.: 4735          \n                                       Max.   :2022   Max.   :61000          \n\n\nDimensions:\n\n\n[1] 224   4"
  },
  {
    "objectID": "4_eda/eda.html",
    "href": "4_eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Build out your website tab for exploratory data analysis"
  },
  {
    "objectID": "4_eda/eda.html#quick-look-at-the-data",
    "href": "4_eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]"
  },
  {
    "objectID": "4_eda/eda.html#basic-visualization",
    "href": "4_eda/eda.html#basic-visualization",
    "title": "Data Exploration",
    "section": "Basic visualization",
    "text": "Basic visualization\n\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nplt.show()"
  },
  {
    "objectID": "4_EDA_df_resources_prices.html",
    "href": "4_EDA_df_resources_prices.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Read csv file\ndf_resources &lt;- read.csv(\"../../data/01-modified-data/clean_resources-price.csv\")\n\nnames(df_resources) &lt;- c(\"DATE\", \"Uranium\", \"Natural Gas\")\n\ndf_resources$DATE &lt;- as.Date(df_resources$DATE)\n\n\ndf_resources &lt;- pivot_longer(df_resources, \n                        cols = -DATE,  # Replace 'ID' with the name of your ID column\n                        names_to = \"Resources\",  # Name for the new variable column\n                        values_to = \"Prices\")    # Name for the new value column\n\ndf_resources\n\n\nA tibble: 264 × 3\n\n\nDATE\nResources\nPrices\n\n\n&lt;date&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n2012-01-01\nUranium\n52.31250\n\n\n2012-01-01\nNatural Gas\n3.25400\n\n\n2012-02-01\nUranium\n52.05556\n\n\n2012-02-01\nNatural Gas\n3.43600\n\n\n2012-03-01\nUranium\n51.28889\n\n\n2012-03-01\nNatural Gas\n3.71700\n\n\n2012-04-01\nUranium\n51.30000\n\n\n2012-04-01\nNatural Gas\n3.87400\n\n\n2012-05-01\nUranium\n51.88889\n\n\n2012-05-01\nNatural Gas\n3.71800\n\n\n2012-06-01\nUranium\n50.83333\n\n\n2012-06-01\nNatural Gas\n3.51800\n\n\n2012-07-01\nUranium\n50.35556\n\n\n2012-07-01\nNatural Gas\n3.29100\n\n\n2012-08-01\nUranium\n49.25000\n\n\n2012-08-01\nNatural Gas\n3.60600\n\n\n2012-09-01\nUranium\n47.72500\n\n\n2012-09-01\nNatural Gas\n3.79700\n\n\n2012-10-01\nUranium\n44.61111\n\n\n2012-10-01\nNatural Gas\n3.75000\n\n\n2012-11-01\nUranium\n41.50000\n\n\n2012-11-01\nNatural Gas\n3.40600\n\n\n2012-12-01\nUranium\n43.66667\n\n\n2012-12-01\nNatural Gas\n3.33700\n\n\n2013-01-01\nUranium\n42.75000\n\n\n2013-01-01\nNatural Gas\n3.23300\n\n\n2013-02-01\nUranium\n43.40625\n\n\n2013-02-01\nNatural Gas\n3.47100\n\n\n2013-03-01\nUranium\n42.28125\n\n\n2013-03-01\nNatural Gas\n3.69800\n\n\n...\n...\n...\n\n\n2021-10-01\nUranium\n38.47619\n\n\n2021-10-01\nNatural Gas\n3.09300\n\n\n2021-11-01\nUranium\n31.09958\n\n\n2021-11-01\nNatural Gas\n3.28000\n\n\n2021-12-01\nUranium\n36.12930\n\n\n2021-12-01\nNatural Gas\n3.20400\n\n\n2022-01-01\nUranium\n36.87292\n\n\n2022-01-01\nNatural Gas\n3.14100\n\n\n2022-02-01\nUranium\n35.83191\n\n\n2022-02-01\nNatural Gas\n3.33000\n\n\n2022-03-01\nUranium\n45.51470\n\n\n2022-03-01\nNatural Gas\n3.96300\n\n\n2022-04-01\nUranium\n48.70473\n\n\n2022-04-01\nNatural Gas\n4.02100\n\n\n2022-05-01\nUranium\n40.89163\n\n\n2022-05-01\nNatural Gas\n4.03100\n\n\n2022-06-01\nUranium\n40.33152\n\n\n2022-06-01\nNatural Gas\n4.70200\n\n\n2022-07-01\nUranium\n38.93916\n\n\n2022-07-01\nNatural Gas\n4.61900\n\n\n2022-08-01\nUranium\n39.80231\n\n\n2022-08-01\nNatural Gas\n4.03400\n\n\n2022-09-01\nUranium\n40.95039\n\n\n2022-09-01\nNatural Gas\n3.61700\n\n\n2022-10-01\nUranium\n41.30360\n\n\n2022-10-01\nNatural Gas\n3.59200\n\n\n2022-11-01\nUranium\n40.95406\n\n\n2022-11-01\nNatural Gas\n3.62800\n\n\n2022-12-01\nUranium\n39.17824\n\n\n2022-12-01\nNatural Gas\n3.26000\n\n\n\n\n\n\nuranium &lt;- ggplot(df_resources[df_resources$Resources == \"Uranium\",]) +\n                geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n                labs(x = \"Price\", y = \"Frequency\", title = \"Resource Distributions\", subtitle = \"Uranium\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nnatural_gas &lt;- ggplot(df_resources[df_resources$Resources == \"Natural Gas\",]) +\n                geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n                labs(x = \"Price\", y = \"Frequency\", subtitle = \"Natural Gas\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nplots &lt;- wrap_plots(uranium, natural_gas, nrow = 2)\n\nplots\n\n\n\n\n\nlibrary(patchwork)\n\nplot_list &lt;- lapply(unique(df_resources$Resources), function(resource) {\n  ggplot(df_resources[df_resources$Resources == resource, ]) +\n    geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n    labs(x = \"Price\", y = \"Frequency\", title = resource) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n      axis.title.x = element_text(size = 10),\n      axis.title.y = element_text(size = 10),\n      plot.margin = margin(1, 1, 1, 1, \"cm\")\n    )\n})\n\n# Arrange the plots in a grid\ngrid_plot &lt;- wrap_plots(plotlist = plot_list, nrow = 2)\n\n# Print the grid of histograms\ngrid_plot\n\n\n\n\n\nuranium &lt;- ggplot(df_resources[df_resources$Resources == \"Uranium\",]) +\n                geom_boxplot(aes(y = Prices)) +\n                labs(x = \"\", y = \"Price\", title = \"Resource Distributions\", subtitle = \"Uranium\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nggplotly(uranium)\n\n\n\n    \n        \n        \n\n\n\n\n\n\n\n    \n    \n        \n\n    \n\n\n\n\np &lt;- ggplot(df_resources) +\n        geom_line(aes(x = DATE, y = Prices), color = \"darkred\") + \n        labs(x = \"Date\", y = \"Price\", title = \"Resources Prices\") +\n        theme_minimal() +\n        theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\"))+\n        facet_grid(Resources ~ ., scales = \"free\")    \n\nggplotly(p, width = 800, height = 500)"
  },
  {
    "objectID": "4_EDA_df_resources_prices.html#df_resources_prices",
    "href": "4_EDA_df_resources_prices.html#df_resources_prices",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Read csv file\ndf_resources &lt;- read.csv(\"../../data/01-modified-data/clean_resources-price.csv\")\n\nnames(df_resources) &lt;- c(\"DATE\", \"Uranium\", \"Natural Gas\")\n\ndf_resources$DATE &lt;- as.Date(df_resources$DATE)\n\n\ndf_resources &lt;- pivot_longer(df_resources, \n                        cols = -DATE,  # Replace 'ID' with the name of your ID column\n                        names_to = \"Resources\",  # Name for the new variable column\n                        values_to = \"Prices\")    # Name for the new value column\n\ndf_resources\n\n\nA tibble: 264 × 3\n\n\nDATE\nResources\nPrices\n\n\n&lt;date&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n2012-01-01\nUranium\n52.31250\n\n\n2012-01-01\nNatural Gas\n3.25400\n\n\n2012-02-01\nUranium\n52.05556\n\n\n2012-02-01\nNatural Gas\n3.43600\n\n\n2012-03-01\nUranium\n51.28889\n\n\n2012-03-01\nNatural Gas\n3.71700\n\n\n2012-04-01\nUranium\n51.30000\n\n\n2012-04-01\nNatural Gas\n3.87400\n\n\n2012-05-01\nUranium\n51.88889\n\n\n2012-05-01\nNatural Gas\n3.71800\n\n\n2012-06-01\nUranium\n50.83333\n\n\n2012-06-01\nNatural Gas\n3.51800\n\n\n2012-07-01\nUranium\n50.35556\n\n\n2012-07-01\nNatural Gas\n3.29100\n\n\n2012-08-01\nUranium\n49.25000\n\n\n2012-08-01\nNatural Gas\n3.60600\n\n\n2012-09-01\nUranium\n47.72500\n\n\n2012-09-01\nNatural Gas\n3.79700\n\n\n2012-10-01\nUranium\n44.61111\n\n\n2012-10-01\nNatural Gas\n3.75000\n\n\n2012-11-01\nUranium\n41.50000\n\n\n2012-11-01\nNatural Gas\n3.40600\n\n\n2012-12-01\nUranium\n43.66667\n\n\n2012-12-01\nNatural Gas\n3.33700\n\n\n2013-01-01\nUranium\n42.75000\n\n\n2013-01-01\nNatural Gas\n3.23300\n\n\n2013-02-01\nUranium\n43.40625\n\n\n2013-02-01\nNatural Gas\n3.47100\n\n\n2013-03-01\nUranium\n42.28125\n\n\n2013-03-01\nNatural Gas\n3.69800\n\n\n...\n...\n...\n\n\n2021-10-01\nUranium\n38.47619\n\n\n2021-10-01\nNatural Gas\n3.09300\n\n\n2021-11-01\nUranium\n31.09958\n\n\n2021-11-01\nNatural Gas\n3.28000\n\n\n2021-12-01\nUranium\n36.12930\n\n\n2021-12-01\nNatural Gas\n3.20400\n\n\n2022-01-01\nUranium\n36.87292\n\n\n2022-01-01\nNatural Gas\n3.14100\n\n\n2022-02-01\nUranium\n35.83191\n\n\n2022-02-01\nNatural Gas\n3.33000\n\n\n2022-03-01\nUranium\n45.51470\n\n\n2022-03-01\nNatural Gas\n3.96300\n\n\n2022-04-01\nUranium\n48.70473\n\n\n2022-04-01\nNatural Gas\n4.02100\n\n\n2022-05-01\nUranium\n40.89163\n\n\n2022-05-01\nNatural Gas\n4.03100\n\n\n2022-06-01\nUranium\n40.33152\n\n\n2022-06-01\nNatural Gas\n4.70200\n\n\n2022-07-01\nUranium\n38.93916\n\n\n2022-07-01\nNatural Gas\n4.61900\n\n\n2022-08-01\nUranium\n39.80231\n\n\n2022-08-01\nNatural Gas\n4.03400\n\n\n2022-09-01\nUranium\n40.95039\n\n\n2022-09-01\nNatural Gas\n3.61700\n\n\n2022-10-01\nUranium\n41.30360\n\n\n2022-10-01\nNatural Gas\n3.59200\n\n\n2022-11-01\nUranium\n40.95406\n\n\n2022-11-01\nNatural Gas\n3.62800\n\n\n2022-12-01\nUranium\n39.17824\n\n\n2022-12-01\nNatural Gas\n3.26000\n\n\n\n\n\n\nuranium &lt;- ggplot(df_resources[df_resources$Resources == \"Uranium\",]) +\n                geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n                labs(x = \"Price\", y = \"Frequency\", title = \"Resource Distributions\", subtitle = \"Uranium\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nnatural_gas &lt;- ggplot(df_resources[df_resources$Resources == \"Natural Gas\",]) +\n                geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n                labs(x = \"Price\", y = \"Frequency\", subtitle = \"Natural Gas\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nplots &lt;- wrap_plots(uranium, natural_gas, nrow = 2)\n\nplots\n\n\n\n\n\nlibrary(patchwork)\n\nplot_list &lt;- lapply(unique(df_resources$Resources), function(resource) {\n  ggplot(df_resources[df_resources$Resources == resource, ]) +\n    geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n    labs(x = \"Price\", y = \"Frequency\", title = resource) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n      axis.title.x = element_text(size = 10),\n      axis.title.y = element_text(size = 10),\n      plot.margin = margin(1, 1, 1, 1, \"cm\")\n    )\n})\n\n# Arrange the plots in a grid\ngrid_plot &lt;- wrap_plots(plotlist = plot_list, nrow = 2)\n\n# Print the grid of histograms\ngrid_plot\n\n\n\n\n\nuranium &lt;- ggplot(df_resources[df_resources$Resources == \"Uranium\",]) +\n                geom_boxplot(aes(y = Prices)) +\n                labs(x = \"\", y = \"Price\", title = \"Resource Distributions\", subtitle = \"Uranium\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nggplotly(uranium)\n\n\n\n    \n        \n        \n\n\n\n\n\n\n\n    \n    \n        \n\n    \n\n\n\n\np &lt;- ggplot(df_resources) +\n        geom_line(aes(x = DATE, y = Prices), color = \"darkred\") + \n        labs(x = \"Date\", y = \"Price\", title = \"Resources Prices\") +\n        theme_minimal() +\n        theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\"))+\n        facet_grid(Resources ~ ., scales = \"free\")    \n\nggplotly(p, width = 800, height = 500)"
  },
  {
    "objectID": "4_EDA_df_lithium.html",
    "href": "4_EDA_df_lithium.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Read csv file\ndf_production &lt;- read.csv(\"../../data/01-modified-data/clean_lithium-production.csv\")\n\n# Read csv file\ndf_demand &lt;- read.csv(\"../../data/01-modified-data/clean_lithium-demand.csv\")\n\n\ndf_production &lt;- df_production %&gt;% filter(df_production$Code == \"USA\")\n\ndf_production\n\n\nA data.frame: 28 × 4\n\n\nEntity\nCode\nYear\nLithium.production...kt\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nUnited States\nUSA\n1995\n3500.000\n\n\nUnited States\nUSA\n1996\n4000.000\n\n\nUnited States\nUSA\n1997\n4000.000\n\n\nUnited States\nUSA\n1998\n1500.000\n\n\nUnited States\nUSA\n1999\n1500.000\n\n\nUnited States\nUSA\n2000\n1500.000\n\n\nUnited States\nUSA\n2001\n1500.000\n\n\nUnited States\nUSA\n2002\n1500.000\n\n\nUnited States\nUSA\n2003\n1500.000\n\n\nUnited States\nUSA\n2004\n1500.000\n\n\nUnited States\nUSA\n2005\n1500.000\n\n\nUnited States\nUSA\n2006\n1500.000\n\n\nUnited States\nUSA\n2007\n1500.000\n\n\nUnited States\nUSA\n2008\n1500.000\n\n\nUnited States\nUSA\n2009\n1500.000\n\n\nUnited States\nUSA\n2010\n1000.000\n\n\nUnited States\nUSA\n2011\n1000.000\n\n\nUnited States\nUSA\n2012\n1000.000\n\n\nUnited States\nUSA\n2013\n870.000\n\n\nUnited States\nUSA\n2014\n900.000\n\n\nUnited States\nUSA\n2015\n900.000\n\n\nUnited States\nUSA\n2016\n900.000\n\n\nUnited States\nUSA\n2017\n900.000\n\n\nUnited States\nUSA\n2018\n900.000\n\n\nUnited States\nUSA\n2019\n900.000\n\n\nUnited States\nUSA\n2020\n900.000\n\n\nUnited States\nUSA\n2021\n939.234\n\n\nUnited States\nUSA\n2022\n939.234"
  },
  {
    "objectID": "4_EDA_df_lithium.html#df_lithium",
    "href": "4_EDA_df_lithium.html#df_lithium",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Read csv file\ndf_production &lt;- read.csv(\"../../data/01-modified-data/clean_lithium-production.csv\")\n\n# Read csv file\ndf_demand &lt;- read.csv(\"../../data/01-modified-data/clean_lithium-demand.csv\")\n\n\ndf_production &lt;- df_production %&gt;% filter(df_production$Code == \"USA\")\n\ndf_production\n\n\nA data.frame: 28 × 4\n\n\nEntity\nCode\nYear\nLithium.production...kt\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nUnited States\nUSA\n1995\n3500.000\n\n\nUnited States\nUSA\n1996\n4000.000\n\n\nUnited States\nUSA\n1997\n4000.000\n\n\nUnited States\nUSA\n1998\n1500.000\n\n\nUnited States\nUSA\n1999\n1500.000\n\n\nUnited States\nUSA\n2000\n1500.000\n\n\nUnited States\nUSA\n2001\n1500.000\n\n\nUnited States\nUSA\n2002\n1500.000\n\n\nUnited States\nUSA\n2003\n1500.000\n\n\nUnited States\nUSA\n2004\n1500.000\n\n\nUnited States\nUSA\n2005\n1500.000\n\n\nUnited States\nUSA\n2006\n1500.000\n\n\nUnited States\nUSA\n2007\n1500.000\n\n\nUnited States\nUSA\n2008\n1500.000\n\n\nUnited States\nUSA\n2009\n1500.000\n\n\nUnited States\nUSA\n2010\n1000.000\n\n\nUnited States\nUSA\n2011\n1000.000\n\n\nUnited States\nUSA\n2012\n1000.000\n\n\nUnited States\nUSA\n2013\n870.000\n\n\nUnited States\nUSA\n2014\n900.000\n\n\nUnited States\nUSA\n2015\n900.000\n\n\nUnited States\nUSA\n2016\n900.000\n\n\nUnited States\nUSA\n2017\n900.000\n\n\nUnited States\nUSA\n2018\n900.000\n\n\nUnited States\nUSA\n2019\n900.000\n\n\nUnited States\nUSA\n2020\n900.000\n\n\nUnited States\nUSA\n2021\n939.234\n\n\nUnited States\nUSA\n2022\n939.234"
  },
  {
    "objectID": "4_eda/4_4_commodity_price.html",
    "href": "4_eda/4_4_commodity_price.html",
    "title": "Commodity Price",
    "section": "",
    "text": "Intro\n\nDataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDATE\nAluminum\nCobalt\nCopper\nNickel\nZinc\nLithium\n\n\n\n\n2012-06-01\n1885.513\n28831.91\n7428.289\n16603.68\n1855.934\n63643.22\n\n\n2012-07-01\n1876.250\n28394.08\n7584.261\n16128.41\n1847.750\n63568.83\n\n\n2012-08-01\n1843.327\n29050.36\n7510.432\n15703.99\n1816.318\n63682.98\n\n\n2012-09-01\n2064.120\n29221.25\n8087.743\n17287.96\n2009.850\n64069.57\n\n\n2012-10-01\n1974.304\n26896.74\n8062.033\n17168.74\n1903.959\n64650.80\n\n\n2012-11-01\n1948.830\n23563.41\n7711.227\n16335.36\n1912.398\n64970.67\n\n\n2012-12-01\n2086.763\n23976.00\n7966.487\n17448.50\n2040.429\n64966.89\n\n\n2013-01-01\n2037.607\n25518.09\n8053.739\n17494.07\n2031.409\n65091.61\n\n\n2013-02-01\n2053.595\n25441.33\n8060.925\n17690.10\n2128.688\n64976.63\n\n\n2013-03-01\n1911.283\n25316.97\n7652.375\n16731.70\n1929.150\n65155.51\n\n\n\n\n\nSummary\n\nDescription\n\n\n\n      DATE               Aluminum        Cobalt          Copper     \n Min.   :2012-06-01   Min.   :1460   Min.   :22276   Min.   : 4472  \n 1st Qu.:2015-01-16   1st Qu.:1742   1st Qu.:28745   1st Qu.: 5828  \n Median :2017-09-01   Median :1871   Median :32100   Median : 6713  \n Mean   :2017-08-31   Mean   :1973   Mean   :41195   Mean   : 6816  \n 3rd Qu.:2020-04-16   3rd Qu.:2081   3rd Qu.:52590   3rd Qu.: 7565  \n Max.   :2022-12-01   Max.   :3498   Max.   :90782   Max.   :10231  \n     Nickel           Zinc         Lithium      \n Min.   : 8298   Min.   :1520   Min.   : 62377  \n 1st Qu.:11664   1st Qu.:2026   1st Qu.: 66030  \n Median :14101   Median :2414   Median : 87823  \n Mean   :14992   Mean   :2488   Mean   :122254  \n 3rd Qu.:17331   3rd Qu.:2892   3rd Qu.:120041  \n Max.   :33924   Max.   :4371   Max.   :490008  \n\n\nHistograms Visualization\n\nDescription\n\n\n# #| echo: true\n# #| message: false\n# #| code-fold: true\n# #| code-summary: \"Histogram Visualization Code\"\n# #| warning: false\n\n# df_resources &lt;- pivot_longer(df_resources, \n#                         cols = -DATE,\n#                         names_to = \"Resources\",\n#                         values_to = \"Prices\")\n\n# uranium &lt;- ggplot(df_resources[df_resources$Resources == \"Uranium\",]) +\n#                 geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n#                 labs(x = \"Price\", y = \"Frequency\", title = \"Resource Distributions\", subtitle = \"Uranium\") +\n#                 theme_minimal() +\n#                 theme(\n#                 plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n#                 plot.subtitle = element_text(hjust = 0.5, size = 14),\n#                 axis.title.x = element_text(size = 10),\n#                 axis.title.y = element_text(size = 10),\n#                 plot.margin = margin(1, 1, 1, 1, \"cm\")\n#                 )\n\n# natural_gas &lt;- ggplot(df_resources[df_resources$Resources == \"Natural Gas\",]) +\n#                 geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n#                 labs(x = \"Price\", y = \"Frequency\", subtitle = \"Natural Gas\") +\n#                 theme_minimal() +\n#                 theme(\n#                 plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n#                 plot.subtitle = element_text(hjust = 0.5, size = 14),\n#                 axis.title.x = element_text(size = 10),\n#                 axis.title.y = element_text(size = 10),\n#                 plot.margin = margin(1, 1, 1, 1, \"cm\")\n#                 )\n\n# plots &lt;- wrap_plots(uranium, natural_gas, nrow = 2)\n\n# plots\n\nBoxplot Visualization\n\nDescription\n\n\n# #| echo: true\n# #| message: false\n# #| code-fold: true\n# #| code-summary: \"Boxplot Visualization Code\"\n# #| warning: false\n\n# box &lt;- ggplot(df_resources[df_resources$Resources == \"Uranium\",]) +\n#             geom_boxplot(aes(y = Prices)) +\n#             labs(x = \"\", y = \"Price\", title = \"Resource Distributions\", subtitle = \"Uranium\") +\n#             theme_minimal() +\n#             theme(\n#             plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n#             plot.subtitle = element_text(hjust = 0.5, size = 14),\n#             axis.title.x = element_text(size = 10),\n#             axis.title.y = element_text(size = 10),\n#             plot.margin = margin(1, 1, 1, 1, \"cm\")\n#             )\n\n# # box &lt;- ggplotly(box, width = 300, height = 600)\n\n# box\n\n\n# #| echo: true\n# #| message: false\n# #| code-fold: true\n# #| code-summary: \"Boxplot Visualization Code\"\n# #| warning: false\n\n# box &lt;-  ggplot(df_resources[df_resources$Resources == \"Natural Gas\",]) +\n#                 geom_boxplot(aes(y = Prices)) +\n#                 labs(x = \"\", y = \"Price\", title = \"Resource Distributions\", subtitle = \"Natural Gas\") +\n#                 theme_minimal() +\n#                 theme(\n#                 plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n#                 plot.subtitle = element_text(hjust = 0.5, size = 14),\n#                 axis.title.x = element_text(size = 10),\n#                 axis.title.y = element_text(size = 10),\n#                 plot.margin = margin(1, 1, 1, 1, \"cm\"))\n\n# # box &lt;- ggplotly(box, width = 300, height = 600)\n\n# box\n\nTime Series Visualization\n\nDescription\nMention Trends\n\n\n\nTime Series Visualization Code\n# p &lt;- ggplot(df_resources) +\n#         geom_line(aes(x = DATE, y = Prices), color = \"darkred\") + \n#         labs(x = \"Date\", y = \"Price\", title = \"Resources Prices\") +\n#         theme_minimal() +\n#         theme(\n#                 plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n#                 axis.title.x = element_text(size = 10),\n#                 axis.title.y = element_text(size = 10),\n#                 plot.margin = margin(1, 1, 1, 1, \"cm\"))+\n#         facet_grid(Resources ~ ., scales = \"free\")    \n\n# #ggplotly(p, width = 650, height = 400)\n# ggplotly(p)\n\n\nCorrelation Analysis\n\nDescription\nMention Trends\n\n\n\nTime Series Visualization Code\ncorrelation_matrix &lt;- cor(df_resources[, -1], use = \"complete.obs\")\n\n# Plot heatmap using corrplot\ncorrplot(correlation_matrix, method = \"color\", type = \"upper\", tl.cex = 0.7)"
  },
  {
    "objectID": "4_1_NaiveBayes_R.html",
    "href": "4_1_NaiveBayes_R.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nimport nltk;\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download([\n    \"names\",\n    \"stopwords\",\n    \"state_union\",\n    \"twitter_samples\",\n    \"movie_reviews\",\n    \"averaged_perceptron_tagger\",\n    \"vader_lexicon\",\n    \"punkt\",])\n\nimport string \n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package names to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package names is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package state_union to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package state_union is already up-to-date!\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n[nltk_data] Downloading package movie_reviews to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package movie_reviews is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\ndf = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\nprint(df.shape)\n\ndf = df.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf\n\n(53, 6)\n\n\n\n\n\n\n\n\n\nname\ncategory\nsentiment\nlabel\ndate\ntext\n\n\n\n\n0\nWired\ntechnology\n-0.5961\nnegative\n2023-10-27\nIn lithium-ion batteries, thats no longer the ...\n\n\n1\nThe Verge\ntechnology\n-0.8783\nnegative\n2023-10-27\nFord hits the brakes on $12 billion in EV spen...\n\n\n2\nArs Technica\ntechnology\n0.2886\npositive\n2023-10-31\nEnlarge/ GreenPower has given its class-D elec...\n\n\n3\nBusiness Insider\nbusiness\n0.6452\npositive\n2023-10-22\nWuling Hongguang Mini EVs on display at the Sh...\n\n\n4\nThe Next Web\ntechnology\n0.6209\npositive\n2023-10-24\nIn a big boost to sustainable mobility, 130 mi...\n\n\n5\nThe Next Web\ntechnology\n0.9080\npositive\n2023-10-23\nNorwegian tech company AutoStore today unveile...\n\n\n6\nBusiness Insider\nbusiness\n-0.6760\nnegative\n2023-10-18\nWhen something enters Earth's atmosphere, it's...\n\n\n7\nEngadget\ntechnology\n0.0000\nneutral\n2023-10-22\nSpace isn't hard only on account of the rocket...\n\n\n8\nBusiness Insider\nbusiness\n0.6035\npositive\n2023-10-29\nThe National Spherical Torus Experiment-Upgrad...\n\n\n9\nTime\ngeneral\n0.0000\nneutral\n2023-10-19\nIn her new book, The Woman in Me, Britney Spea...\n\n\n10\nTime\ngeneral\n0.0000\nneutral\n2023-10-19\nThe rocky relationship between Britney Spears ...\n\n\n11\nArs Technica\ntechnology\n0.8393\npositive\n2023-10-16\nEnlarge/ The back of the 2019 iPad Air.\\r\\n13 ...\n\n\n12\nCBS News\ngeneral\n-0.6290\nnegative\n2023-10-23\nRiders of Toos Elite 60-volt electric scooters...\n\n\n13\nNext Big Future\nscience\n0.0000\nneutral\n2023-10-25\nBrian Wang is a Futurist Thought Leader and a ...\n\n\n14\nNew Scientist\nscience\n0.6051\npositive\n2023-10-31\nPassive cooling could be more efficient using ...\n\n\n15\nCBS News\ngeneral\n-0.7609\nnegative\n2023-10-17\nAs e-scooters, hoverboards and e-bikes increas...\n\n\n16\nFortune\nbusiness\n-0.8252\nnegative\n2023-10-19\nWith the growing popularity of disposable e-ci...\n\n\n17\nFortune\nbusiness\n0.0000\nneutral\n2023-10-31\nToyota will invest an additional $8 billion in...\n\n\n18\nNew Scientist\nscience\n0.8984\npositive\n2023-10-24\nSolid-state batteries could be lighter and mor...\n\n\n19\nTechRadar\ntechnology\n0.0000\nneutral\n2023-10-16\nMercedes-Benz launched the series production v...\n\n\n20\nCBS News\ngeneral\n0.6510\npositive\n2023-10-17\nRemote Retriever via CBS Deals\\r\\nThis week on...\n\n\n21\nNewsweek\ngeneral\n-0.9520\nnegative\n2023-10-23\nU.S. consumers are being strongly warned again...\n\n\n22\nABC News\ngeneral\n0.0000\nneutral\n2023-10-31\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n23\nFortune\nbusiness\n-0.6701\nnegative\n2023-10-31\nSince Russian President Vladimir Putin invaded...\n\n\n24\nBreitbart News\ngeneral\n0.6588\npositive\n2023-10-16\nThe Taliban jihadist regime in Afghanistan sen...\n\n\n25\nBreitbart News\ngeneral\n0.0000\nneutral\n2023-10-19\nHaji Nooruddin Azizi, the acting commerce mini...\n\n\n26\nABC News\ngeneral\n0.6487\npositive\n2023-11-01\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n27\nABC News\ngeneral\n-0.8907\nnegative\n2023-10-19\nWASHINGTON -- With the growing popularity of d...\n\n\n28\nABC News\ngeneral\n0.6301\npositive\n2023-10-29\nTOKYO -- Trade and economy officials from the ...\n\n\n29\nNew Scientist\nscience\n0.0000\nneutral\n2023-10-30\nThe Athel tamarisk has ingenious ways of survi...\n\n\n30\nNewsweek\ngeneral\n0.4480\npositive\n2023-10-30\nAround the world, entrepreneurs are trying to ...\n\n\n31\nABC News\ngeneral\n0.0000\nneutral\n2023-10-31\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n32\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n33\nABC News\ngeneral\n-0.8907\nnegative\n2023-10-19\nWASHINGTON -- With the growing popularity of d...\n\n\n34\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n35\nABC News\ngeneral\n0.6487\npositive\n2023-11-01\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n36\nABC News\ngeneral\n0.6301\npositive\n2023-10-29\nTOKYO -- Trade and economy officials from the ...\n\n\n37\nTime\ngeneral\n-0.3817\nnegative\n2023-10-20\nBritney Spears knows what it means to be depri...\n\n\n38\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n39\nABC News\ngeneral\n0.0000\nneutral\n2023-10-17\nBEIJING -- China is hosting its third internat...\n\n\n40\nBreitbart News\ngeneral\n0.0000\nneutral\n2023-10-17\nChiles far-left President Gabriel Boric met wi...\n\n\n41\nArs Technica\ntechnology\n-0.7175\nnegative\n2023-10-25\nEnlarge/ A GM Ultium battery pack. \\r\\n75 with...\n\n\n42\nBreitbart News\ngeneral\n0.3935\npositive\n2023-10-31\nAfter investing billions to adhere to Presiden...\n\n\n43\nBreitbart News\ngeneral\n0.0000\nneutral\n2023-10-23\nNearly 7,000 auto workers at Stellantis’ Sterl...\n\n\n44\nFox News\ngeneral\n-0.8252\nnegative\n2023-10-19\nWith the growing popularity of disposable e-ci...\n\n\n45\nTechRadar\ntechnology\n-0.5295\nnegative\n2023-10-30\nYeedi Cube: One-minute review\\r\\nThe Yeedi Cub...\n\n\n46\nNBC News\ngeneral\n-0.5513\nnegative\n2023-10-20\nBritney Spears is shedding new light onto what...\n\n\n47\nBreitbart News\ngeneral\n0.3316\npositive\n2023-10-27\nAfter investing billions into a green energy a...\n\n\n48\nFox News\ngeneral\n0.0000\nneutral\n2023-10-18\nIn yet another attempt to regulate the car mar...\n\n\n49\nEngadget\ntechnology\n-0.6099\nnegative\n2023-11-01\nYour EV may go a long way between charges, but...\n\n\n50\nReuters\ngeneral\n-0.3458\nnegative\n2023-10-24\nDETROIT, Oct 24 (Reuters) - General Motors (GM...\n\n\n51\nTechCrunch\ntechnology\n0.0000\nneutral\n2023-10-15\nTen billion. That’s how many commercially proc...\n\n\n52\nUSA Today\ngeneral\n-0.6586\nnegative\n2023-10-26\nMore than a dozen states now have near-total a..."
  },
  {
    "objectID": "4_1_NaiveBayes_R.html#news-data",
    "href": "4_1_NaiveBayes_R.html#news-data",
    "title": "Naive Bayes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nimport nltk;\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download([\n    \"names\",\n    \"stopwords\",\n    \"state_union\",\n    \"twitter_samples\",\n    \"movie_reviews\",\n    \"averaged_perceptron_tagger\",\n    \"vader_lexicon\",\n    \"punkt\",])\n\nimport string \n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package names to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package names is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package state_union to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package state_union is already up-to-date!\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n[nltk_data] Downloading package movie_reviews to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package movie_reviews is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\ndf = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\nprint(df.shape)\n\ndf = df.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf\n\n(53, 6)\n\n\n\n\n\n\n\n\n\nname\ncategory\nsentiment\nlabel\ndate\ntext\n\n\n\n\n0\nWired\ntechnology\n-0.5961\nnegative\n2023-10-27\nIn lithium-ion batteries, thats no longer the ...\n\n\n1\nThe Verge\ntechnology\n-0.8783\nnegative\n2023-10-27\nFord hits the brakes on $12 billion in EV spen...\n\n\n2\nArs Technica\ntechnology\n0.2886\npositive\n2023-10-31\nEnlarge/ GreenPower has given its class-D elec...\n\n\n3\nBusiness Insider\nbusiness\n0.6452\npositive\n2023-10-22\nWuling Hongguang Mini EVs on display at the Sh...\n\n\n4\nThe Next Web\ntechnology\n0.6209\npositive\n2023-10-24\nIn a big boost to sustainable mobility, 130 mi...\n\n\n5\nThe Next Web\ntechnology\n0.9080\npositive\n2023-10-23\nNorwegian tech company AutoStore today unveile...\n\n\n6\nBusiness Insider\nbusiness\n-0.6760\nnegative\n2023-10-18\nWhen something enters Earth's atmosphere, it's...\n\n\n7\nEngadget\ntechnology\n0.0000\nneutral\n2023-10-22\nSpace isn't hard only on account of the rocket...\n\n\n8\nBusiness Insider\nbusiness\n0.6035\npositive\n2023-10-29\nThe National Spherical Torus Experiment-Upgrad...\n\n\n9\nTime\ngeneral\n0.0000\nneutral\n2023-10-19\nIn her new book, The Woman in Me, Britney Spea...\n\n\n10\nTime\ngeneral\n0.0000\nneutral\n2023-10-19\nThe rocky relationship between Britney Spears ...\n\n\n11\nArs Technica\ntechnology\n0.8393\npositive\n2023-10-16\nEnlarge/ The back of the 2019 iPad Air.\\r\\n13 ...\n\n\n12\nCBS News\ngeneral\n-0.6290\nnegative\n2023-10-23\nRiders of Toos Elite 60-volt electric scooters...\n\n\n13\nNext Big Future\nscience\n0.0000\nneutral\n2023-10-25\nBrian Wang is a Futurist Thought Leader and a ...\n\n\n14\nNew Scientist\nscience\n0.6051\npositive\n2023-10-31\nPassive cooling could be more efficient using ...\n\n\n15\nCBS News\ngeneral\n-0.7609\nnegative\n2023-10-17\nAs e-scooters, hoverboards and e-bikes increas...\n\n\n16\nFortune\nbusiness\n-0.8252\nnegative\n2023-10-19\nWith the growing popularity of disposable e-ci...\n\n\n17\nFortune\nbusiness\n0.0000\nneutral\n2023-10-31\nToyota will invest an additional $8 billion in...\n\n\n18\nNew Scientist\nscience\n0.8984\npositive\n2023-10-24\nSolid-state batteries could be lighter and mor...\n\n\n19\nTechRadar\ntechnology\n0.0000\nneutral\n2023-10-16\nMercedes-Benz launched the series production v...\n\n\n20\nCBS News\ngeneral\n0.6510\npositive\n2023-10-17\nRemote Retriever via CBS Deals\\r\\nThis week on...\n\n\n21\nNewsweek\ngeneral\n-0.9520\nnegative\n2023-10-23\nU.S. consumers are being strongly warned again...\n\n\n22\nABC News\ngeneral\n0.0000\nneutral\n2023-10-31\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n23\nFortune\nbusiness\n-0.6701\nnegative\n2023-10-31\nSince Russian President Vladimir Putin invaded...\n\n\n24\nBreitbart News\ngeneral\n0.6588\npositive\n2023-10-16\nThe Taliban jihadist regime in Afghanistan sen...\n\n\n25\nBreitbart News\ngeneral\n0.0000\nneutral\n2023-10-19\nHaji Nooruddin Azizi, the acting commerce mini...\n\n\n26\nABC News\ngeneral\n0.6487\npositive\n2023-11-01\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n27\nABC News\ngeneral\n-0.8907\nnegative\n2023-10-19\nWASHINGTON -- With the growing popularity of d...\n\n\n28\nABC News\ngeneral\n0.6301\npositive\n2023-10-29\nTOKYO -- Trade and economy officials from the ...\n\n\n29\nNew Scientist\nscience\n0.0000\nneutral\n2023-10-30\nThe Athel tamarisk has ingenious ways of survi...\n\n\n30\nNewsweek\ngeneral\n0.4480\npositive\n2023-10-30\nAround the world, entrepreneurs are trying to ...\n\n\n31\nABC News\ngeneral\n0.0000\nneutral\n2023-10-31\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n32\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n33\nABC News\ngeneral\n-0.8907\nnegative\n2023-10-19\nWASHINGTON -- With the growing popularity of d...\n\n\n34\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n35\nABC News\ngeneral\n0.6487\npositive\n2023-11-01\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n36\nABC News\ngeneral\n0.6301\npositive\n2023-10-29\nTOKYO -- Trade and economy officials from the ...\n\n\n37\nTime\ngeneral\n-0.3817\nnegative\n2023-10-20\nBritney Spears knows what it means to be depri...\n\n\n38\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n39\nABC News\ngeneral\n0.0000\nneutral\n2023-10-17\nBEIJING -- China is hosting its third internat...\n\n\n40\nBreitbart News\ngeneral\n0.0000\nneutral\n2023-10-17\nChiles far-left President Gabriel Boric met wi...\n\n\n41\nArs Technica\ntechnology\n-0.7175\nnegative\n2023-10-25\nEnlarge/ A GM Ultium battery pack. \\r\\n75 with...\n\n\n42\nBreitbart News\ngeneral\n0.3935\npositive\n2023-10-31\nAfter investing billions to adhere to Presiden...\n\n\n43\nBreitbart News\ngeneral\n0.0000\nneutral\n2023-10-23\nNearly 7,000 auto workers at Stellantis’ Sterl...\n\n\n44\nFox News\ngeneral\n-0.8252\nnegative\n2023-10-19\nWith the growing popularity of disposable e-ci...\n\n\n45\nTechRadar\ntechnology\n-0.5295\nnegative\n2023-10-30\nYeedi Cube: One-minute review\\r\\nThe Yeedi Cub...\n\n\n46\nNBC News\ngeneral\n-0.5513\nnegative\n2023-10-20\nBritney Spears is shedding new light onto what...\n\n\n47\nBreitbart News\ngeneral\n0.3316\npositive\n2023-10-27\nAfter investing billions into a green energy a...\n\n\n48\nFox News\ngeneral\n0.0000\nneutral\n2023-10-18\nIn yet another attempt to regulate the car mar...\n\n\n49\nEngadget\ntechnology\n-0.6099\nnegative\n2023-11-01\nYour EV may go a long way between charges, but...\n\n\n50\nReuters\ngeneral\n-0.3458\nnegative\n2023-10-24\nDETROIT, Oct 24 (Reuters) - General Motors (GM...\n\n\n51\nTechCrunch\ntechnology\n0.0000\nneutral\n2023-10-15\nTen billion. That’s how many commercially proc...\n\n\n52\nUSA Today\ngeneral\n-0.6586\nnegative\n2023-10-26\nMore than a dozen states now have near-total a..."
  },
  {
    "objectID": "4_1_NaiveBayes.html",
    "href": "4_1_NaiveBayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "```{css, echo = FALSE} .justify { text-align: justify !important; text-indent: 20px; }\n.epigrafe { text-align: justify !important; text-indent: 20px; border: 1.5px solid #87c8b5; padding-top: 15px; padding-bottom: 5px; padding-right: 15px; padding-left: 15px; font-size: 14px; background-color: #f9f9f9; margin: 20px 0px 30px 0px; }\n:::"
  },
  {
    "objectID": "4_1_NaiveBayes.html#prepare-data-for-naïve-bayes",
    "href": "4_1_NaiveBayes.html#prepare-data-for-naïve-bayes",
    "title": "Naive Bayes",
    "section": "Prepare Data for Naïve Bayes",
    "text": "Prepare Data for Naïve Bayes\n\nOverview of NB labeled text.\n\nDataset\n\n\n                name  ...                                               text\n0          The Verge  ...  Redwood Materials will recycle stationary stor...\n1       Ars Technica  ...  Enlarge/ GreenPower has given its class-D elec...\n2       Ars Technica  ...  Enlarge/ These are piles of lithium harvested ...\n3   Business Insider  ...  BP's EV charging arm has bought $100 million w...\n4               Time  ...  A lithium-ion battery recycler and a program t...\n..               ...  ...                                                ...\n80         The Verge  ...  The effort to climate-proof our housing is run...\n81           Reuters  ...  ROME, Nov 6 (Reuters) - Italy's Industrie De N...\n82           Reuters  ...  WASHINGTON, Nov 13 (Reuters) - Democratic Sena...\n83          Engadget  ...  Your EV may go a long way between charges, but...\n84          Politico  ...  The administration is well aware of the high s...\n\n[85 rows x 6 columns]\n\n\nShape\n\n\n(85, 6)"
  },
  {
    "objectID": "4_1_NaiveBayes.html#naïve-bayes-nb-with-labeled-text-data",
    "href": "4_1_NaiveBayes.html#naïve-bayes-nb-with-labeled-text-data",
    "title": "Naive Bayes",
    "section": "Naïve Bayes (NB) with Labeled Text Data",
    "text": "Naïve Bayes (NB) with Labeled Text Data\n\n\nClean Text Data\nnew_text=\"\"\n\nfor index, row in df.iterrows():\n    for character in df.at[index, 'text']:\n        if character in string.printable:\n            new_text+=character\n    df.at[index, 'text'] = new_text\n    new_text=\"\"\n\n\n# Convert from string labels to integers\nlabels=[]; #y1=[]; y2=[]\ny1=[]\nfor label in df[\"label\"]:\n    if label not in labels:\n        labels.append(label)\n        print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            y1.append(i)\n\n\nindex = 0 : label = positive\nindex = 1 : label = neutral\nindex = 2 : label = negative\n\n\nClean Text Data\ny1=np.array(y1)\n\n# Convert dataframe to list of strings\ncorpus=df[\"text\"].to_list()\ny2=df[\"sentiment\"].to_numpy()\n\nprint(\"number of text chunks = \",len(corpus))\n\n\nnumber of text chunks =  85\n\n\nClean Text Data\nprint(corpus[0:3])\n\n\n['Redwood Materials will recycle stationary storage batteries as it expands its scope\\r\\nRedwood Materials will recycle stationary storage batteries as it expands its scope\\r\\n / The company is recycling b [+3842 chars]', 'Enlarge/ GreenPower has given its class-D electric school bus a big battery bump.\\r\\n39 with \\r\\nOn Tuesday morning, the West Virginia-based GreenPower Motor Company debuted its latest electric vehicle.  [+4366 chars]', \"Enlarge/ These are piles of lithium harvested in Bolivia; Exxon's site in Arkansas will look almost entirely unlike this as it will use direct lithium extraction, not evaporation, to harvest the mine [+1611 chars]\"]\n\n\nClean Text Data\n#Vectorize the text data\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\nvectorizer=CountVectorizer(min_df=0.0001)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \nXs  =  vectorizer.fit_transform(corpus)   \nX = np.array(Xs.todense())\n\n#CONVERT TO ONE-HOT VECTORS\nmaxs=np.max(X,axis=0)\nX=np.ceil(X/maxs)\n\n# DOUBLE CHECK \nprint(X.shape,y1.shape,y2.shape)\n\n\n(85, 1184) (85,) (85,)\n\n\nClean Text Data\nprint(\"DATA POINT-0:\",X[0,0:10],\"y1 =\",y1[0],\"  y2 =\",y2[0])\n\n\nDATA POINT-0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] y1 = 0   y2 = 0.6783\n\n\n\n\nData Partitioning\n# Partition of dataset into training and test set\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y1, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\n\n\nx_train.shape       : (68, 1184)\n\n\nData Partitioning\nprint(\"y_train.shape        :\",y_train.shape)\n\n\ny_train.shape       : (68,)\n\n\nData Partitioning\nprint(\"X_test.shape     :\",x_test.shape)\n\n\nX_test.shape        : (17, 1184)\n\n\nData Partitioning\nprint(\"y_test.shape     :\",y_test.shape)\n\n\ny_test.shape        : (17,)\n\n\n\n\nCreate functions\ndef report(y,ypred):\n      # Accuracy compute \n      print(\"Accuracy:\",accuracy_score(y, ypred)*100)\n      print(\"Number of mislabeled points out of a total %d points = %d\"\n            % (y.shape[0], (y != ypred).sum()))\n\ndef print_model_summary():\n      # Label predictions for training and test set\n      yp_train = model.predict(x_train)\n      yp_test = model.predict(x_test)\n\n      print(\"ACCURACY CALCULATION\\n\")\n\n      print(\"TRAINING SET:\")\n      report(y_train,yp_train)\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      report(y_test,yp_test)\n\n      print(\"\\nCHECK FIRST 20 PREDICTIONS\")\n      print(\"TRAINING SET:\")\n      print(y_train[0:20])\n      print(yp_train[0:20])\n      print(\"ERRORS:\",yp_train[0:20]-y_train[0:20])\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      print(y_test[0:20])\n      print(yp_test[0:20])\n      print(\"ERRORS:\",yp_test[0:20]-y_test[0:20])\n\n\n\n\nCreate functions\n# Model \nmodel = MultinomialNB()\n\n# Train model \nmodel.fit(x_train,y_train)\n\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\nCreate functions\n# Show summary using previous function\nprint_model_summary()\n\n\nACCURACY CALCULATION\n\nTRAINING SET:\nAccuracy: 100.0\nNumber of mislabeled points out of a total 68 points = 0\n\nTEST SET (UNTRAINED DATA):\nAccuracy: 52.94117647058824\nNumber of mislabeled points out of a total 17 points = 8\n\nCHECK FIRST 20 PREDICTIONS\nTRAINING SET:\n[2 2 2 1 2 1 0 2 1 2 1 1 2 0 2 1 2 2 0 1]\n[2 2 2 1 2 1 0 2 1 2 1 1 2 0 2 1 2 2 0 1]\nERRORS: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\nTEST SET (UNTRAINED DATA):\n[1 0 2 2 2 1 1 2 1 2 2 2 2 2 1 0 1]\n[1 2 0 1 2 0 1 1 0 2 2 1 2 2 1 2 1]\nERRORS: [ 0  2 -2 -1  0 -1  0 -1 -1  0  0 -1  0  0  0  2  0]"
  },
  {
    "objectID": "4_data_exploration.html",
    "href": "4_data_exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Below are the libraries used in this section:\nR Libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa)\nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(gridExtra)\nlibrary(readxl)\nlibrary(zoo)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(corrplot)\nPython Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom cleantext import clean\nFunction generate_word_cloud()\ndef generate_word_cloud(my_text, title=\"Word Cloud\"):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    \n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud, title):\n        # Set figure size\n        plt.figure(figsize=(30, 20))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\")\n        # Add title\n        plt.title(title, fontsize=100)\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width=2000,\n        height=1000, \n        random_state=1, \n        # background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords=STOPWORDS\n    ).generate(my_text)\n\n    plot_cloud(wordcloud, title)\n    plt.show()"
  },
  {
    "objectID": "4_1_NaiveBayes.html#introduction-to-naive-bayes",
    "href": "4_1_NaiveBayes.html#introduction-to-naive-bayes",
    "title": "Naive Bayes",
    "section": "Introduction to Naive Bayes",
    "text": "Introduction to Naive Bayes\n\nOverview of Naive Bayes classification.\nHow it works?\nExplain the probabilistic nature of Naive Bayes and its Bayes’ theorem foundation.\nDefine the objectives of what you are trying to do.\nWhat you aim to achieve through Naive Bayes classification.\nDescribe different variants of Naive Bayes, such as Gaussian, Multinomial, and Bernoulli Naive Bayes, and explain when to use each.\n\n\n\nLibraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nimport nltk;\nnltk.download('punkt')\n\n\nLibraries\nnltk.download('averaged_perceptron_tagger')\n\n\nLibraries\nnltk.download('wordnet')\n\n\nLibraries\nnltk.download('stopwords')\n\n\nLibraries\nnltk.download([\n    \"names\",\n    \"stopwords\",\n    \"state_union\",\n    \"twitter_samples\",\n    \"movie_reviews\",\n    \"averaged_perceptron_tagger\",\n    \"vader_lexicon\",\n    \"punkt\",])\n\n\nLibraries\n\nimport string \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB"
  },
  {
    "objectID": "4_eda/4_5_news_api.html",
    "href": "4_eda/4_5_news_api.html",
    "title": "Lithium News",
    "section": "",
    "text": "Before cleaning the dataset, the dataset had 532 rows and 4 columns:\n\nEntity that represents the region/country of the information\nCode of the Entity\nYear the information was collected\nLithium production measured in tons.\n\n\n\n\nData Gathering\n# Read csv file\ndf_news &lt;- read.csv('../../../data/01-modified-data/clean_sentiment_analysis.csv')\n\n# Edit datatypes\ndf_news$date &lt;- as.Date(df_news$date)\n\ndf_news\n\n\n               name   category ibm_score ibm_label       date\n1             Wired technology   -0.5961  negative 2023-10-27\n2         The Verge technology   -0.8783  negative 2023-10-27\n3      Ars Technica technology    0.2886  positive 2023-10-31\n4  Business Insider   business    0.6452  positive 2023-10-22\n5      The Next Web technology    0.6209  positive 2023-10-24\n6      The Next Web technology    0.9080  positive 2023-10-23\n7  Business Insider   business   -0.6760  negative 2023-10-18\n8          Engadget technology    0.0000   neutral 2023-10-22\n9  Business Insider   business    0.6035  positive 2023-10-29\n10             Time    general    0.0000   neutral 2023-10-19\n11             Time    general    0.0000   neutral 2023-10-19\n12     Ars Technica technology    0.8393  positive 2023-10-16\n13         CBS News    general   -0.6290  negative 2023-10-23\n14  Next Big Future    science    0.0000   neutral 2023-10-25\n15    New Scientist    science    0.6051  positive 2023-10-31\n16         CBS News    general   -0.7609  negative 2023-10-17\n17          Fortune   business   -0.8252  negative 2023-10-19\n18          Fortune   business    0.0000   neutral 2023-10-31\n19    New Scientist    science    0.8984  positive 2023-10-24\n20        TechRadar technology    0.0000   neutral 2023-10-16\n21         CBS News    general    0.6510  positive 2023-10-17\n22         Newsweek    general   -0.9520  negative 2023-10-23\n23         ABC News    general    0.0000   neutral 2023-10-31\n24          Fortune   business   -0.6701  negative 2023-10-31\n25   Breitbart News    general    0.6588  positive 2023-10-16\n26   Breitbart News    general    0.0000   neutral 2023-10-19\n27         ABC News    general    0.6487  positive 2023-11-01\n28         ABC News    general   -0.8907  negative 2023-10-19\n29         ABC News    general    0.6301  positive 2023-10-29\n30    New Scientist    science    0.0000   neutral 2023-10-30\n31         Newsweek    general    0.4480  positive 2023-10-30\n32         ABC News    general    0.0000   neutral 2023-10-31\n33         ABC News    general    0.5985  positive 2023-10-25\n34         ABC News    general   -0.8907  negative 2023-10-19\n35         ABC News    general    0.5985  positive 2023-10-25\n36         ABC News    general    0.6487  positive 2023-11-01\n37         ABC News    general    0.6301  positive 2023-10-29\n38             Time    general   -0.3817  negative 2023-10-20\n39         ABC News    general    0.5985  positive 2023-10-25\n40         ABC News    general    0.0000   neutral 2023-10-17\n41   Breitbart News    general    0.0000   neutral 2023-10-17\n42     Ars Technica technology   -0.7175  negative 2023-10-25\n43   Breitbart News    general    0.3935  positive 2023-10-31\n44   Breitbart News    general    0.0000   neutral 2023-10-23\n45         Fox News    general   -0.8252  negative 2023-10-19\n46        TechRadar technology   -0.5295  negative 2023-10-30\n47         NBC News    general   -0.5513  negative 2023-10-20\n48   Breitbart News    general    0.3316  positive 2023-10-27\n49         Fox News    general    0.0000   neutral 2023-10-18\n50         Engadget technology   -0.6099  negative 2023-11-01\n51          Reuters    general   -0.3458  negative 2023-10-24\n52       TechCrunch technology    0.0000   neutral 2023-10-15\n53        USA Today    general   -0.6586  negative 2023-10-26\n                                                                                                                                                                                                               ibm_content\n1   In lithium-ion batteries, thats no longer the case. Modern batteries are capable of reading their state no matter their level of charge, and when your device isnt in use the strain on the battery is … [+3255 chars]\n2   Ford hits the brakes on $12 billion in EV spending because EVs are too expensive\\nFord hits the brakes on $12 billion in EV spending because EVs are too expensive\\n / The car company says it isnt bac… [+2216 chars]\n3   Enlarge/ GreenPower has given its class-D electric school bus a big battery bump.\\n39 with \\nOn Tuesday morning, the West Virginia-based GreenPower Motor Company debuted its latest electric vehicle. … [+4366 chars]\n4  Wuling Hongguang Mini EVs on display at the Shanghai auto expo.VCG/VCG via Getty Images\\n&lt;ul&gt;\\n&lt;li&gt;China has led the world in production and sales of electric vehicles, helped by government subsidies.… [+6141 chars]\n5   In a big boost to sustainable mobility, 130 million EVs are expected to hit Europes streets by 2035, reaching about half of the total vehicles on the continent. However, this also translates to 130 m… [+3822 chars]\n6   Norwegian tech company AutoStore today unveiled the latest version of its warehouse storage and retrieval robot, as it seeks to boost the efficiency of its automated fulfilment system. \\nDubbed the R… [+3374 chars]\n7  When something enters Earth's atmosphere, it's typically in a fiery blaze.solarseven/Getty Images\\n&lt;ul&gt;\\n&lt;li&gt;Scientists have discovered evidence of various pollutant metals in Earth's stratosphere.&lt;/l… [+1984 chars]\n8  Space isn't hard only on account of the rocket science. The task of taking a NASA mission from development and funding through construction and launch all before we even use the thing for science can… [+11147 chars]\n9  The National Spherical Torus Experiment-Upgrade (NSTX-U) at the Princeton Plasma Physics Laboratory.Elle Starkman, PPPL\\n&lt;ul&gt;\\n&lt;li&gt;It's been almost 90 years since scientists made the first fusion reac… [+8038 chars]\n10  In her new book, The Woman in Me, Britney Spears details the extent of her family's attempts to control her life and her considerable estate through a conservatorship for 13 years. She suggests that … [+3975 chars]\n11  The rocky relationship between Britney Spears and her younger sister, Jamie Lynn Spears, has long been documented in the press. In January 2022, Britney sent Jamie Lynn a cease and desist letter, ask… [+5806 chars]\n12 Enlarge/ The back of the 2019 iPad Air.\\n13 with \\nWhether you're shopping for a new laptop or you're looking to build your own PC, we found some great savings for you with some post-Prime Day sales.… [+12394 chars]\n13  Riders of Toos Elite 60-volt electric scooters are being urged to find another means of travel immediately, after the deaths of two people in a fire caused by the product.\\nThe urgent warning to stop… [+1457 chars]\n14   Brian Wang is a Futurist Thought Leader and a popular Science blogger with 1 million readers per month. His blog Nextbigfuture.com is ranked #1 Science News Blog. It covers many disruptive technology… [+593 chars]\n15  Passive cooling could be more efficient using a device containing a salty water solution\\ndaniiD/Shutterstock\\nComputers could see their performance jump by one third through an inexpensive cooling s… [+2046 chars]\n16  As e-scooters, hoverboards and e-bikes increase in popularity, emergency rooms are seeing a surge in injuries — fractures, contusions, burns and cuts — related to the products, continuing a multiyear… [+1968 chars]\n17  With the growing popularity of disposable e-cigarettes, communities across the U.S. are confronting a new vaping problem: how to safely get rid of millions of small, battery-powered devices that are … [+7951 chars]\n18  Toyota will invest an additional $8 billion in the hybrid and electric vehicle battery factory its constructing in North Carolina, more than doubling its prior investments and expected number of new … [+3679 chars]\n19  Solid-state batteries could be lighter and more powerful than current batteries\\nPhonlamai Photo/Shutterstock\\nToyota says it has made a breakthrough that will allow “game-changing” solid-state batte… [+3641 chars]\n20  Mercedes-Benz launched the series production version of its eActros 600 battery-electric long-haul truck last week, signifying its intent to assist in phasing out diesel counterparts in the coming ye… [+3861 chars]\n21  Remote Retriever via CBS Deals\\nThis week on \"CBS Mornings,\" lifestyle expert Elizabeth Werner discussed deals on items that might make your life easier -- all at exclusive discounts. Discover this w… [+1904 chars]\n22  U.S. consumers are being strongly warned against using a brand of electric scooters after reports found that a malfunctioning battery caused a fire earlier this year that killed two people, including… [+2738 chars]\n23  RALEIGH, N.C. -- Toyota will invest an additional $8 billion in the hybrid and electric vehicle battery factory it's constructing in North Carolina, more than doubling its prior investments and expec… [+3480 chars]\n24  Since Russian President Vladimir Putin invaded Ukraine, most of the attention has focused on Russias imagined security dangers on its western border, the threat of Ukraines democratic success as a mo… [+5789 chars]\n25  The Taliban jihadist regime in Afghanistan sent its Industry and Commerce Minister Nooruddin Azizi to Beijing on Monday to attend this week’s “Belt and Road Forum,” a celebration of ten years of Chin… [+6220 chars]\n26  Haji Nooruddin Azizi, the acting commerce minister for the Taliban regime, announced on Thursday that the terrorist organization plans to formally join Chinas Belt and Road Initiative (BRI) and will … [+4762 chars]\n27  TOKYO -- Toyotas July-September profit jumped nearly threefold from a year ago as vehicle sales grew around the world and a cheap yen boosted the Japanese automakers overseas earnings.\\nToyota Motor … [+2983 chars]\n28  WASHINGTON -- With the growing popularity of disposable e-cigarettes, communities across the U.S. are confronting a new vaping problem: how to safely get rid of millions of small, battery-powered dev… [+8104 chars]\n29  TOKYO -- Trade and economy officials from the Group of Seven wealthy democracies strengthened their pledge Sunday to work together to ensure smooth supply chains for essentials like energy and food d… [+3548 chars]\n30  The Athel tamarisk has ingenious ways of surviving in a dry climate\\nShutterstock / Wirestock Creators\\nAn evergreen desert shrub common in the Middle East excretes salt crystals onto its leaves that… [+2510 chars]\n31 Around the world, entrepreneurs are trying to turn green tech ideas into viable businesses. Many are working with technology that has been proven to work, but is too expensive for widespread use. The… [+11350 chars]\n32  RALEIGH, N.C. -- Toyota will invest an additional $8 billion in the hybrid and electric vehicle battery factory it's constructing in North Carolina, more than doubling its prior investments and expec… [+3480 chars]\n33  TOKYO -- TOKYO (AP) We love battery EVs.\\nTakero Kato, the executive in charge of electric vehicles at Toyota, said that not once, but twice, to emphasize what he considers the message at this years … [+5263 chars]\n34  WASHINGTON -- With the growing popularity of disposable e-cigarettes, communities across the U.S. are confronting a new vaping problem: how to safely get rid of millions of small, battery-powered dev… [+8105 chars]\n35  TOKYO -- TOKYO (AP) We love battery EVs.\\nTakero Kato, the executive in charge of electric vehicles at Toyota, said that not once, but twice, to emphasize what he considers the message at this years … [+5263 chars]\n36  TOKYO -- Toyotas July-September profit jumped nearly threefold from a year ago as vehicle sales grew around the world and a cheap yen boosted the Japanese automakers overseas earnings.\\nToyota Motor … [+2983 chars]\n37  TOKYO -- Trade and economy officials from the Group of Seven wealthy democracies strengthened their pledge Sunday to work together to ensure smooth supply chains for essentials like energy and food d… [+3548 chars]\n38 Britney Spears knows what it means to be deprived of adulthood. As told in her highly anticipated memoir, The Woman in Me, hers is a tale of rapid maturity followed by arrested development, freedom f… [+11066 chars]\n39  TOKYO -- TOKYO (AP) We love battery EVs.\\nTakero Kato, the executive in charge of electric vehicles at Toyota, said that not once, but twice, to emphasize what he considers the message at this years … [+5263 chars]\n40  BEIJING -- China is hosting its third international forum centered around President Xi Jinpings signature policy, the Belt and Road Initiative, which over the past 10 years has built infrastructure a… [+5446 chars]\n41  Chiles far-left President Gabriel Boric met with his Chinese counterpart, genocidal dictator Xi Jinping, on Tuesday, pledging to defend Beijing’s false claims over Taiwan and seeking support for his … [+5595 chars]\n42  Enlarge/ A GM Ultium battery pack. \\n75 with \\nBad news for fans of cheaper electric vehicles: The planned collaboration between Honda and General Motors on a range of cheaper EVs has been canceled. … [+3548 chars]\n43  After investing billions to adhere to President Joe Biden’s green energy agenda, General Motors (GM) is backtracking on all fronts when it comes to Electric Vehicles (EVs).\\nAs GM was the last of the… [+3230 chars]\n44  Nearly 7,000 auto workers at Stellantis’ Sterling Heights Assembly Plant in Sterling Heights, Michigan, have joined the United Auto Workers (UAW) strike — a major blow for the automaker as the plant … [+2519 chars]\n45  With the growing popularity of disposable e-cigarettes, communities across the U.S. are confronting a new vaping problem: how to safely get rid of millions of small, battery-powered devices that are … [+7914 chars]\n46 Yeedi Cube: One-minute review\\nThe Yeedi Cube comes among a litany of combo robot vacuums capable of both vacuuming and mopping. In that regard, it’s one of many. What sets it apart then is the fact … [+15208 chars]\n47  Britney Spears is shedding new light onto what her life was like during her 13-year court-ordered conservatorship.\\nIn her upcoming memoir, The Woman In Me, obtained early by TheNew York Times, which… [+3213 chars]\n48  After investing billions into a green energy agenda mimicking President Joe Biden’s federal rules, American automakers are quickly learning that Americans are not buying electric vehicles (EVs) at th… [+5334 chars]\n49  In yet another attempt to regulate the car market and phase out internal combustion engines (ICE), another agency has proposed a rule with lofty ambitions of increasing fleet-wide Corporate Average F… [+4942 chars]\n50  Your EV may go a long way between charges, but can it carry dozens of passengers? GreenPower can boast both of those things with its latest electric school bus called \"Mega Beast,\" it announced in a … [+1575 chars]\n51  DETROIT, Oct 24 (Reuters) - General Motors (GM.N) on Tuesday withdrew its 2023 profit outlook, blaming the rising costs of United Auto Workers strikes, but the automaker's shares rose on better-than-… [+5638 chars]\n52  Ten billion. That’s how many commercially procurable molecules are available today. Start looking at them in groups of five the typical combination used to make electrolyte materials in batteries and… [+4802 chars]\n53 More than a dozen states now have near-total abortion bans following the overturning of Roe v. Wade, with limited medical exceptions meant to protect the patients health or life.\\nBut among those sta… [+12492 chars]\n\n\nData Gathering\n# Original dataset\n#knitr::kable(head(df_news, n = 10), format = \"markdown\")\n\n\nDimensions:\n\n\n[1] 53  6\n\n\n\nDescription\n\nCleaned Data:\n\n\n               name   category ibm_score ibm_label       date\n1             Wired technology   -0.5961  negative 2023-10-27\n2         The Verge technology   -0.8783  negative 2023-10-27\n3      Ars Technica technology    0.2886  positive 2023-10-31\n4  Business Insider   business    0.6452  positive 2023-10-22\n5      The Next Web technology    0.6209  positive 2023-10-24\n6      The Next Web technology    0.9080  positive 2023-10-23\n7  Business Insider   business   -0.6760  negative 2023-10-18\n8          Engadget technology    0.0000   neutral 2023-10-22\n9  Business Insider   business    0.6035  positive 2023-10-29\n10             Time    general    0.0000   neutral 2023-10-19\n                                                                                                                                                                                                               ibm_content\n1   In lithium-ion batteries, thats no longer the case. Modern batteries are capable of reading their state no matter their level of charge, and when your device isnt in use the strain on the battery is … [+3255 chars]\n2   Ford hits the brakes on $12 billion in EV spending because EVs are too expensive\\nFord hits the brakes on $12 billion in EV spending because EVs are too expensive\\n / The car company says it isnt bac… [+2216 chars]\n3   Enlarge/ GreenPower has given its class-D electric school bus a big battery bump.\\n39 with \\nOn Tuesday morning, the West Virginia-based GreenPower Motor Company debuted its latest electric vehicle. … [+4366 chars]\n4  Wuling Hongguang Mini EVs on display at the Shanghai auto expo.VCG/VCG via Getty Images\\n&lt;ul&gt;\\n&lt;li&gt;China has led the world in production and sales of electric vehicles, helped by government subsidies.… [+6141 chars]\n5   In a big boost to sustainable mobility, 130 million EVs are expected to hit Europes streets by 2035, reaching about half of the total vehicles on the continent. However, this also translates to 130 m… [+3822 chars]\n6   Norwegian tech company AutoStore today unveiled the latest version of its warehouse storage and retrieval robot, as it seeks to boost the efficiency of its automated fulfilment system. \\nDubbed the R… [+3374 chars]\n7  When something enters Earth's atmosphere, it's typically in a fiery blaze.solarseven/Getty Images\\n&lt;ul&gt;\\n&lt;li&gt;Scientists have discovered evidence of various pollutant metals in Earth's stratosphere.&lt;/l… [+1984 chars]\n8  Space isn't hard only on account of the rocket science. The task of taking a NASA mission from development and funding through construction and launch all before we even use the thing for science can… [+11147 chars]\n9  The National Spherical Torus Experiment-Upgrade (NSTX-U) at the Princeton Plasma Physics Laboratory.Elle Starkman, PPPL\\n&lt;ul&gt;\\n&lt;li&gt;It's been almost 90 years since scientists made the first fusion reac… [+8038 chars]\n10  In her new book, The Woman in Me, Britney Spears details the extent of her family's attempts to control her life and her considerable estate through a conservatorship for 13 years. She suggests that … [+3975 chars]\n\n\nSummary:\n\n\n     name             category           ibm_score          ibm_label        \n Length:53          Length:53          Min.   :-0.952000   Length:53         \n Class :character   Class :character   1st Qu.:-0.596100   Class :character  \n Mode  :character   Mode  :character   Median : 0.000000   Mode  :character  \n                                       Mean   :-0.002707                     \n                                       3rd Qu.: 0.598500                     \n                                       Max.   : 0.908000                     \n      date            ibm_content       \n Min.   :2023-10-15   Length:53         \n 1st Qu.:2023-10-19   Class :character  \n Median :2023-10-24   Mode  :character  \n Mean   :2023-10-23                     \n 3rd Qu.:2023-10-29                     \n Max.   :2023-11-01                     \n\n\nDimensions:\n\n\n[1] 53  6"
  },
  {
    "objectID": "3_data_cleaning/3_4_commodity_price2.html",
    "href": "3_data_cleaning/3_4_commodity_price2.html",
    "title": "Commodity Price",
    "section": "",
    "text": "Before cleaning the dataset, the dataset had ### rows and # columns:\n\n\n\n\n\n\n\n\n\nData Gathering\n# Read csv file\ndf_commodity_price &lt;- read_excel(\"../../../data/00-raw-data/commodity_price.xlsx\")\n\n# Original dataset\nknitr::kable(head(df_commodity_price, n = 10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n…1\nJan 2012\n…3\nFeb 2012\n…5\nMar 2012\n…7\nApr 2012\n…9\nMay 2012\n…11\nJun 2012\n…13\nJul 2012\n…15\nAug 2012\n…17\nSep 2012\n…19\nOct 2012\n…21\nNov 2012\n…23\nDec 2012\n…25\nJan 2013\n…27\nFeb 2013\n…29\nMar 2013\n…31\nApr 2013\n…33\nMay 2013\n…35\nJun 2013\n…37\nJul 2013\n…39\nAug 2013\n…41\nSep 2013\n…43\nOct 2013\n…45\nNov 2013\n…47\nDec 2013\n…49\nJan 2014\n…51\nFeb 2014\n…53\nMar 2014\n…55\nApr 2014\n…57\nMay 2014\n…59\nJun 2014\n…61\nJul 2014\n…63\nAug 2014\n…65\nSep 2014\n…67\nOct 2014\n…69\nNov 2014\n…71\nDec 2014\n…73\nJan 2015\n…75\nFeb 2015\n…77\nMar 2015\n…79\nApr 2015\n…81\nMay 2015\n…83\nJun 2015\n…85\nJul 2015\n…87\nAug 2015\n…89\nSep 2015\n…91\nOct 2015\n…93\nNov 2015\n…95\nDec 2015\n…97\nJan 2016\n…99\nFeb 2016\n…101\nMar 2016\n…103\nApr 2016\n…105\nMay 2016\n…107\nJun 2016\n…109\nJul 2016\n…111\nAug 2016\n…113\nSep 2016\n…115\nOct 2016\n…117\nNov 2016\n…119\nDec 2016\n…121\nJan 2017\n…123\nFeb 2017\n…125\nMar 2017\n…127\nApr 2017\n…129\nMay 2017\n…131\nJun 2017\n…133\nJul 2017\n…135\nAug 2017\n…137\nSep 2017\n…139\nOct 2017\n…141\nNov 2017\n…143\nDec 2017\n…145\nJan 2018\n…147\nFeb 2018\n…149\nMar 2018\n…151\nApr 2018\n…153\nMay 2018\n…155\nJun 2018\n…157\nJul 2018\n…159\nAug 2018\n…161\nSep 2018\n…163\nOct 2018\n…165\nNov 2018\n…167\nDec 2018\n…169\nJan 2019\n…171\nFeb 2019\n…173\nMar 2019\n…175\nApr 2019\n…177\nMay 2019\n…179\nJun 2019\n…181\nJul 2019\n…183\nAug 2019\n…185\nSep 2019\n…187\nOct 2019\n…189\nNov 2019\n…191\nDec 2019\n…193\nJan 2020\n…195\nFeb 2020\n…197\nMar 2020\n…199\nApr 2020\n…201\nMay 2020\n…203\nJun 2020\n…205\nJul 2020\n…207\nAug 2020\n…209\nSep 2020\n…211\nOct 2020\n…213\nNov 2020\n…215\nDec 2020\n…217\nJan 2021\n…219\nFeb 2021\n…221\nMar 2021\n…223\nApr 2021\n…225\nMay 2021\n…227\nJun 2021\n…229\nJul 2021\n…231\nAug 2021\n…233\nSep 2021\n…235\nOct 2021\n…237\nNov 2021\n…239\nDec 2021\n…241\nJan 2022\n…243\nFeb 2022\n…245\nMar 2022\n…247\nApr 2022\n…249\nMay 2022\n…251\nJun 2022\n…253\nJul 2022\n…255\nAug 2022\n…257\nSep 2022\n…259\nOct 2022\n…261\nNov 2022\n…263\nDec 2022\n…265\n\n\n\n\nAustralian, export markets\n123.08571\nNA\n124.96684\nNA\n114.150974\nNA\n110.35376\nNA\n103.819480\nNA\n92.754699\nNA\n91.364123\nNA\n93.456331\nNA\n93.181607\nNA\n86.88540\nNA\n87.21964\nNA\n96.076128\nNA\n96.952110\nNA\n100.3473\nNA\n97.316786\nNA\n93.233163\nNA\n93.126020\nNA\n89.64268\nNA\n82.222360\nNA\n82.221429\nNA\n83.295918\nNA\n85.443634\nNA\n88.363265\nNA\n89.444464\nNA\n87.973539\nNA\n83.02768\nNA\n79.753061\nNA\n78.520714\nNA\n78.834643\nNA\n77.348469\nNA\n74.038044\nNA\n74.248393\nNA\n71.525649\nNA\n68.984627\nNA\n67.33071\nNA\n67.162245\nNA\n64.716326\nNA\n70.65911\nNA\n68.344967\nNA\n61.197857\nNA\n65.671241\nNA\n63.104708\nNA\n64.48323\nNA\n63.393214\nNA\n61.765422\nNA\n57.330682\nNA\n55.778061\nNA\n55.865816\nNA\n53.428929\nNA\n53.781633\nNA\n55.435204\nNA\n54.658673\nNA\n54.527143\nNA\n56.228571\nNA\n64.418367\nNA\n71.326948\nNA\n76.373864\nNA\n96.239796\nNA\n111.428084\nNA\n93.134464\nNA\n91.424490\nNA\n85.969286\nNA\n86.333851\nNA\n90.110714\nNA\n80.118367\nNA\n85.673377\nNA\n90.773980\nNA\n102.772403\nNA\n104.775000\nNA\n103.249675\nNA\n102.487013\nNA\n105.968797\nNA\n111.932630\nNA\n111.477321\nNA\n105.728061\nNA\n99.223393\nNA\n109.375000\nNA\n121.524490\nNA\n125.085877\nNA\n123.734903\nNA\n122.22911\nNA\n114.762112\nNA\n107.552922\nNA\n106.143609\nNA\n105.401299\nNA\n101.357143\nNA\n99.669898\nNA\n88.764643\nNA\n89.564286\nNA\n77.629821\nNA\n77.845808\nNA\n69.739286\nNA\n66.958674\nNA\n69.194255\nNA\n69.729082\nNA\n70.464643\nNA\n72.106169\nNA\n71.050179\nNA\n70.204870\nNA\n64.620000\nNA\n54.148308\nNA\n55.002760\nNA\n51.672205\nNA\n51.382500\nNA\n52.005682\nNA\n58.069480\nNA\n62.946429\nNA\n80.589796\nNA\n90.995357\nNA\n92.485714\nNA\n95.230901\nNA\n97.847679\nNA\n104.207143\nNA\n132.152922\nNA\n153.198214\nNA\n173.568367\nNA\n188.122890\nNA\n240.732143\nNA\n177.801623\nNA\n181.473980\nNA\n231.760179\nNA\n272.48679\nNA\n353.440528\nNA\n334.718233\nNA\n418.49388\nNA\n427.425000\nNA\n433.626531\nNA\n447.447565\nNA\n467.78367\nNA\n426.90918\nNA\n368.258279\nNA\n427.686964\nNA\n\n\nSouth African, export markets\n106.04524\nNA\n105.28333\nNA\n103.428182\nNA\n101.15421\nNA\n93.850454\nNA\n84.998421\nNA\n87.164091\nNA\n89.160455\nNA\n86.084500\nNA\n82.71087\nNA\n86.10455\nNA\n89.108947\nNA\n86.392727\nNA\n85.1960\nNA\n82.474500\nNA\n81.907619\nNA\n81.726191\nNA\n77.50100\nNA\n72.790000\nNA\n72.996190\nNA\n73.248095\nNA\n81.746956\nNA\n83.245238\nNA\n84.661000\nNA\n82.825455\nNA\n77.59800\nNA\n74.577143\nNA\n75.442000\nNA\n75.876500\nNA\n74.137143\nNA\n71.463043\nNA\n71.082000\nNA\n67.589545\nNA\n65.782174\nNA\n65.66000\nNA\n65.602857\nNA\n60.187143\nNA\n63.10550\nNA\n60.829091\nNA\n59.641000\nNA\n63.228421\nNA\n60.915455\nNA\n57.02826\nNA\n54.185500\nNA\n51.482273\nNA\n49.825000\nNA\n54.868571\nNA\n49.863810\nNA\n50.668000\nNA\n52.365714\nNA\n53.567143\nNA\n52.868095\nNA\n53.449000\nNA\n58.070000\nNA\n62.680476\nNA\n65.950000\nNA\n67.491818\nNA\n83.600000\nNA\n89.594545\nNA\n82.719000\nNA\n86.087143\nNA\n83.033500\nNA\n78.302609\nNA\n77.305000\nNA\n72.502857\nNA\n78.063182\nNA\n81.690952\nNA\n87.174091\nNA\n91.869048\nNA\n91.262727\nNA\n91.639545\nNA\n94.904737\nNA\n97.381818\nNA\n94.102000\nNA\n91.027143\nNA\n94.398000\nNA\n102.740952\nNA\n104.565714\nNA\n107.569546\nNA\n98.450909\nNA\n98.26700\nNA\n99.337391\nNA\n91.533182\nNA\n95.724737\nNA\n90.365454\nNA\n80.911500\nNA\n74.088571\nNA\n68.701000\nNA\n65.757619\nNA\n63.141000\nNA\n65.379565\nNA\n58.813809\nNA\n59.720476\nNA\n67.588696\nNA\n77.570476\nNA\n83.141500\nNA\n87.266364\nNA\n82.515000\nNA\n65.795455\nNA\n53.397500\nNA\n53.901053\nNA\n54.532273\nNA\n53.459130\nNA\n55.126500\nNA\n57.656818\nNA\n58.776818\nNA\n69.616191\nNA\n90.627143\nNA\n90.285000\nNA\n87.258000\nNA\n94.999565\nNA\n92.479500\nNA\n105.719474\nNA\n115.443182\nNA\n123.615909\nNA\n138.505238\nNA\n162.838182\nNA\n214.269524\nNA\n140.071364\nNA\n137.473333\nNA\n174.546500\nNA\n210.71050\nNA\n326.108696\nNA\n298.866316\nNA\n325.03619\nNA\n327.821000\nNA\n328.681429\nNA\n332.844091\nNA\n291.36810\nNA\n230.95619\nNA\n199.325455\nNA\n206.895500\nNA\n\n\nIndonesian in Japan\n17.46000\nNA\n16.71000\nNA\n18.420000\nNA\n19.57000\nNA\n18.330000\nNA\n19.380000\nNA\n19.100000\nNA\n18.390000\nNA\n18.760000\nNA\n17.27000\nNA\n16.83000\nNA\n17.520000\nNA\n17.790000\nNA\n17.7200\nNA\n18.310000\nNA\n17.710000\nNA\n16.940000\nNA\n17.69000\nNA\n16.980000\nNA\n17.000000\nNA\n17.010000\nNA\n16.490000\nNA\n16.710000\nNA\n17.720000\nNA\n17.760000\nNA\n17.96000\nNA\n17.810000\nNA\n17.670000\nNA\n17.680000\nNA\n17.510000\nNA\n17.430000\nNA\n16.230000\nNA\n15.700000\nNA\n15.230000\nNA\n16.41000\nNA\n16.580000\nNA\n15.500000\nNA\n14.69000\nNA\n13.050000\nNA\n11.020000\nNA\n9.040000\nNA\n8.790000\nNA\n9.32000\nNA\n10.160000\nNA\n10.010000\nNA\n10.370000\nNA\n9.400000\nNA\n10.150000\nNA\n8.460000\nNA\n7.990000\nNA\n8.010000\nNA\n6.680000\nNA\n6.810000\nNA\n7.080000\nNA\n6.850000\nNA\n7.420000\nNA\n7.790000\nNA\n7.930000\nNA\n7.160000\nNA\n7.090000\nNA\n9.174091\nNA\n7.128000\nNA\n5.794783\nNA\n5.709500\nNA\n5.719062\nNA\n5.523636\nNA\n5.631905\nNA\n6.130870\nNA\n7.084286\nNA\n8.792727\nNA\n9.571429\nNA\n10.709048\nNA\n11.669565\nNA\n10.471000\nNA\n8.460909\nNA\n7.475238\nNA\n8.612609\nNA\n10.437619\nNA\n9.972273\nNA\n10.590435\nNA\n11.09050\nNA\n10.035652\nNA\n9.777273\nNA\n8.951905\nNA\n8.285652\nNA\n6.501000\nNA\n5.245238\nNA\n5.079762\nNA\n5.033261\nNA\n4.389250\nNA\n4.419348\nNA\n4.280227\nNA\n5.054524\nNA\n6.017391\nNA\n5.456429\nNA\n5.573409\nNA\n4.913696\nNA\n2.920250\nNA\n3.278864\nNA\n2.126364\nNA\n2.032143\nNA\n2.050454\nNA\n2.286957\nNA\n3.625714\nNA\n4.609773\nNA\n6.121818\nNA\n6.822143\nNA\n11.608913\nNA\n20.415238\nNA\n7.168750\nNA\n6.443478\nNA\n7.894130\nNA\n10.014500\nNA\n12.017273\nNA\n14.096818\nNA\n16.643864\nNA\n24.062955\nNA\n35.065238\nNA\n31.963571\nNA\n37.419783\nNA\n26.828333\nNA\n27.82325\nNA\n38.735870\nNA\n29.014762\nNA\n21.93773\nNA\n29.538182\nNA\n41.169500\nNA\n54.157500\nNA\n44.68227\nNA\n29.74200\nNA\n25.217727\nNA\n30.717143\nNA\n\n\nNetherlands TFF\n12.33000\nNA\n12.22000\nNA\n12.510000\nNA\n12.57000\nNA\n12.570000\nNA\n12.560000\nNA\n11.380000\nNA\n11.400000\nNA\n11.380000\nNA\n11.57000\nNA\n11.64000\nNA\n11.640000\nNA\n11.390000\nNA\n11.3600\nNA\n11.360000\nNA\n11.640000\nNA\n11.410000\nNA\n11.32000\nNA\n10.980000\nNA\n10.970000\nNA\n10.960000\nNA\n10.930000\nNA\n10.960000\nNA\n10.990000\nNA\n10.900000\nNA\n10.83000\nNA\n10.690000\nNA\n10.790000\nNA\n10.640000\nNA\n10.520000\nNA\n9.400000\nNA\n10.380000\nNA\n10.400000\nNA\n10.400000\nNA\n10.16000\nNA\n10.450000\nNA\n9.500000\nNA\n9.29000\nNA\n9.290000\nNA\n7.390000\nNA\n7.370000\nNA\n7.300000\nNA\n6.68000\nNA\n6.660000\nNA\n6.490000\nNA\n6.010000\nNA\n5.870000\nNA\n5.810000\nNA\n5.090000\nNA\n4.790000\nNA\n4.090000\nNA\n4.020000\nNA\n3.990000\nNA\n4.040000\nNA\n4.300000\nNA\n4.250000\nNA\n3.960000\nNA\n4.010000\nNA\n4.540000\nNA\n5.160000\nNA\n6.278140\nNA\n6.100298\nNA\n4.945020\nNA\n5.035127\nNA\n5.077171\nNA\n4.983818\nNA\n5.096028\nNA\n5.536059\nNA\n6.040606\nNA\n5.892519\nNA\n6.734038\nNA\n7.209792\nNA\n6.648303\nNA\n7.841036\nNA\n8.536361\nNA\n7.064733\nNA\n7.460863\nNA\n7.504423\nNA\n7.621063\nNA\n8.104114\nNA\n9.51888\nNA\n8.595915\nNA\n8.204490\nNA\n7.892946\nNA\n7.161065\nNA\n5.986339\nNA\n5.197180\nNA\n4.953250\nNA\n4.360183\nNA\n3.483036\nNA\n3.595944\nNA\n3.270197\nNA\n3.086805\nNA\n3.339724\nNA\n4.781020\nNA\n4.243549\nNA\n3.626270\nNA\n2.995477\nNA\n2.789824\nNA\n2.083882\nNA\n1.462612\nNA\n1.647300\nNA\n1.640863\nNA\n2.611566\nNA\n3.854996\nNA\n4.813822\nNA\n4.783508\nNA\n5.828639\nNA\n7.302821\nNA\n6.171341\nNA\n6.205197\nNA\n7.282724\nNA\n9.004704\nNA\n10.324139\nNA\n12.524247\nNA\n15.279029\nNA\n22.232827\nNA\n29.814143\nNA\n27.383826\nNA\n37.363363\nNA\n27.890945\nNA\n26.98428\nNA\n41.727688\nNA\n31.989874\nNA\n27.46417\nNA\n32.912607\nNA\n51.145913\nNA\n69.977239\nNA\n55.17913\nNA\n20.80664\nNA\n28.790805\nNA\n35.368731\nNA\n\n\nUS, domestic market\n2.67727\nNA\n2.50486\nNA\n2.163636\nNA\n1.94875\nNA\n2.432727\nNA\n2.458571\nNA\n2.954005\nNA\n2.840548\nNA\n2.852105\nNA\n3.31913\nNA\n3.53950\nNA\n3.338235\nNA\n3.338571\nNA\n3.3055\nNA\n3.784286\nNA\n4.161364\nNA\n4.074348\nNA\n3.80600\nNA\n3.643044\nNA\n3.412273\nNA\n3.615714\nNA\n3.655217\nNA\n3.651429\nNA\n4.283636\nNA\n4.528261\nNA\n5.16350\nNA\n4.484762\nNA\n4.614091\nNA\n4.528636\nNA\n4.593333\nNA\n4.041739\nNA\n3.899048\nNA\n3.926364\nNA\n3.801304\nNA\n4.23950\nNA\n3.486522\nNA\n2.936364\nNA\n2.75650\nNA\n2.746364\nNA\n2.596364\nNA\n2.857619\nNA\n2.768636\nNA\n2.80913\nNA\n2.752857\nNA\n2.638182\nNA\n2.384546\nNA\n2.275714\nNA\n2.042174\nNA\n2.232381\nNA\n1.930476\nNA\n1.812609\nNA\n2.014286\nNA\n2.087273\nNA\n2.623636\nNA\n2.772857\nNA\n2.722174\nNA\n2.898636\nNA\n3.072381\nNA\n2.879091\nNA\n3.585454\nNA\n3.315909\nNA\n2.901500\nNA\n2.991739\nNA\n3.191500\nNA\n3.235652\nNA\n2.993636\nNA\n2.954286\nNA\n2.903913\nNA\n3.009048\nNA\n2.911818\nNA\n3.053636\nNA\n2.769524\nNA\n3.151739\nNA\n2.658000\nNA\n2.700909\nNA\n2.723810\nNA\n2.833913\nNA\n2.941429\nNA\n2.793182\nNA\n2.908261\nNA\n2.89750\nNA\n3.207391\nNA\n4.108182\nNA\n3.904286\nNA\n3.108696\nNA\n2.679500\nNA\n2.803333\nNA\n2.597273\nNA\n2.591739\nNA\n2.330000\nNA\n2.302174\nNA\n2.173636\nNA\n2.507143\nNA\n2.340870\nNA\n2.625238\nNA\n2.282727\nNA\n2.033913\nNA\n1.845000\nNA\n1.733182\nNA\n1.763636\nNA\n1.809048\nNA\n1.699545\nNA\n1.764783\nNA\n2.340476\nNA\n2.285000\nNA\n2.829091\nNA\n2.870476\nNA\n2.584783\nNA\n2.649048\nNA\n2.917000\nNA\n2.622174\nNA\n2.683636\nNA\n2.961429\nNA\n3.272727\nNA\n3.809546\nNA\n4.032273\nNA\n5.096818\nNA\n5.571429\nNA\n5.119091\nNA\n3.858261\nNA\n4.233333\nNA\n4.47500\nNA\n4.971304\nNA\n6.744762\nNA\n8.16000\nNA\n7.570909\nNA\n7.109048\nNA\n8.777391\nNA\n7.83500\nNA\n6.09381\nNA\n6.478182\nNA\n5.746818\nNA\n\n\nDubai\n108.60455\nNA\n116.67619\nNA\n122.598636\nNA\n117.49714\nNA\n107.650870\nNA\n94.438095\nNA\n99.964545\nNA\n108.954783\nNA\n110.853500\nNA\n108.42696\nNA\n107.10545\nNA\n106.397619\nNA\n108.186522\nNA\n111.1215\nNA\n105.704286\nNA\n101.495000\nNA\n100.521739\nNA\n100.38550\nNA\n103.828696\nNA\n107.071364\nNA\n108.073809\nNA\n106.866087\nNA\n106.060000\nNA\n107.932273\nNA\n103.767826\nNA\n105.25850\nNA\n104.344286\nNA\n104.820909\nNA\n105.534091\nNA\n108.168095\nNA\n105.957826\nNA\n101.608095\nNA\n96.380000\nNA\n86.377826\nNA\n76.11950\nNA\n59.825652\nNA\n46.492273\nNA\n55.92000\nNA\n54.316818\nNA\n58.433636\nNA\n63.178095\nNA\n61.797273\nNA\n55.98739\nNA\n47.980952\nNA\n45.191364\nNA\n45.859546\nNA\n41.597143\nNA\n34.138261\nNA\n27.458095\nNA\n29.672857\nNA\n35.450000\nNA\n39.323810\nNA\n44.350000\nNA\n46.315909\nNA\n42.444762\nNA\n43.969130\nNA\n43.676364\nNA\n48.802381\nNA\n44.039546\nNA\n52.136818\nNA\n53.569091\nNA\n54.295000\nNA\n51.021304\nNA\n52.554500\nNA\n50.134348\nNA\n46.356364\nNA\n47.792381\nNA\n50.269565\nNA\n53.922857\nNA\n55.774546\nNA\n60.586818\nNA\n61.384286\nNA\n66.024783\nNA\n62.631000\nNA\n63.917273\nNA\n68.667143\nNA\n74.077826\nNA\n73.311429\nNA\n72.769546\nNA\n72.594783\nNA\n76.92500\nNA\n78.762609\nNA\n64.685909\nNA\n55.669048\nNA\n59.159130\nNA\n64.548000\nNA\n66.952381\nNA\n70.939545\nNA\n68.667391\nNA\n60.915000\nNA\n62.860870\nNA\n58.504091\nNA\n60.856190\nNA\n58.863478\nNA\n61.220000\nNA\n64.453182\nNA\n63.125217\nNA\n54.048500\nNA\n33.509091\nNA\n26.192727\nNA\n32.602857\nNA\n40.068636\nNA\n42.533913\nNA\n43.917143\nNA\n41.373182\nNA\n40.814546\nNA\n43.473810\nNA\n49.626956\nNA\n54.646667\nNA\n61.250500\nNA\n64.287826\nNA\n63.128636\nNA\n66.350000\nNA\n71.303636\nNA\n72.136818\nNA\n68.731364\nNA\n72.758182\nNA\n81.437143\nNA\n78.657727\nNA\n72.847826\nNA\n83.450952\nNA\n91.52500\nNA\n107.587826\nNA\n102.755238\nNA\n107.86136\nNA\n111.859545\nNA\n100.318571\nNA\n95.693044\nNA\n89.74273\nNA\n91.21476\nNA\n84.599091\nNA\n76.855909\nNA\n\n\nU.K. Brent\n111.41500\nNA\n119.16571\nNA\n124.703182\nNA\n120.47000\nNA\n110.822174\nNA\n95.889048\nNA\n102.979546\nNA\n113.780435\nNA\n113.641500\nNA\n111.79261\nNA\n109.91182\nNA\n109.569048\nNA\n112.527826\nNA\n116.4880\nNA\n109.585714\nNA\n102.960000\nNA\n102.980435\nNA\n103.35850\nNA\n107.779565\nNA\n111.058182\nNA\n111.962857\nNA\n109.618696\nNA\n108.150476\nNA\n111.070454\nNA\n107.926957\nNA\n108.83800\nNA\n107.868095\nNA\n108.003636\nNA\n109.832273\nNA\n112.240952\nNA\n107.462609\nNA\n102.401905\nNA\n97.852727\nNA\n87.584783\nNA\n79.22950\nNA\n62.903913\nNA\n48.933182\nNA\n58.25400\nNA\n56.524546\nNA\n59.826364\nNA\n65.193810\nNA\n62.620000\nNA\n56.82957\nNA\n47.533809\nNA\n48.030454\nNA\n48.906364\nNA\n45.095238\nNA\n38.525217\nNA\n32.045238\nNA\n33.762381\nNA\n39.792174\nNA\n43.330476\nNA\n47.735000\nNA\n49.877273\nNA\n46.598571\nNA\n47.052174\nNA\n47.376818\nNA\n51.413809\nNA\n46.942273\nNA\n55.022727\nNA\n55.710909\nNA\n56.096000\nNA\n52.655217\nNA\n53.978000\nNA\n51.374348\nNA\n47.652727\nNA\n49.253333\nNA\n51.927826\nNA\n55.557619\nNA\n57.536364\nNA\n62.836364\nNA\n64.064286\nNA\n68.889130\nNA\n65.696500\nNA\n66.891818\nNA\n71.932857\nNA\n76.934348\nNA\n75.802857\nNA\n75.229091\nNA\n73.851304\nNA\n79.16300\nNA\n80.783478\nNA\n66.228636\nNA\n57.948095\nNA\n59.944783\nNA\n64.384000\nNA\n66.941905\nNA\n71.475909\nNA\n70.389130\nNA\n63.208500\nNA\n64.322174\nNA\n59.618182\nNA\n62.430952\nNA\n59.631304\nNA\n62.686667\nNA\n65.348182\nNA\n63.898696\nNA\n55.691000\nNA\n33.947273\nNA\n26.848636\nNA\n32.422381\nNA\n40.860455\nNA\n43.295652\nNA\n45.081429\nNA\n41.926818\nNA\n41.606818\nNA\n44.050000\nNA\n50.376956\nNA\n55.224762\nNA\n62.362500\nNA\n65.796087\nNA\n65.531818\nNA\n68.375238\nNA\n73.512727\nNA\n74.403182\nNA\n70.588182\nNA\n74.749545\nNA\n83.865238\nNA\n80.890455\nNA\n74.678261\nNA\n85.622381\nNA\n94.26700\nNA\n112.440000\nNA\n106.155714\nNA\n112.11364\nNA\n117.692727\nNA\n105.252857\nNA\n97.643044\nNA\n90.60818\nNA\n93.71857\nNA\n90.938636\nNA\n81.503182\nNA\n\n\nWest Texas Intermediate\n100.10227\nNA\n102.29714\nNA\n106.187273\nNA\n103.32571\nNA\n94.534783\nNA\n82.400476\nNA\n87.919091\nNA\n94.118261\nNA\n94.704500\nNA\n89.59652\nNA\n86.72091\nNA\n88.262857\nNA\n94.729565\nNA\n95.3095\nNA\n93.199048\nNA\n92.067727\nNA\n94.766957\nNA\n95.76950\nNA\n104.426522\nNA\n106.435455\nNA\n106.395238\nNA\n100.556956\nNA\n93.850952\nNA\n97.890455\nNA\n94.963043\nNA\n100.70500\nNA\n100.568095\nNA\n102.175000\nNA\n101.996364\nNA\n105.234762\nNA\n102.948261\nNA\n96.317143\nNA\n93.271818\nNA\n84.406087\nNA\n75.70750\nNA\n59.139565\nNA\n47.557273\nNA\n50.85700\nNA\n47.782273\nNA\n54.378636\nNA\n59.388571\nNA\n59.828636\nNA\n51.19870\nNA\n42.912857\nNA\n45.510000\nNA\n46.268636\nNA\n42.585238\nNA\n37.396087\nNA\n31.784286\nNA\n30.377619\nNA\n37.901739\nNA\n41.031905\nNA\n46.841818\nNA\n48.791818\nNA\n44.896191\nNA\n44.751739\nNA\n45.200000\nNA\n49.809524\nNA\n45.470909\nNA\n52.052727\nNA\n52.561818\nNA\n53.450000\nNA\n49.363044\nNA\n51.174000\nNA\n48.559565\nNA\n45.185454\nNA\n46.581905\nNA\n48.047826\nNA\n49.735714\nNA\n51.573182\nNA\n56.738182\nNA\n57.920476\nNA\n63.584783\nNA\n62.227000\nNA\n62.830455\nNA\n66.313810\nNA\n69.898696\nNA\n67.876191\nNA\n71.074545\nNA\n67.927391\nNA\n70.19050\nNA\n70.753044\nNA\n56.188182\nNA\n48.919048\nNA\n51.227391\nNA\n54.996000\nNA\n58.170476\nNA\n63.870455\nNA\n60.740000\nNA\n54.666000\nNA\n57.376087\nNA\n54.830455\nNA\n56.860000\nNA\n53.960870\nNA\n56.677619\nNA\n59.867727\nNA\n57.716957\nNA\n50.608000\nNA\n29.320455\nNA\n16.975000\nNA\n28.781429\nNA\n38.314546\nNA\n40.715217\nNA\n42.370952\nNA\n39.598182\nNA\n39.405000\nNA\n41.389048\nNA\n47.070435\nNA\n51.849524\nNA\n59.234500\nNA\n62.183913\nNA\n61.417727\nNA\n65.160952\nNA\n71.377273\nNA\n72.591818\nNA\n67.872273\nNA\n71.535000\nNA\n81.364286\nNA\n79.095455\nNA\n71.802174\nNA\n83.277619\nNA\n91.60800\nNA\n108.399130\nNA\n101.985238\nNA\n109.71273\nNA\n114.675909\nNA\n101.918095\nNA\n93.692609\nNA\n84.40000\nNA\n87.28667\nNA\n84.076364\nNA\n76.581818\nNA\n\n\nAgricultural raw materials\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nCotton\n101.11429\nNA\n100.74524\nNA\n99.504546\nNA\n100.09737\nNA\n88.534091\nNA\n82.181579\nNA\n83.968182\nNA\n84.397727\nNA\n84.150000\nNA\n81.95217\nNA\n80.87273\nNA\n83.373684\nNA\n85.506818\nNA\n89.7100\nNA\n94.447500\nNA\n92.535000\nNA\n92.622727\nNA\n93.08000\nNA\n92.615217\nNA\n92.714286\nNA\n90.092857\nNA\n89.347826\nNA\n84.647619\nNA\n87.487500\nNA\n90.963636\nNA\n94.05000\nNA\n96.947619\nNA\n94.202500\nNA\n92.712500\nNA\n90.897619\nNA\n83.836956\nNA\n73.995000\nNA\n73.381818\nNA\n70.343478\nNA\n67.52500\nNA\n68.304762\nNA\n67.350000\nNA\n69.84250\nNA\n69.352273\nNA\n71.702500\nNA\n72.863158\nNA\n72.352273\nNA\n72.34783\nNA\n71.822500\nNA\n68.736364\nNA\n69.027273\nNA\n69.221429\nNA\n70.388095\nNA\n68.750000\nNA\n66.571429\nNA\n65.457143\nNA\n69.278571\nNA\n70.277500\nNA\n74.102273\nNA\n81.064286\nNA\n80.261364\nNA\n77.861364\nNA\n78.516667\nNA\n78.922727\nNA\n79.502500\nNA\n82.330952\nNA\n85.155000\nNA\n86.782609\nNA\n87.036111\nNA\n88.638095\nNA\n84.763636\nNA\n84.088095\nNA\n79.340909\nNA\n80.604762\nNA\n78.604546\nNA\n80.411364\nNA\n85.423684\nNA\n91.056818\nNA\n88.267500\nNA\n92.135714\nNA\n92.237500\nNA\n94.478571\nNA\n97.707143\nNA\n96.179546\nNA\n94.552273\nNA\n90.35500\nNA\n86.800000\nNA\n86.775000\nNA\n85.997368\nNA\n82.354546\nNA\n81.150000\nNA\n83.809524\nNA\n87.247500\nNA\n80.138095\nNA\n77.650000\nNA\n75.539130\nNA\n70.776190\nNA\n71.311905\nNA\n73.884783\nNA\n74.857143\nNA\n75.830000\nNA\n79.068182\nNA\n76.567500\nNA\n67.686364\nNA\n63.532500\nNA\n65.702632\nNA\n67.800000\nNA\n68.523913\nNA\n70.000000\nNA\n70.813636\nNA\n74.815909\nNA\n77.723810\nNA\n81.021429\nNA\n87.235000\nNA\n92.760000\nNA\n91.452174\nNA\n90.730000\nNA\n90.886842\nNA\n94.504546\nNA\n97.700000\nNA\n101.304762\nNA\n103.681818\nNA\n117.383333\nNA\n126.543182\nNA\n120.040476\nNA\n132.332500\nNA\n138.40500\nNA\n141.126087\nNA\n155.315789\nNA\n163.97955\nNA\n154.445455\nNA\n131.035714\nNA\n124.758696\nNA\n117.94091\nNA\n100.27619\nNA\n101.165909\nNA\n100.886364\nNA\n\n\n\n\n\nDimensions:\n\n\n[1]  85 265\n\n\n\nData Cleaning Description.\n\n\n\nData Cleaning Code\ndf_commodity_price &lt;- read_excel(\"../../../data/00-raw-data/commodity_price.xlsx\")\n\ndf_commodity_price &lt;- df_commodity_price %&gt;%\n  pivot_longer(cols = -c('...1'), \n               names_to = \"Month_Year\",\n               values_to = \"Price\")\n\ndf_commodity_price &lt;- df_commodity_price %&gt;% filter(!is.na(Price) & Price != \"\")\n\ndf_commodity_price$Month_Year &lt;- as.yearmon(df_commodity_price$Month_Year, format = \"%b %Y\")\n\ndf_commodity_price$Month_Year &lt;- format(df_commodity_price$Month_Year, \"%m-%Y\")\n\ndf_commodity_price$Month_Year &lt;- paste(\"01-\", df_commodity_price$Month_Year, sep = \"\")\n\ndf_commodity_price$Month_Year &lt;- as.Date(df_commodity_price$Month_Year, format = \"%d-%m-%Y\")\n\nnames(df_commodity_price) &lt;- c('Commodity', 'DATE', 'Price')\n\n\nCleaned Data:\n\n\n\n\n\nCommodity\nDATE\nPrice\n\n\n\n\nAustralian, export markets\n2012-01-01\n123.08571\n\n\nAustralian, export markets\n2012-02-01\n124.96684\n\n\nAustralian, export markets\n2012-03-01\n114.15097\n\n\nAustralian, export markets\n2012-04-01\n110.35376\n\n\nAustralian, export markets\n2012-05-01\n103.81948\n\n\nAustralian, export markets\n2012-06-01\n92.75470\n\n\nAustralian, export markets\n2012-07-01\n91.36412\n\n\nAustralian, export markets\n2012-08-01\n93.45633\n\n\nAustralian, export markets\n2012-09-01\n93.18161\n\n\nAustralian, export markets\n2012-10-01\n86.88540\n\n\n\n\n\n\nUranium Price\nCleaned Data:\n\n\n\n\n\nCommodity\nDATE\nPrice\n\n\n\n\nUranium\n2012-01-01\n52.31250\n\n\nUranium\n2012-02-01\n52.05556\n\n\nUranium\n2012-03-01\n51.28889\n\n\nUranium\n2012-04-01\n51.30000\n\n\nUranium\n2012-05-01\n51.88889\n\n\n\n\n\nSummary:\n\n\n  Commodity              DATE                Price      \n Length:132         Min.   :2012-01-01   Min.   :18.57  \n Class :character   1st Qu.:2014-09-23   1st Qu.:25.57  \n Mode  :character   Median :2017-06-16   Median :32.14  \n                    Mean   :2017-06-16   Mean   :32.71  \n                    3rd Qu.:2020-03-08   3rd Qu.:38.26  \n                    Max.   :2022-12-01   Max.   :52.31  \n\n\nDimensions:\n\n\n[1] 132   3\n\n\n\n\nAluminum Price\nCleaned Data:\n\n\n\n\n\nCommodity\nDATE\nPrice\n\n\n\n\nAluminum\n2012-01-01\n2151.333\n\n\nAluminum\n2012-02-01\n2207.917\n\n\nAluminum\n2012-03-01\n2184.159\n\n\nAluminum\n2012-04-01\n2048.505\n\n\nAluminum\n2012-05-01\n2002.523\n\n\n\n\n\nSummary:\n\n\n  Commodity              DATE                Price     \n Length:132         Min.   :2012-01-01   Min.   :1460  \n Class :character   1st Qu.:2014-09-23   1st Qu.:1747  \n Mode  :character   Median :2017-06-16   Median :1885  \n                    Mean   :2017-06-16   Mean   :1978  \n                    3rd Qu.:2020-03-08   3rd Qu.:2089  \n                    Max.   :2022-12-01   Max.   :3498  \n\n\nDimensions:\n\n\n[1] 132   3\n\n\n\n\nCobalt Price\nCleaned Data:\n\n\n\n\n\nCommodity\nDATE\nPrice\n\n\n\n\nCobalt\n2012-01-01\n33006.67\n\n\nCobalt\n2012-02-01\n31772.62\n\n\nCobalt\n2012-03-01\n31142.73\n\n\nCobalt\n2012-04-01\n30795.66\n\n\nCobalt\n2012-05-01\n30516.82\n\n\n\n\n\nSummary:\n\n\n  Commodity              DATE                Price      \n Length:132         Min.   :2012-01-01   Min.   :22276  \n Class :character   1st Qu.:2014-09-23   1st Qu.:28823  \n Mode  :character   Median :2017-06-16   Median :31827  \n                    Mean   :2017-06-16   Mean   :40826  \n                    3rd Qu.:2020-03-08   3rd Qu.:51952  \n                    Max.   :2022-12-01   Max.   :90782  \n\n\nDimensions:\n\n\n[1] 132   3\n\n\n\n\nCopper Price\nCleaned Data:\n\n\n\n\n\nCommodity\nDATE\nPrice\n\n\n\n\nCopper\n2012-01-01\n8061.917\n\n\nCopper\n2012-02-01\n8441.488\n\n\nCopper\n2012-03-01\n8470.784\n\n\nCopper\n2012-04-01\n8285.526\n\n\nCopper\n2012-05-01\n7896.909\n\n\n\n\n\nSummary:\n\n\n  Commodity              DATE                Price      \n Length:132         Min.   :2012-01-01   Min.   : 4472  \n Class :character   1st Qu.:2014-09-23   1st Qu.: 5832  \n Mode  :character   Median :2017-06-16   Median : 6768  \n                    Mean   :2017-06-16   Mean   : 6869  \n                    3rd Qu.:2020-03-08   3rd Qu.: 7720  \n                    Max.   :2022-12-01   Max.   :10231  \n\n\nDimensions:\n\n\n[1] 132   3\n\n\n\n\nNatural Gas Price\n\nBefore cleaning the dataset, the dataset had ### rows and ### columns:\n\n\n\n\n\n\n\nData Gathering\n# Set the start and end dates\nstart_date &lt;- \"2000-01-01\"\nend_date &lt;- \"2022-12-31\"\n\n# Define the symbol for Gas Price to USD exchange rate\nsymbol &lt;- \"GASREGCOVW\"\n\n# Use getSymbols() to fetch the data\ngetSymbols(symbol, from = start_date, to = end_date, src = \"FRED\")\n\n\n[1] \"GASREGCOVW\"\n\n\nData Gathering\n# Access the data as a data frame\ndf_gas_price &lt;- as.data.frame(GASREGCOVW)\n\ndf_gas_price &lt;- rownames_to_column(df_gas_price, var = \"DATE\")\n\ndf_gas_price$DATE &lt;- as.Date(df_gas_price$DATE)\n\n\n\n\n\n\n\nDATE\nGASREGCOVW\n\n\n\n\n2000-01-03\n1.260\n\n\n2000-01-10\n1.252\n\n\n2000-01-17\n1.268\n\n\n2000-01-24\n1.307\n\n\n2000-01-31\n1.307\n\n\n2000-02-07\n1.319\n\n\n2000-02-14\n1.350\n\n\n2000-02-21\n1.400\n\n\n2000-02-28\n1.413\n\n\n2000-03-06\n1.490\n\n\n\n\n\n\n\n      DATE              GASREGCOVW   \n Min.   :2000-01-03   Min.   :1.042  \n 1st Qu.:2005-10-01   1st Qu.:1.933  \n Median :2011-06-30   Median :2.479  \n Mean   :2011-06-30   Mean   :2.510  \n 3rd Qu.:2017-03-28   3rd Qu.:3.078  \n Max.   :2022-12-26   Max.   :4.844  \n\n\nDimensions:\n\n\n[1] 1200    2\n\n\n\nData Cleaning Description.\n\n\n\nData Cleaning\n# Separate the Date by terms\ndf_gas_price &lt;- df_gas_price %&gt;%\n    mutate(Year = year(DATE)) %&gt;%\n    mutate(Month = month(DATE)) %&gt;%\n    mutate(Day = day(DATE))\n\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    group_by(Year, Month) %&gt;%\n    filter(Day == min(Day)) %&gt;%\n    ungroup()\n\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    select(GASREGCOVW, Year, Month, Day)\n\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    mutate(DATE = paste(Year, Month, '01', sep = \"-\"))\n\n\ndf_gas_price$DATE &lt;- as.Date(df_gas_price$DATE)\n\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    select(GASREGCOVW, DATE)\n\n\nCleaned Data:\n\n\n\n\n\nGASREGCOVW\nDATE\n\n\n\n\n1.260\n2000-01-01\n\n\n1.319\n2000-02-01\n\n\n1.490\n2000-03-01\n\n\n1.478\n2000-04-01\n\n\n1.386\n2000-05-01\n\n\n1.535\n2000-06-01\n\n\n1.606\n2000-07-01\n\n\n1.437\n2000-08-01\n\n\n1.502\n2000-09-01\n\n\n1.498\n2000-10-01\n\n\n\n\n\nSummary:\n\n\n   GASREGCOVW         DATE           \n Min.   :1.084   Min.   :2000-01-01  \n 1st Qu.:1.919   1st Qu.:2005-09-23  \n Median :2.485   Median :2011-06-16  \n Mean   :2.510   Mean   :2011-06-16  \n 3rd Qu.:3.089   3rd Qu.:2017-03-08  \n Max.   :4.702   Max.   :2022-12-01  \n\n\nDimensions:\n\n\n[1] 276   2\n\n\n\n\nResources Price\n\nData Cleaning Description.\n\n\n\nData Cleaning\ndf_uranium_price &lt;- df_uranium_price %&gt;% select(DATE, Price)\n\ndf_aluminum_price &lt;- df_aluminum_price %&gt;% select(DATE, Price)\n\ndf_cobalt_price &lt;- df_cobalt_price %&gt;% select(DATE, Price)\n\ndf_copper_price &lt;- df_copper_price %&gt;% select(DATE, Price)\n\nnames(df_uranium_price) &lt;- c('DATE', 'Uranium')\n\nnames(df_aluminum_price) &lt;- c('DATE', 'Aluminum')\n\nnames(df_cobalt_price) &lt;- c('DATE', 'Cobalt')\n\nnames(df_copper_price) &lt;- c('DATE', 'Copper')\n\nnames(df_gas_price) &lt;- c('Natural_Gas', 'DATE')\n\ndf_resource_price &lt;- merge(df_uranium_price, df_gas_price, by.x = 'DATE', by.y = 'DATE', all = TRUE)\n\ndf_resource_price &lt;- merge(df_resource_price, df_aluminum_price, by.x = 'DATE', by.y = 'DATE', all = TRUE)\n\ndf_resource_price &lt;- merge(df_resource_price, df_cobalt_price, by.x = 'DATE', by.y = 'DATE', all = TRUE)\n\ndf_resource_price &lt;- merge(df_resource_price, df_copper_price, by.x = 'DATE', by.y = 'DATE', all = TRUE)\n\ndf_resource_price &lt;- na.omit(df_resource_price)\n\n\nCleaned Data:\n\n\n\n\n\n\nDATE\nUranium\nNatural_Gas\nAluminum\nCobalt\nCopper\n\n\n\n\n145\n2012-01-01\n52.31250\n3.254\n2151.333\n33006.67\n8061.917\n\n\n146\n2012-02-01\n52.05556\n3.436\n2207.917\n31772.62\n8441.488\n\n\n147\n2012-03-01\n51.28889\n3.717\n2184.159\n31142.73\n8470.784\n\n\n148\n2012-04-01\n51.30000\n3.874\n2048.505\n30795.66\n8285.526\n\n\n149\n2012-05-01\n51.88889\n3.718\n2002.523\n30516.82\n7896.909\n\n\n150\n2012-06-01\n50.83333\n3.518\n1885.513\n28831.91\n7428.289\n\n\n151\n2012-07-01\n50.35556\n3.291\n1876.250\n28394.08\n7584.261\n\n\n152\n2012-08-01\n49.25000\n3.606\n1843.327\n29050.36\n7510.432\n\n\n153\n2012-09-01\n47.72500\n3.797\n2064.120\n29221.25\n8087.743\n\n\n154\n2012-10-01\n44.61111\n3.750\n1974.304\n26896.74\n8062.033\n\n\n\n\n\nSummary:\n\n\n      DATE               Uranium       Natural_Gas       Aluminum   \n Min.   :2012-01-01   Min.   :18.57   Min.   :1.680   Min.   :1460  \n 1st Qu.:2014-09-23   1st Qu.:25.57   1st Qu.:2.309   1st Qu.:1747  \n Median :2017-06-16   Median :32.14   Median :2.680   Median :1885  \n Mean   :2017-06-16   Mean   :32.71   Mean   :2.818   Mean   :1978  \n 3rd Qu.:2020-03-08   3rd Qu.:38.26   3rd Qu.:3.354   3rd Qu.:2089  \n Max.   :2022-12-01   Max.   :52.31   Max.   :4.702   Max.   :3498  \n     Cobalt          Copper     \n Min.   :22276   Min.   : 4472  \n 1st Qu.:28823   1st Qu.: 5832  \n Median :31827   Median : 6768  \n Mean   :40826   Mean   : 6869  \n 3rd Qu.:51952   3rd Qu.: 7720  \n Max.   :90782   Max.   :10231  \n\n\nDimensions:\n\n\n[1] 132   6"
  },
  {
    "objectID": "3_data_cleaning/3_2_lithium_companies.html",
    "href": "3_data_cleaning/3_2_lithium_companies.html",
    "title": "Lithium Companies Stocks",
    "section": "",
    "text": "Function Fill in NAs Stock Data\nstocks_NAs &lt;- function(data_frame){\n\n    symbol &lt;- max(data_frame$symbol)\n\n    # Select relevant columns\n    data_frame &lt;- data.frame(data_frame$date, data_frame$adjusted)\n\n    # Rename columsn\n    names(data_frame) &lt;- c(\"date\", \"adjusted\")\n\n    # Create a sequence of dates from start_date to end_date\n    start_date &lt;- as.Date(min(data_frame$date))  \n    end_date &lt;- as.Date(max(data_frame$date))\n\n    # Create data range\n    date_range &lt;- seq(start_date, end_date, by = \"1 day\")\n\n    # Create a dataset with the date range\n    date_dataset &lt;- data.frame(Date = date_range)\n\n    # Merge dataframes\n    data_frame &lt;- merge(data_frame, date_dataset, by.x = \"date\", by.y = \"Date\", all = TRUE)\n\n    # Extract rows with missing values\n    df_na_rows &lt;- data_frame[which(rowSums(is.na(data_frame)) &gt; 0),]\n\n    # Extract columns with missing values\n    df_na_cols &lt;- data_frame[, which(colSums(is.na(data_frame)) &gt; 0)]\n\n    # Modify data\n    imputed_time_series &lt;- na_ma(data_frame, k = 4, weighting = \"exponential\")\n\n    # Add modified data\n    data_frame &lt;- data.frame(imputed_time_series)\n\n    # Change data type\n    data_frame$date &lt;- as.Date(data_frame$date,format = \"%m/%d/%y\")\n\n    names(data_frame) &lt;- c(\"date\", symbol)\n\n    return(data_frame)\n}\n\n\n\n\nFunction Merge Dataframs\nmerge_dataframes &lt;- function(dataframes_list) {\n  # Perform inner join using the first data frame as the base\n  df_complete &lt;- dataframes_list[[1]]\n  \n  # Iterate over the remaining data frames and merge\n  for (i in 2:length(dataframes_list)) {\n    df_complete &lt;- merge(df_complete, dataframes_list[[i]], by = \"date\", all.x = TRUE)\n  }\n  \n  return(df_complete)\n}\n\n\n\n\nData Gathering\ndf_list &lt;- list()\n\nstart &lt;- \"2000-01-01\"\nend &lt;- \"2022-12-31\"\n\ntickers_Lithium &lt;- c(\"ALB\", \"SQM\", \"MALRY\", \"LTHM\", \"SGML\", \"GNENF\", \"PILBF\")\ntickers_EV &lt;- c(\"TSLA\", \"F\", \"LI\", \"ON\", \"RIVN\", \"XPEV\", \"LVWR\", \"AEHR\")\n\ntickers &lt;- append(tickers_Lithium, tickers_EV)\n\nfor(i in tickers){\n    df &lt;- tq_get(i, get = \"stock.prices\", from = start, to = end)\n\n    df &lt;- stocks_NAs(df)\n\n    df_list &lt;- append(df_list, list(df))\n}\n\n\nCleaned Data:\n\n\n\n\n\n\ndate\nALB\nSQM\nMALRY\nLTHM\nSGML\nGNENF\nPILBF\nTSLA\nF\nLI\nON\nRIVN\nXPEV\nLVWR\nAEHR\n\n\n\n\n8389\n2022-12-21\n231.2842\n78.71401\n52.24585\n20.90000\n31.50000\n7.866558\n2.370224\n137.5700\n10.660634\n20.80000\n65.04000\n21.03000\n11.300000\n5.010000\n22.80000\n\n\n8390\n2022-12-22\n224.6726\n76.76344\n52.24585\n20.65000\n31.50000\n7.613114\n2.356391\n125.3500\n10.235293\n20.43000\n62.12000\n19.73000\n10.830000\n5.090000\n20.86000\n\n\n8391\n2022-12-23\n225.6470\n76.67011\n52.24585\n20.76000\n30.85000\n7.397833\n2.333334\n123.1500\n10.280542\n18.77000\n62.39000\n19.14000\n10.060000\n4.990000\n20.98000\n\n\n8392\n2022-12-24\n224.2904\n76.33982\n52.24585\n20.61111\n30.62333\n7.524066\n2.342044\n123.9139\n10.281548\n19.39500\n62.48389\n19.37333\n10.310000\n4.960000\n21.09556\n\n\n8393\n2022-12-25\n219.7655\n74.71152\n52.24585\n20.27429\n29.73071\n7.426980\n2.331358\n118.8936\n10.218487\n19.08929\n61.96786\n18.73000\n10.075715\n4.802857\n20.69643\n\n\n8394\n2022-12-26\n215.4762\n73.26099\n52.24585\n19.95778\n28.78222\n7.345427\n2.322318\n114.5617\n10.162896\n18.85500\n61.48278\n18.15444\n9.811667\n4.672222\n20.30778\n\n\n8395\n2022-12-27\n212.9210\n72.58228\n52.24585\n19.94000\n29.00000\n7.299978\n2.365613\n109.1000\n10.135746\n18.54000\n61.36000\n17.74000\n9.800000\n4.760000\n20.22000\n\n\n8396\n2022-12-28\n212.8017\n71.54633\n52.24585\n19.52000\n27.42000\n7.372391\n2.213439\n112.7100\n9.909503\n18.43000\n60.28000\n17.74000\n9.390000\n4.200000\n19.90000\n\n\n8397\n2022-12-29\n216.2119\n73.61823\n52.24585\n19.80000\n27.49000\n7.348906\n2.333334\n121.8200\n10.443440\n19.49000\n62.71000\n18.73000\n9.880000\n4.650000\n20.63000\n\n\n8398\n2022-12-30\n215.6054\n74.51419\n52.24585\n19.87000\n28.22000\n7.221694\n2.333334\n123.1800\n10.524887\n20.40000\n62.37000\n18.43000\n9.940000\n4.850000\n20.10000\n\n\n\n\n\n\n\n\nSummary:\n\n\n      date                 ALB               SQM               MALRY       \n Min.   :2000-01-03   Min.   :  5.025   Min.   :  0.7274   Min.   : 1.893  \n 1st Qu.:2005-10-02   1st Qu.: 14.615   1st Qu.:  5.3422   1st Qu.: 6.596  \n Median :2011-07-02   Median : 44.952   Median : 17.3408   Median : 8.229  \n Mean   :2011-07-02   Mean   : 56.984   Mean   : 19.7365   Mean   :13.423  \n 3rd Qu.:2017-03-31   3rd Qu.: 69.799   3rd Qu.: 30.5012   3rd Qu.:12.922  \n Max.   :2022-12-30   Max.   :322.982   Max.   :100.5391   Max.   :59.326  \n                                                           NA's   :4634    \n      LTHM             SGML            GNENF            PILBF      \n Min.   : 4.190   Min.   : 1.060   Min.   : 1.323   Min.   :0.078  \n 1st Qu.: 8.117   1st Qu.: 1.436   1st Qu.: 2.767   1st Qu.:0.304  \n Median :16.765   Median : 1.894   Median : 7.901   Median :0.500  \n Mean   :16.515   Mean   : 6.615   Mean   : 8.506   Mean   :0.784  \n 3rd Qu.:23.400   3rd Qu.: 9.315   3rd Qu.:13.413   3rd Qu.:0.858  \n Max.   :35.050   Max.   :37.460   Max.   :22.228   Max.   :3.412  \n NA's   :6856     NA's   :6756     NA's   :6974     NA's   :5971   \n      TSLA               F                 LI              ON        \n Min.   :  1.053   Min.   : 0.7448   Min.   :13.62   Min.   : 0.910  \n 1st Qu.:  8.781   1st Qu.: 5.6821   1st Qu.:21.15   1st Qu.: 6.110  \n Median : 16.204   Median : 7.8660   Median :27.30   Median : 8.588  \n Mean   : 58.739   Mean   : 7.8924   Mean   :26.56   Mean   :13.597  \n 3rd Qu.: 24.389   3rd Qu.: 9.5818   3rd Qu.:30.94   3rd Qu.:15.530  \n Max.   :409.970   Max.   :22.0766   Max.   :43.96   Max.   :76.710  \n NA's   :3830                        NA's   :7514    NA's   :120     \n      RIVN             XPEV            LVWR            AEHR       \n Min.   : 17.74   Min.   : 6.41   Min.   : 4.20   Min.   : 0.500  \n 1st Qu.: 30.65   1st Qu.:22.14   1st Qu.: 9.75   1st Qu.: 1.702  \n Median : 34.41   Median :32.83   Median : 9.88   Median : 2.690  \n Mean   : 48.52   Mean   :31.74   Mean   : 9.54   Mean   : 3.829  \n 3rd Qu.: 60.29   3rd Qu.:42.02   3rd Qu.: 9.94   3rd Qu.: 4.510  \n Max.   :172.01   Max.   :72.17   Max.   :11.21   Max.   :27.100  \n NA's   :7982     NA's   :7542    NA's   :7631                    \n\n\n\nDimensions:\n\n\n[1] 8398   16\n\n\n\n\nData Gathering\ndf_companies2 &lt;- read_excel('../../../data/00-raw-data/Lithium_Companies.xlsx', sheet = \"Sheet1\")\n\ndf_companies2 &lt;- na.omit(df_companies2)\n\nknitr::kable(head(df_companies2, 5))\n\n\n\n\n\nType\nName\nTicker\n\n\n\n\nLithium Production\nAlbemarle Corporation\nALB\n\n\nLithium Production\nLivent Corporation\nLTHM\n\n\nLithium Production\nSigma Lithium Corporation\nSGML\n\n\nLithium Production\nPiedmont Lithium Inc.\nPLL\n\n\nElectric Vehicles\nTesla, Inc.\nTSLA\n\n\n\n\n\n\n\n\n\nData Gathering\nfile &lt;- '../../../data/00-raw-data/Lithium_Companies.xlsx'\n\nsheet_names &lt;- excel_sheets(file)\n\nsheet_names &lt;- sheet_names[2:length(sheet_names)]\n\ncompanies_list &lt;- list()\n\n# Print the list of sheet names\nfor(i in (2:length(sheet_names))){\n  name &lt;- unlist(strsplit(sheet_names[i], \" \"))\n  companies_list &lt;- append(companies_list, name[1])\n}\n\n# Function to read and merge data for a company\nread_and_merge &lt;- function(company_name) {\n  # Read Income Statement and Balance Sheet sheets\n  income_sheet &lt;- read_excel(file, sheet = paste0(company_name, \" Income Statement\"))\n  balance_sheet &lt;- read_excel(file, sheet = paste0(company_name, \" Balance Sheet\"))\n\n  income_sheet$'Quarter Ended' &lt;- as.Date(income_sheet$'Quarter Ended')\n  balance_sheet$'Quarter Ended' &lt;- as.Date(balance_sheet$'Quarter Ended')\n\n  # Merge by \"Quarter Ended\"\n  merged_data &lt;- merge(income_sheet, balance_sheet, by = \"Quarter Ended\")\n\n  merged_data &lt;- merged_data %&gt;% mutate(Company = company_name)\n\n  return(merged_data)\n\n}\n\n# Initialize an empty list to store dataframes\nmerged_data_list &lt;- list()\n\n# Loop through each company, read and merge data, and store in the list\nfor (i in companies_list) {\n  merged_data &lt;- read_and_merge(i)\n  merged_data_list[[i]] &lt;- merged_data\n}\n\n\nCleaned Data:\n\n\n\n\n\n\nQuarter Ended\nRevenue\nRevenue Growth (YoY)\nCost of Revenue\nGross Profit\nSelling, General & Admin\nResearch & Development\nOther Operating Expenses\nOperating Expenses\nOperating Income\nInterest Expense / Income\nOther Expense / Income\nPretax Income\nIncome Tax\nNet Income\nNet Income Growth\nShares Outstanding (Basic)\nShares Outstanding (Diluted)\nShares Change\nEPS (Basic)\nEPS (Diluted)\nEPS Growth\nFree Cash Flow\nFree Cash Flow Per Share\nGross Margin\nOperating Margin\nProfit Margin\nFree Cash Flow Margin\nEffective Tax Rate\nEBITDA\nEBITDA Margin\nDepreciation & Amortization\nEBIT\nEBIT Margin\nCash & Equivalents\nCash & Cash Equivalents\nCash Growth\nReceivables\nInventory\nOther Current Assets\nTotal Current Assets\nProperty, Plant & Equipment\nLong-Term Investments\nGoodwill and Intangibles\nOther Long-Term Assets\nTotal Long-Term Assets\nTotal Assets\nAccounts Payable\nDeferred Revenue\nCurrent Debt\nOther Current Liabilities\nTotal Current Liabilities\nLong-Term Debt\nOther Long-Term Liabilities\nTotal Long-Term Liabilities\nTotal Liabilities\nTotal Debt\nDebt Growth\nCommon Stock\nRetained Earnings\nComprehensive Income\nShareholders' Equity\nNet Cash / Debt\nNet Cash Per Share\nWorking Capital\nBook Value Per Share\nCompany\n\n\n\n\n22\n2022-09-30\n231600.0\n1.2355\n112200.0\n119400.0\n15000.0\n900.0\n800\n16700.0\n102700.0\n0.0\n3600.0\n99100.0\n21500.0\n77600.0\n-\n179300\n209400.0\n0.2958\n0.43\n0.37\n-\n175300.0\n0.98\n0.5155\n0.4434\n0.3351\n0.7569\n0.217\n105700.0\n0.4564\n6600.0\n99100.0\n0.4279\n211600.0\n211600\n0.0835\n164000.0\n141800.0\n56500.0\n573900.0\n887100.0\n433900.0\n0.0\n112400.0\n1433400.0\n2007300.0\n72200.0\n198000.0\n14500.0\n-121700.0\n163000.0\n245700.0\n244000.0\n489700.0\n652700.0\n260200.0\n0.0552\n1157100.0\n251700.0\n-54200.0\n1354600.0\n-48600\n-0.23\n410900.0\n7.55\nLTHM\n\n\n23\n2022-12-31\n219400.0\n0.7852\n105500.0\n113900.0\n14600.0\n1300.0\n3100\n19000.0\n94900.0\n0.0\n6700.0\n88200.0\n5500.0\n82700.0\n10.0267\n179300\n208800.0\n0.1329\n0.46\n0.4\n7.0\n17900.0\n0.1\n0.5191\n0.4325\n0.3769\n0.0816\n0.0624\n96500.0\n0.4398\n8300.0\n88200.0\n0.4020\n189000.0\n189000\n0.6726\n141600.0\n152300.0\n61100.0\n544000.0\n973100.0\n440300.0\n0.0\n116800.0\n1530200.0\n2074200.0\n81700.0\n213500.0\n900.0\n-147400.0\n148700.0\n246100.0\n236400.0\n482500.0\n631200.0\n247000.0\n4.0E-4\n1159600.0\n334400.0\n-51000.0\n1443000.0\n-58000\n-0.28\n395300.0\n8.05\nLTHM\n\n\n24\n2023-03-31\n253500.0\n0.7666\n87500.0\n166000.0\n16300.0\n1000.0\n1900\n19200.0\n146800.0\n0.0\n8100.0\n138700.0\n23900.0\n114800.0\n1.1579\n179600\n209200.0\n0.093\n0.64\n0.55\n0.9643\n29400.0\n0.16\n0.6548\n0.5791\n0.4529\n0.116\n0.1723\n145500.0\n0.574\n6800.0\n138700.0\n0.5471\n194100.0\n194100\n1.8336\n112900.0\n184100.0\n67500.0\n558600.0\n1045500.0\n453300.0\n0.0\n122900.0\n1621700.0\n2180300.0\n64700.0\n200500.0\n1100.0\n-134200.0\n132100.0\n246800.0\n240400.0\n487200.0\n619300.0\n247900.0\n0.0041\n1161100.0\n449200.0\n-49300.0\n1561000.0\n-53800\n-0.26\n426500.0\n8.69\nLTHM\n\n\n25\n2023-06-30\n235800.0\n0.0782\n92400.0\n143400.0\n17600.0\n1000.0\n24200\n42800.0\n100600.0\n0.0\n-4200.0\n104800.0\n14600.0\n90200.0\n0.5033\n179700\n209500.0\n0.0662\n0.5\n0.43\n0.3871\n-5600.0\n-0.03\n0.6081\n0.4266\n0.3825\n-0.0237\n0.1393\n111800.0\n0.4741\n7000.0\n104800.0\n0.4444\n167800.0\n167800\n2.4245\n122300.0\n197800.0\n44800.0\n532700.0\n1144200.0\n455700.0\n0.0\n151200.0\n1751100.0\n2283800.0\n80500.0\n200300.0\n1000.0\n-140300.0\n141500.0\n248700.0\n240600.0\n489300.0\n630800.0\n249700.0\n-0.0407\n1163500.0\n539400.0\n-49900.0\n1653000.0\n-81900\n-0.39\n391200.0\n9.2\nLTHM\n\n\n26\n2023-09-30\n211400.0\n-0.0872\n94900.0\n116500.0\n13200.0\n1300.0\n8600\n23100.0\n93400.0\n0.0\n-3300.0\n96700.0\n9300.0\n87400.0\n0.1263\n179700\n209300.0\n-5.0E-4\n0.49\n0.42\n0.1351\n-1400.0\n-0.01\n0.5511\n0.4418\n0.4134\n-0.0066\n0.0962\n104400.0\n0.4939\n7700.0\n96700.0\n0.4574\n112600.0\n112600\n-0.4679\n110100.0\n202700.0\n52800.0\n478200.0\n1221800.0\n504800.0\n0.0\n156300.0\n1882900.0\n2361100.0\n71100.0\n207700.0\n1100.0\n-141600.0\n138300.0\n248600.0\n233600.0\n482200.0\n620500.0\n249700.0\n-0.0404\n1165900.0\n626800.0\n-52100.0\n1740600.0\n-137100\n-0.66\n339900.0\n9.69\nLTHM\n\n\n\n\n\n\n\n\n\n\n\nQuarter Ended\nRevenue\nRevenue Growth (YoY)\nCost of Revenue\nGross Profit\nSelling, General & Admin\nResearch & Development\nOperating Expenses\nOperating Income\nInterest Expense / Income\nOther Expense / Income\nPretax Income\nIncome Tax\nNet Income\nPreferred Dividends\nNet Income Common\nShares Outstanding (Basic)\nShares Outstanding (Diluted)\nShares Change\nEPS (Basic)\nEPS (Diluted)\nGross Margin\nOperating Margin\nProfit Margin\nEBIT\nEBIT Margin\nCash & Equivalents\nShort-Term Investments\nCash & Cash Equivalents\nCash Growth\nReceivables\nInventory\nOther Current Assets\nTotal Current Assets\nProperty, Plant & Equipment\nLong-Term Investments\nGoodwill and Intangibles\nOther Long-Term Assets\nTotal Long-Term Assets\nTotal Assets\nAccounts Payable\nDeferred Revenue\nCurrent Debt\nOther Current Liabilities\nTotal Current Liabilities\nLong-Term Debt\nOther Long-Term Liabilities\nTotal Long-Term Liabilities\nTotal Liabilities\nTotal Debt\nDebt Growth\nCommon Stock\nRetained Earnings\nComprehensive Income\nShareholders' Equity\nNet Cash / Debt\nNet Cash / Debt Growth\nNet Cash Per Share\nWorking Capital\nBook Value Per Share\nCompany\n\n\n\n\n14\n2022-09-30\n6823487.0\n0.1929\n5900249.0\n923238.0\n1626343.0\n1498550.0\n3124893.0\n-2201655.0\n38968.0\n114508.0\n-2355131.0\n21017.0\n-2376148.0\n0.0\n-2376148.0\n1718163\n1718163.0\n0.0167\n-2.76\n-2.76\n0.1353\n-0.3227\n-0.3482\n-2316163.0\n-0.3394\n1.154382E7\n2.1832643E7\n33376463\n-0.2309\n3761939.0\n4378007.0\n2423483.0\n4.3939892E7\n1.1434988E7\n1.2567331E7\n3624048.0\n223883.0\n2.785025E7\n7.1790142E7\n1.4999534E7\n981130.0\n2608190.0\n4692907.0\n2.3281761E7\n5939362.0\n3127890.0\n9067252.0\n3.2349013E7\n8547552.0\n2.2478\n6.0548396E7\n-2.2969363E7\n1862096.0\n3.9441129E7\n24828911\n-0.3909\n14.45\n2.0658131E7\n45.91\nXPEV\n\n\n15\n2022-12-31\n5140349.0\n-0.3992\n4695285.0\n445064.0\n1755815.0\n1230049.0\n2985864.0\n-2540800.0\n51079.0\n-220259.0\n-2371620.0\n-10445.0\n-2361175.0\n0.0\n-2361175.0\n1720449\n1720449.0\n0.0472\n-2.74\n-2.74\n0.0866\n-0.4943\n-0.4593\n-2320541.0\n-0.4514\n1.4714046E7\n1.7905948E7\n32619994\n-0.2085\n3919970.0\n4521373.0\n2466084.0\n4.3527421E7\n1.2561363E7\n1.1410125E7\n3790826.0\n201271.0\n2.7963585E7\n7.1491006E7\n1.4313967E7\n1083249.0\n3800159.0\n4917478.0\n2.4114853E7\n7265376.0\n3200112.0\n1.0465488E7\n3.4580341E7\n1.1065535E7\n2.417\n6.0697557E7\n-2.5330916E7\n1544024.0\n3.6910665E7\n21554459\n-0.4324\n12.53\n1.9412568E7\n42.91\nXPEV\n\n\n16\n2023-03-31\n4033420.0\n-0.459\n3966388.0\n67032.0\n1386620.0\n1295854.0\n2682474.0\n-2615442.0\n62667.0\n-347308.0\n-2330801.0\n6157.0\n-2336958.0\n0.0\n-2336958.0\n1722080\n1722080.0\n0.0114\n-2.72\n-2.72\n0.0166\n-0.6484\n-0.5794\n-2268134.0\n-0.5623\n9138493.0\n1.8271774E7\n27410267\n-0.2557\n3777231.0\n4324646.0\n2547619.0\n3.8059763E7\n1.2796482E7\n1.2585271E7\n3850917.0\n166602.0\n2.9399272E7\n6.7459035E7\n1.1477854E7\n1111605.0\n4878397.0\n5164485.0\n2.2632341E7\n7731497.0\n2640805.0\n1.0372302E7\n3.3004643E7\n1.2609894E7\n1.7893\n6.0822195E7\n-2.7667874E7\n1300071.0\n3.4454392E7\n14800373\n-0.5418\n8.59\n1.5427422E7\n40.02\nXPEV\n\n\n17\n2023-06-30\n5062696.0\n-0.3192\n5260087.0\n-197391.0\n1543625.0\n1367107.0\n2910732.0\n-3108123.0\n67007.0\n-378695.0\n-2796435.0\n8217.0\n-2804652.0\n0.0\n-2804652.0\n1723370\n1723370.0\n0.0087\n-3.26\n-3.26\n-0.039\n-0.6139\n-0.554\n-2729428.0\n-0.5391\n1.1686319E7\n1.9113899E7\n30800218\n-0.0875\n3618975.0\n3572087.0\n2364098.0\n4.0355378E7\n1.3141341E7\n9195075.0\n3897447.0\n95423.0\n2.6329286E7\n6.6684664E7\n1.1336543E7\n1127554.0\n5430943.0\n5537383.0\n2.3432423E7\n7922501.0\n2656777.0\n1.0579278E7\n3.4011701E7\n1.3353444E7\n1.4897\n6.0969152E7\n-3.0472526E7\n2176337.0\n3.2672963E7\n17446774\n-0.3854\n10.12\n1.6922955E7\n37.92\nXPEV\n\n\n18\n2023-09-30\n8529521.0\n0.25\n8757479.0\n-227958.0\n1692194.0\n1305868.0\n2998062.0\n-3226020.0\n65767.0\n594081.0\n-3885868.0\n682.0\n-3886550.0\n0.0\n-3886550.0\n1729980\n1729980.0\n0.0069\n-4.5\n-4.5\n-0.0267\n-0.3782\n-0.4557\n-3820101.0\n-0.4479\n1.3337268E7\n1.9527892E7\n32865160\n-0.0153\n3272267.0\n5287550.0\n2232540.0\n4.3657517E7\n1.2472369E7\n1.0422772E7\n3842176.0\n147690.0\n2.6885007E7\n7.0542524E7\n1.6618544E7\n1215621.0\n5478530.0\n6978279.0\n3.0290974E7\n8755790.0\n2701822.0\n1.1457612E7\n4.1748586E7\n1.423432E7\n0.6653\n6.1099758E7\n-3.4359076E7\n2053256.0\n2.8793938E7\n18630840\n-0.2496\n10.77\n1.3366543E7\n33.29\nXPEV"
  },
  {
    "objectID": "5_1_NaiveBayes_Text.html#prepare-data-for-naïve-bayes",
    "href": "5_1_NaiveBayes_Text.html#prepare-data-for-naïve-bayes",
    "title": "Naive Bayes",
    "section": "Prepare Data for Naïve Bayes",
    "text": "Prepare Data for Naïve Bayes\n\nOverview of NB labeled text.\n\nDataset\n\n\n                  name  ...                                               text\n0             Engadget  ...  If you recently moved into a new place or are ...\n1            The Verge  ...  Tesla Cybertruck will usher in a new Powershar...\n2                Wired  ...  The robotic line cooks were deep in their reci...\n3         The Next Web  ...  Renewable energies like wind and solar are cle...\n4             ABC News  ...  NEW YORK -- One person was killed and six othe...\n..                 ...  ...                                                ...\n183            Fortune  ...  United Auto Workers union members have voted t...\n184  New York Magazine  ...  Donald Trump is on pace to win back the White ...\n185           ABC News  ...  RABAT, Morocco -- A mining company controlled ...\n186            Fortune  ...  President Biden has been touting his economic ...\n187            Fortune  ...  When Bill Gates speaks, people listen. His con...\n\n[188 rows x 6 columns]\n\n\nShape\n\n\n(188, 6)"
  },
  {
    "objectID": "5_1_NaiveBayes_Text.html#naïve-bayes-nb-with-labeled-text-data",
    "href": "5_1_NaiveBayes_Text.html#naïve-bayes-nb-with-labeled-text-data",
    "title": "Naive Bayes",
    "section": "Naïve Bayes (NB) with Labeled Text Data",
    "text": "Naïve Bayes (NB) with Labeled Text Data\n\n\nClean Text Data\nnew_text=\"\"\n\nfor index, row in df.iterrows():\n    for character in df.at[index, 'text']:\n        if character in string.printable:\n            new_text+=character\n    df.at[index, 'text'] = new_text\n    new_text=\"\"\n\n\n# Convert from string labels to integers\nlabels=[]; #y1=[]; y2=[]\ny1=[]\nfor label in df[\"label\"]:\n    if label not in labels:\n        labels.append(label)\n        print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            y1.append(i)\n\n\nindex = 0 : label = positive\nindex = 1 : label = neutral\nindex = 2 : label = negative\n\n\nClean Text Data\ny1=np.array(y1)\n\n# Convert dataframe to list of strings\ncorpus=df[\"text\"].to_list()\ny2=df[\"sentiment\"].to_numpy()\n\nprint(\"number of text chunks = \",len(corpus))\n\n\nnumber of text chunks =  188\n\n\nClean Text Data\nprint(corpus[0:3])\n\n\n[\"If you recently moved into a new place or are just looking to update your home's security, now's a good time to do so. Though Black Friday has come and gone, Blink's video doorbell and two fourth-gen [+1310 chars]\", 'Tesla Cybertruck will usher in a new Powershare bidirectional charging feature\\r\\nTesla Cybertruck will usher in a new Powershare bidirectional charging feature\\r\\n / The EV maker finally jumps on the ve [+2497 chars]', 'The robotic line cooks were deep in their recipe, toiling away in a room tightly packed with equipment. In one corner, an articulated arm selected and mixed ingredients, while another slid back and f [+3678 chars]']\n\n\nClean Text Data\n# Vectorize the text data\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\nvectorizer=CountVectorizer(min_df=0.0001)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \nXs  =  vectorizer.fit_transform(corpus)   \nX = np.array(Xs.todense())\n\n#CONVERT TO ONE-HOT VECTORS\nmaxs=np.max(X,axis=0)\nX=np.ceil(X/maxs)\n\n# DOUBLE CHECK \nprint(X.shape,y1.shape,y2.shape)\n\n\n(188, 1976) (188,) (188,)\n\n\nClean Text Data\nprint(\"DATA POINT-0:\",X[0,0:10],\"y1 =\",y1[0],\"  y2 =\",y2[0])\n\n\nDATA POINT-0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] y1 = 0   y2 = 0.965\n\n\n\n\nData Partitioning\n# Partition of dataset into training and test set\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y1, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\n\n\nx_train.shape       : (150, 1976)\n\n\nData Partitioning\nprint(\"y_train.shape        :\",y_train.shape)\n\n\ny_train.shape       : (150,)\n\n\nData Partitioning\nprint(\"X_test.shape     :\",x_test.shape)\n\n\nX_test.shape        : (38, 1976)\n\n\nData Partitioning\nprint(\"y_test.shape     :\",y_test.shape)\n\n\ny_test.shape        : (38,)\n\n\n\n\nCreate functions\ndef report(y,ypred):\n      # Accuracy compute \n      print(\"Accuracy:\",accuracy_score(y, ypred)*100)\n      print(\"Number of mislabeled points out of a total %d points = %d\"\n            % (y.shape[0], (y != ypred).sum()))\n\ndef print_model_summary():\n      # Label predictions for training and test set\n      yp_train = model.predict(x_train)\n      yp_test = model.predict(x_test)\n\n      print(\"ACCURACY CALCULATION\\n\")\n\n      print(\"TRAINING SET:\")\n      report(y_train,yp_train)\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      report(y_test,yp_test)\n\n      print(\"\\nCHECK FIRST 20 PREDICTIONS\")\n      print(\"TRAINING SET:\")\n      print(y_train[0:20])\n      print(yp_train[0:20])\n      print(\"ERRORS:\",yp_train[0:20]-y_train[0:20])\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      print(y_test[0:20])\n      print(yp_test[0:20])\n      print(\"ERRORS:\",yp_test[0:20]-y_test[0:20])\n\n\n\n\nCreate functions\n# Model \nmodel = MultinomialNB()\n\n# Train model \nmodel.fit(x_train,y_train)\n\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\nCreate functions\n# Show summary using previous function\nprint_model_summary()\n\n\nACCURACY CALCULATION\n\nTRAINING SET:\nAccuracy: 99.33333333333333\nNumber of mislabeled points out of a total 150 points = 1\n\nTEST SET (UNTRAINED DATA):\nAccuracy: 65.78947368421053\nNumber of mislabeled points out of a total 38 points = 13\n\nCHECK FIRST 20 PREDICTIONS\nTRAINING SET:\n[2 1 0 1 1 2 2 0 2 2 1 0 0 1 1 0 2 1 0 1]\n[2 1 0 1 1 2 2 0 2 2 1 0 0 1 1 0 2 1 0 1]\nERRORS: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\nTEST SET (UNTRAINED DATA):\n[2 0 1 0 1 0 2 2 0 0 2 0 1 2 1 2 1 2 1 1]\n[1 0 0 0 1 0 2 2 0 0 2 0 1 1 1 2 0 0 1 1]\nERRORS: [-1  0 -1  0  0  0  0  0  0  0  0  0  0 -1  0  0 -1 -2  0  0]\n\n\n\n\nResults\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ny_pred = model.predict(x_test)\n\n# Print model summary\ndef print_model_summary():\n    # print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n    print(\"Classification Report:\")\n    print(metrics.classification_report(y_test, y_pred))\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n# Show summary\nprint_model_summary()\n\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.60      0.60      0.60        10\n           1       0.69      0.69      0.69        13\n           2       0.67      0.67      0.67        15\n\n    accuracy                           0.66        38\n   macro avg       0.65      0.65      0.65        38\nweighted avg       0.66      0.66      0.66        38"
  },
  {
    "objectID": "5_NaiveBayes.html#introduction-to-naive-bayes",
    "href": "5_NaiveBayes.html#introduction-to-naive-bayes",
    "title": "Naive Bayes",
    "section": "Introduction to Naive Bayes",
    "text": "Introduction to Naive Bayes\n\nOverview of Naive Bayes classification.\nHow it works?\nExplain the probabilistic nature of Naive Bayes and its Bayes’ theorem foundation.\nDefine the objectives of what you are trying to do.\nWhat you aim to achieve through Naive Bayes classification.\nDescribe different variants of Naive Bayes, such as Gaussian, Multinomial, and Bernoulli Naive Bayes, and explain when to use each.\n\n\n\nPython Libraries\n\nimport pandas as pd\nimport numpy as np \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nimport sklearn \nfrom scipy.stats import spearmanr\n\n# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\n# Multinomial Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\n\n\n# Split the data\nfrom sklearn.model_selection import train_test_split\n\n# Performance Metrics\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport random\n\n# Features Text Data\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn import metrics\n\n\n\n\nFuction print_model_summary()\ndef print_model_summary(y_test, y_pred):\n    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n    print(\"Classification Report:\")\n    print(\"\\n\")\n    print(metrics.classification_report(y_test, y_pred))\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n\n\n\nEV Characteristics\n\n\nLithium News Sentiments\n\n\nLithium News Categories\n\n\n\n\nData Selection\n\nFor this section…\n\n\n\nFuction train_GNB_model()\ndef train_GNB_model(x_train,y_train,x_test,y_test,i_print=False):\n\n    # INSERT CODE HERE  \n    gnb_model = GaussianNB()\n\n    gnb_model.fit(x_train, y_train)\n\n    y_train_pred = gnb_model.predict(x_train)\n    y_test_pred = gnb_model.predict(x_test)\n\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n\n    if i_print == True:\n        print(train_accuracy*100, test_accuracy*100)\n\n    return train_accuracy, test_accuracy\n\n\n\n\n{python}\n# Save dataframe as a new file\ndf_vehicles = pd.read_csv('../../data/01-modified-data/clean_vehicles.csv')\n\ndf_vehicles = df_vehicles.dropna()\n\nx = df_vehicles.columns[df_vehicles.dtypes != 'object']\n\nx = df_vehicles[x]\n\ny = pd.Categorical(df_vehicles['Continent']).codes\n\nprint(pd.Categorical(df_vehicles['Continent']).categories)\n\n\nIndex(['Asia', 'Europe', 'North America'], dtype='object')\n\n\n\n\nResults\n\nFor this section…\n\n\n\n{python}\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nprint(\"Train Set Accuracy:\", train_GNB_model(x_train,y_train,x_test,y_test,i_print=False)[0])\nprint(\"Test Set Accuracy:\", train_GNB_model(x_train,y_train,x_test,y_test,i_print=False)[1])\n\n\n\n\n57.14285714285714 62.903225806451616\nTrain Set Accuracy: 0.5714285714285714\n\n\n57.14285714285714 62.903225806451616\nTest Set Accuracy: 0.6290322580645161\n\n\n\n\n\n\nData Selection\n\nFor this section…\n\n\n\n{python}\ndf_sentiment = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\n# Rename columns\ndf_sentiment = df_sentiment.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf_sentiment = df_sentiment[['label', 'text']]\n\nvectorizer = CountVectorizer()\n\nx1 = vectorizer.fit_transform(df_sentiment['text'])\ny1 = df_sentiment['label']\n\nx1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.2, random_state=42)\n\nmodel = MultinomialNB()\n\nmodel.fit(x1_train, y1_train)\n\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n{python}\n\ny1_pred = model.predict(x1_test)\n\n\n\n\nResults\n\nFor this section…\n\n\n\n{python}\n# Show summary\nprint_model_summary(y1_test, y1_pred)\n\n\n\n\n\n\n\n\n\nData Selection\n\nFor this section…\n\n\n\n{python}\ndf_sentiment = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\n# Rename columns\ndf_sentiment = df_sentiment.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf_sentiment = df_sentiment[['category', 'text']]\n\nvectorizer = CountVectorizer()\n\nx2 = vectorizer.fit_transform(df_sentiment['text'])\ny2 = df_sentiment['category']\n\nx2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.2, random_state=42)\n\nmodel = MultinomialNB()\n\nmodel.fit(x2_train, y2_train)\n\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n{python}\n\ny2_pred = model.predict(x2_test)\n\n\n\n\nResults\n\nFor this section…\n\n\n\n{python}\n# Show summary\nprint_model_summary(y2_test, y2_pred)\n\n\nAccuracy: 0.6052631578947368\nClassification Report:\n\n              precision    recall  f1-score   support\n\n    business       0.60      0.60      0.60        10\n     general       0.73      0.80      0.76        20\n     science       1.00      0.25      0.40         4\n  technology       0.00      0.00      0.00         4\n\n    accuracy                           0.61        38\n   macro avg       0.58      0.41      0.44        38\nweighted avg       0.65      0.61      0.60        38"
  },
  {
    "objectID": "6_clustering.html",
    "href": "6_clustering.html",
    "title": "Data Clustering",
    "section": "",
    "text": "Libraries\n# Import Data\nimport pandas as pd\n\n# Data Cleaning\nimport string\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Clustering\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\n\nimport sklearn.cluster"
  },
  {
    "objectID": "6_clustering.html#introduction",
    "href": "6_clustering.html#introduction",
    "title": "Data Clustering",
    "section": "Introduction",
    "text": "Introduction\n\nProvided brief summary (1 to 2 paragraphs) about your feature data , and what you are trying to achieve with your clustering analysis."
  },
  {
    "objectID": "6_clustering.html#theory",
    "href": "6_clustering.html#theory",
    "title": "Data Clustering",
    "section": "Theory",
    "text": "Theory\n\nWrite a brief technical write up about how EACH clustering method works (KMEAN, DBSAN, hierarchical clustering). Also include details on model selection methods that you use (elbow, silhouette, etc)."
  },
  {
    "objectID": "6_clustering.html#methods",
    "href": "6_clustering.html#methods",
    "title": "Data Clustering",
    "section": "Methods",
    "text": "Methods\n\nDescribe your coding workflow\n\n\nData selection\n\nIf you have not done so already, create either a numeric record feature dataset AND/OR a text feature dataset from your existing data.\n\nText Data\n\n\n{python}\n# Read csv\ndf = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\n# Rename columns\ndf = df.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\n# Print dataframe\ndf['text']\n\n\n0     Redwood Materials will recycle stationary stor...\n1     Enlarge/ GreenPower has given its class-D elec...\n2     Enlarge/ These are piles of lithium harvested ...\n3     BP's EV charging arm has bought $100 million w...\n4     A lithium-ion battery recycler and a program t...\n                            ...                        \n80    The effort to climate-proof our housing is run...\n81    ROME, Nov 6 (Reuters) - Italy's Industrie De N...\n82    WASHINGTON, Nov 13 (Reuters) - Democratic Sena...\n83    Your EV may go a long way between charges, but...\n84    The administration is well aware of the high s...\nName: text, Length: 85, dtype: object\n\n\n\n\nFeature selection\n\nIf you want, you can perform filter based feature selection on your data-set as a pre-processing step before clustering\nIn this case the feature selection that has been done is keeping the terms that appear in more than 1% of the articles.\n\nClean Vectorized Text Data\n\n\n{python}\nnew_text=\"\"\n\nfor index, row in df.iterrows():\n    for character in df.at[index, 'text']:\n        if character in string.printable:\n            new_text+=character\n    df.at[index, 'text'] = new_text\n    new_text=\"\"\n\n\n# Convert from string labels to integers\nlabels=[]; #y1=[]; y2=[]\ny1=[]\nfor label in df[\"label\"]:\n    if label not in labels:\n        labels.append(label)\n        # print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            y1.append(i)\n\ny1=np.array(y1)\n\n# Convert dataframe to list of strings\ncorpus=df[\"text\"].to_list()\ny2=df[\"sentiment\"].to_numpy()\n\n# Vectorize the text data\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \nvectorizer=CountVectorizer(min_df=0.01)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \nXs  =  vectorizer.fit_transform(corpus)   \nX = np.array(Xs.todense())\n\nX\n\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 1, 0],\n       [0, 0, 0, ..., 0, 0, 0]])\n\n\n\n\n\nNumber of terms: 1184\n\n\n\n\nHyper-parameter tuning\n\nFor each of the three clustering algorithms, perform any relevant parameter tuning in an attempt to achieve the optimal clustering results\nExplore different choices of distance metric for the algorithm. Which distance metric seems to works best in which cases and why?\n\nSilhouette Score Function\n\n\n{python}\ndef maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X)\n\n    # LOOP OVER HYPER-PARAM\n    params=[]; \n    sil_scores=[]\n    sil_max=-10\n\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.25*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue\n\n        if(i_print): print(param,sil_scores[-1])\n\n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")\n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\nK-MEANS\n\n\n{python}\nopt_labels_1=maximize_silhouette(X,algo=\"kmeans\",nmax=5, i_plot=True)\n\n\nOPTIMAL PARAMETER = 5\n\n\n\n\n\n{python}\n\n# plot(X,opt_labels_1)\n\n\nDBSCAN\n\n\n{python}\n# opt_labels_2=maximize_silhouette(X,algo=\"dbscan\",nmax=5, i_plot=True)\n# plot(X, opt_labels_2)\n\n\nHIERARCHICAL\n\n\n{python}\n# opt_labels_3=maximize_silhouette(X,algo=\"ag\",nmax=5, i_plot=True)\n# plot(X,opt_labels_3)\n\n\n\n\nFinal Models\n\nRe-do the analysis one last time with the optimal parameter choice to get your “final results”"
  },
  {
    "objectID": "6_clustering.html#results",
    "href": "6_clustering.html#results",
    "title": "Data Clustering",
    "section": "Results",
    "text": "Results\n\nDiscuss, illustrate, and compare the results of your various clustering analysis methods\nWhich method seemed to work the best and why, which was easier to use or preferable\nDid the clustering results provide any new insights into your data?"
  },
  {
    "objectID": "6_clustering.html#conclusions",
    "href": "6_clustering.html#conclusions",
    "title": "Data Clustering",
    "section": "Conclusions",
    "text": "Conclusions\n\nSummarize & wrap-up the report\nKey and important findings and how these findings affect real-life and real people\n\n\n\n\nReferences:"
  },
  {
    "objectID": "7_dimensionality_reduction.html#introduction",
    "href": "7_dimensionality_reduction.html#introduction",
    "title": "Dimensionality Reduction",
    "section": "Introduction",
    "text": "Introduction\n\nProposal outlining your project’s objectives, dataset selection, and the tools or libraries you plan to use (e.g., Python, scikit-learn)"
  },
  {
    "objectID": "7_dimensionality_reduction.html#theory",
    "href": "7_dimensionality_reduction.html#theory",
    "title": "Dimensionality Reduction",
    "section": "Theory",
    "text": "Theory\n\nWrite a brief technical introduction to Dimensionality Reduction with PCA and Dimensionality Reduction with t-SNE:"
  },
  {
    "objectID": "7_dimensionality_reduction.html#methods",
    "href": "7_dimensionality_reduction.html#methods",
    "title": "Dimensionality Reduction",
    "section": "Methods",
    "text": "Methods\n\nDescribe your coding workflow"
  },
  {
    "objectID": "7_dimensionality_reduction.html#results",
    "href": "7_dimensionality_reduction.html#results",
    "title": "Dimensionality Reduction",
    "section": "Results",
    "text": "Results\n\nDiscuss, illustrate, and compare the results of your various dimensionality reduction methods"
  },
  {
    "objectID": "7_dimensionality_reduction.html#conclusions",
    "href": "7_dimensionality_reduction.html#conclusions",
    "title": "Dimensionality Reduction",
    "section": "Conclusions",
    "text": "Conclusions\n\nSummarize & wrap-up the report\nKey and important findings and how these findings affect real-life and real people\n\n\n\n\nReferences:"
  },
  {
    "objectID": "Jupyter_Notebooks/4_EDA_df_lithium.html",
    "href": "Jupyter_Notebooks/4_EDA_df_lithium.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Read csv file\ndf_production &lt;- read.csv(\"../../data/01-modified-data/clean_lithium-production.csv\")\n\n# Read csv file\ndf_demand &lt;- read.csv(\"../../data/01-modified-data/clean_lithium-demand.csv\")\n\n\ndf_production &lt;- df_production %&gt;% filter(df_production$Code == \"USA\")\n\ndf_production\n\n\nA data.frame: 28 × 4\n\n\nEntity\nCode\nYear\nLithium.production...kt\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nUnited States\nUSA\n1995\n3500.000\n\n\nUnited States\nUSA\n1996\n4000.000\n\n\nUnited States\nUSA\n1997\n4000.000\n\n\nUnited States\nUSA\n1998\n1500.000\n\n\nUnited States\nUSA\n1999\n1500.000\n\n\nUnited States\nUSA\n2000\n1500.000\n\n\nUnited States\nUSA\n2001\n1500.000\n\n\nUnited States\nUSA\n2002\n1500.000\n\n\nUnited States\nUSA\n2003\n1500.000\n\n\nUnited States\nUSA\n2004\n1500.000\n\n\nUnited States\nUSA\n2005\n1500.000\n\n\nUnited States\nUSA\n2006\n1500.000\n\n\nUnited States\nUSA\n2007\n1500.000\n\n\nUnited States\nUSA\n2008\n1500.000\n\n\nUnited States\nUSA\n2009\n1500.000\n\n\nUnited States\nUSA\n2010\n1000.000\n\n\nUnited States\nUSA\n2011\n1000.000\n\n\nUnited States\nUSA\n2012\n1000.000\n\n\nUnited States\nUSA\n2013\n870.000\n\n\nUnited States\nUSA\n2014\n900.000\n\n\nUnited States\nUSA\n2015\n900.000\n\n\nUnited States\nUSA\n2016\n900.000\n\n\nUnited States\nUSA\n2017\n900.000\n\n\nUnited States\nUSA\n2018\n900.000\n\n\nUnited States\nUSA\n2019\n900.000\n\n\nUnited States\nUSA\n2020\n900.000\n\n\nUnited States\nUSA\n2021\n939.234\n\n\nUnited States\nUSA\n2022\n939.234"
  },
  {
    "objectID": "Jupyter_Notebooks/4_EDA_df_lithium.html#df_lithium",
    "href": "Jupyter_Notebooks/4_EDA_df_lithium.html#df_lithium",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Read csv file\ndf_production &lt;- read.csv(\"../../data/01-modified-data/clean_lithium-production.csv\")\n\n# Read csv file\ndf_demand &lt;- read.csv(\"../../data/01-modified-data/clean_lithium-demand.csv\")\n\n\ndf_production &lt;- df_production %&gt;% filter(df_production$Code == \"USA\")\n\ndf_production\n\n\nA data.frame: 28 × 4\n\n\nEntity\nCode\nYear\nLithium.production...kt\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nUnited States\nUSA\n1995\n3500.000\n\n\nUnited States\nUSA\n1996\n4000.000\n\n\nUnited States\nUSA\n1997\n4000.000\n\n\nUnited States\nUSA\n1998\n1500.000\n\n\nUnited States\nUSA\n1999\n1500.000\n\n\nUnited States\nUSA\n2000\n1500.000\n\n\nUnited States\nUSA\n2001\n1500.000\n\n\nUnited States\nUSA\n2002\n1500.000\n\n\nUnited States\nUSA\n2003\n1500.000\n\n\nUnited States\nUSA\n2004\n1500.000\n\n\nUnited States\nUSA\n2005\n1500.000\n\n\nUnited States\nUSA\n2006\n1500.000\n\n\nUnited States\nUSA\n2007\n1500.000\n\n\nUnited States\nUSA\n2008\n1500.000\n\n\nUnited States\nUSA\n2009\n1500.000\n\n\nUnited States\nUSA\n2010\n1000.000\n\n\nUnited States\nUSA\n2011\n1000.000\n\n\nUnited States\nUSA\n2012\n1000.000\n\n\nUnited States\nUSA\n2013\n870.000\n\n\nUnited States\nUSA\n2014\n900.000\n\n\nUnited States\nUSA\n2015\n900.000\n\n\nUnited States\nUSA\n2016\n900.000\n\n\nUnited States\nUSA\n2017\n900.000\n\n\nUnited States\nUSA\n2018\n900.000\n\n\nUnited States\nUSA\n2019\n900.000\n\n\nUnited States\nUSA\n2020\n900.000\n\n\nUnited States\nUSA\n2021\n939.234\n\n\nUnited States\nUSA\n2022\n939.234"
  },
  {
    "objectID": "Jupyter_Notebooks/5_1_NaiveBayes_Text.html",
    "href": "Jupyter_Notebooks/5_1_NaiveBayes_Text.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nimport nltk;\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download([\n    \"names\",\n    \"stopwords\",\n    \"state_union\",\n    \"twitter_samples\",\n    \"movie_reviews\",\n    \"averaged_perceptron_tagger\",\n    \"vader_lexicon\",\n    \"punkt\",])\n\nimport string \n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package names to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package names is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package state_union to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package state_union is already up-to-date!\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n[nltk_data] Downloading package movie_reviews to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package movie_reviews is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\ndf = pd.read_csv('../../../data/01-modified-data/clean_sentiment_analysis.csv')\n\nprint(df.shape)\n\ndf = df.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf\n\n(85, 6)\n\n\n\n\n\n\n\n\n\nname\ncategory\nsentiment\nlabel\ndate\ntext\n\n\n\n\n0\nThe Verge\ntechnology\n0.6783\npositive\n2023-11-07\nRedwood Materials will recycle stationary stor...\n\n\n1\nArs Technica\ntechnology\n0.2886\npositive\n2023-10-31\nEnlarge/ GreenPower has given its class-D elec...\n\n\n2\nArs Technica\ntechnology\n0.0000\nneutral\n2023-11-13\nEnlarge/ These are piles of lithium harvested ...\n\n\n3\nBusiness Insider\nbusiness\n-0.5678\nnegative\n2023-11-03\nBP's EV charging arm has bought $100 million w...\n\n\n4\nTime\ngeneral\n-0.7521\nnegative\n2023-11-07\nA lithium-ion battery recycler and a program t...\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n80\nThe Verge\ntechnology\n-0.9134\nnegative\n2023-11-16\nThe effort to climate-proof our housing is run...\n\n\n81\nReuters\ngeneral\n-0.5973\nnegative\n2023-11-06\nROME, Nov 6 (Reuters) - Italy's Industrie De N...\n\n\n82\nReuters\ngeneral\n0.0000\nneutral\n2023-11-13\nWASHINGTON, Nov 13 (Reuters) - Democratic Sena...\n\n\n83\nEngadget\ntechnology\n-0.6099\nnegative\n2023-11-01\nYour EV may go a long way between charges, but...\n\n\n84\nPolitico\ngeneral\n0.5868\npositive\n2023-11-06\nThe administration is well aware of the high s...\n\n\n\n\n85 rows × 6 columns"
  },
  {
    "objectID": "Jupyter_Notebooks/5_1_NaiveBayes_Text.html#news-data",
    "href": "Jupyter_Notebooks/5_1_NaiveBayes_Text.html#news-data",
    "title": "Naive Bayes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nimport nltk;\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download([\n    \"names\",\n    \"stopwords\",\n    \"state_union\",\n    \"twitter_samples\",\n    \"movie_reviews\",\n    \"averaged_perceptron_tagger\",\n    \"vader_lexicon\",\n    \"punkt\",])\n\nimport string \n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package names to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package names is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package state_union to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package state_union is already up-to-date!\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n[nltk_data] Downloading package movie_reviews to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package movie_reviews is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\ndf = pd.read_csv('../../../data/01-modified-data/clean_sentiment_analysis.csv')\n\nprint(df.shape)\n\ndf = df.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf\n\n(85, 6)\n\n\n\n\n\n\n\n\n\nname\ncategory\nsentiment\nlabel\ndate\ntext\n\n\n\n\n0\nThe Verge\ntechnology\n0.6783\npositive\n2023-11-07\nRedwood Materials will recycle stationary stor...\n\n\n1\nArs Technica\ntechnology\n0.2886\npositive\n2023-10-31\nEnlarge/ GreenPower has given its class-D elec...\n\n\n2\nArs Technica\ntechnology\n0.0000\nneutral\n2023-11-13\nEnlarge/ These are piles of lithium harvested ...\n\n\n3\nBusiness Insider\nbusiness\n-0.5678\nnegative\n2023-11-03\nBP's EV charging arm has bought $100 million w...\n\n\n4\nTime\ngeneral\n-0.7521\nnegative\n2023-11-07\nA lithium-ion battery recycler and a program t...\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n80\nThe Verge\ntechnology\n-0.9134\nnegative\n2023-11-16\nThe effort to climate-proof our housing is run...\n\n\n81\nReuters\ngeneral\n-0.5973\nnegative\n2023-11-06\nROME, Nov 6 (Reuters) - Italy's Industrie De N...\n\n\n82\nReuters\ngeneral\n0.0000\nneutral\n2023-11-13\nWASHINGTON, Nov 13 (Reuters) - Democratic Sena...\n\n\n83\nEngadget\ntechnology\n-0.6099\nnegative\n2023-11-01\nYour EV may go a long way between charges, but...\n\n\n84\nPolitico\ngeneral\n0.5868\npositive\n2023-11-06\nThe administration is well aware of the high s...\n\n\n\n\n85 rows × 6 columns"
  },
  {
    "objectID": "Jupyter_Notebooks/3_data_cleaning Python.html",
    "href": "Jupyter_Notebooks/3_data_cleaning Python.html",
    "title": "Data Gathering Python",
    "section": "",
    "text": "from datetime import date\nfrom datetime import date, timedelta"
  },
  {
    "objectID": "Jupyter_Notebooks/3_data_cleaning Python.html#lithium-news",
    "href": "Jupyter_Notebooks/3_data_cleaning Python.html#lithium-news",
    "title": "Data Gathering Python",
    "section": "Lithium News",
    "text": "Lithium News\n\nfrom newsapi.newsapi_client import NewsApiClient\nimport pandas as pd\n\ndate = date.today()\n\nprint(date)\n\ndate_past = date.today() - timedelta(days=15)\n\nprint(date_past)\n\nf = open('auth.k','r', encoding=\"utf-8\")\nak = f.readlines()\nf.close()\n\n#newsapi = NewsApiClient(api_key=ak[0])\n\nnewsapi = NewsApiClient(api_key='6c4d77d5f3ab45749cf47b85c1032fe6')\n\nsources = newsapi.get_sources()\n\nsources = pd.DataFrame(sources['sources'])\n\nsources = sources[(sources['language'] == 'en') & (sources['country'] == 'us') & ~sources['category'].isin(['sports', 'entertainment', 'health'])]\n\n\ndf_sources = ', '.join(sources['id'].astype(str))\n\ndf_domains = ', '.join(sources['url'].astype(str))\n\nall_articles = newsapi.get_everything(q='lithium',\n                                      sources=str(df_sources),\n                                      domains=str(df_domains),\n                                      from_param=date_past,\n                                      to=date,\n                                      language='en',\n                                      sort_by='relevancy')\n\ndf_articles = pd.DataFrame(all_articles['articles'])\n\ndf_articles\n\n2023-11-02\n2023-10-18\n\n\n\n\n\n\n\n\n\nsource\nauthor\ntitle\ndescription\nurl\nurlToImage\npublishedAt\ncontent\n\n\n\n\n0\n{'id': 'wired', 'name': 'Wired'}\nGrace Browne, Matt Reynolds\nHere’s the Truth Behind the Biggest (and Dumbe...\nYes, charging your phone overnight is bad for ...\nhttps://www.wired.com/story/how-to-improve-bat...\nhttps://media.wired.com/photos/653b8f898ed7be7...\n2023-10-27T11:00:00Z\nIn lithium-ion batteries, thats no longer the ...\n\n\n1\n{'id': 'the-verge', 'name': 'The Verge'}\nAndrew J. Hawkins\nFord hits the brakes on $12 billion in EV spen...\nFord is pausing about $12 billion in spending ...\nhttps://www.theverge.com/2023/10/26/23934172/f...\nhttps://cdn.vox-cdn.com/thumbor/LpKbkjEO0XFAxX...\n2023-10-27T00:16:44Z\nFord hits the brakes on $12 billion in EV spen...\n\n\n2\n{'id': 'ars-technica', 'name': 'Ars Technica'}\nJonathan M. Gitlin\nA giant battery gives this new school bus a 30...\nThe Type-D school bus uses a 387 kWh lithium i...\nhttps://arstechnica.com/cars/2023/10/this-elec...\nhttps://cdn.arstechnica.net/wp-content/uploads...\n2023-10-31T11:00:53Z\nEnlarge/ GreenPower has given its class-D elec...\n\n\n3\n{'id': 'business-insider', 'name': 'Business I...\nNathan Rennolds\nChina's led the EV race – but it may be runnin...\nThe world has woken up to China's control over...\nhttps://www.businessinsider.com/electric-cars-...\nhttps://i.insider.com/652eba9496f7540cd05e9155...\n2023-10-22T09:43:12Z\nWuling Hongguang Mini EVs on display at the Sh...\n\n\n4\n{'id': 'the-next-web', 'name': 'The Next Web'}\nIoanna Lykiardopoulou\nStartup bags €8.5M to bolster Europe’s EV batt...\nIn a big boost to sustainable mobility, 130 mi...\nhttps://thenextweb.com/news/startup-to-bolster...\nhttps://img-cdn.tnwcdn.com/image/tnw-blurple?f...\n2023-10-24T04:03:53Z\nIn a big boost to sustainable mobility, 130 mi...\n\n\n5\n{'id': 'the-next-web', 'name': 'The Next Web'}\nSiôn Geschwindt\nNorway’s AutoStore unveils next-gen electric w...\nNorwegian tech company AutoStore today unveile...\nhttps://thenextweb.com/news/norway-autostore-u...\nhttps://img-cdn.tnwcdn.com/image/tnw-blurple?f...\n2023-10-23T04:00:52Z\nNorwegian tech company AutoStore today unveile...\n\n\n6\n{'id': 'business-insider', 'name': 'Business I...\nSonam Sheth,Jessica Orwig\nHumans have launched so much to space that it'...\nResearchers traced the metals back to rockets ...\nhttps://www.businessinsider.com/rocket-satelli...\nhttps://i.insider.com/6530534796f7540cd05fff1c...\n2023-10-18T23:13:29Z\nWhen something enters Earth's atmosphere, it's...\n\n\n7\n{'id': 'engadget', 'name': 'Engadget'}\nAndrew Tarantola\nNASA's John Mather keeps redefining our unders...\nSpace isn't hard only on account of the rocket...\nhttps://www.engadget.com/inside-the-star-facto...\nhttps://s.yimg.com/ny/api/res/1.2/pcPe4lUVbQVQ...\n2023-10-22T14:30:46Z\nSpace isn't hard only on account of the rocket...\n\n\n8\n{'id': 'business-insider', 'name': 'Business I...\nJenny McGrath\nWhy we don't have fusion power plants yet, and...\nTrying to create a fusion reaction on Earth is...\nhttps://www.businessinsider.com/why-no-fusion-...\nhttps://i.insider.com/6531858096f7540cd060d9a2...\n2023-10-29T12:19:01Z\nThe National Spherical Torus Experiment-Upgrad...\n\n\n9\n{'id': 'time', 'name': 'Time'}\nEliana Dockterman\nBritney Spears Feared Her Family Would Kill He...\nThe pop star writes in her new memoir, The Wom...\nhttps://time.com/6326001/britney-spears-book-f...\nhttps://api.time.com/wp-content/uploads/2023/1...\n2023-10-19T20:32:08Z\nIn her new book, The Woman in Me, Britney Spea...\n\n\n10\n{'id': 'time', 'name': 'Time'}\nMoises Mendez II\n‘She Should Have Taken My Side.’ Britney Spear...\nIn 'The Woman in Me,' Britney Spears writes ca...\nhttps://time.com/6326006/britney-spears-jamie-...\nhttps://api.time.com/wp-content/uploads/2023/1...\n2023-10-19T21:13:59Z\nThe rocky relationship between Britney Spears ...\n\n\n11\n{'id': 'cbs-news', 'name': 'CBS News'}\nKate Gibson\nFeds warn against using Toos electric scooters...\nThe U.S. Consumer Product Safety Commission sa...\nhttps://www.cbsnews.com/news/product-recall-el...\nhttps://assets2.cbsnewsstatic.com/hub/i/r/2023...\n2023-10-23T19:46:00Z\nRiders of Toos Elite 60-volt electric scooters...\n\n\n12\n{'id': 'next-big-future', 'name': 'Next Big Fu...\nBrian Wang\nBetter EV Batteries and a Glut of Batteries Sh...\nCATL’s new Shenxing ultra-fast charging iron L...\nhttps://www.nextbigfuture.com/2023/10/better-e...\nhttps://nextbigfuture.s3.amazonaws.com/uploads...\n2023-10-25T15:47:05Z\nBrian Wang is a Futurist Thought Leader and a ...\n\n\n13\n{'id': 'new-scientist', 'name': 'New Scientist'}\nJeremy Hsu\nCheap salty solution cools computers and boost...\nWater containing a cheap lithium bromide salt ...\nhttps://www.newscientist.com/article/2400457-c...\nhttps://images.newscientist.com/wp-content/upl...\n2023-10-31T15:00:23Z\nPassive cooling could be more efficient using ...\n\n\n14\n{'id': 'fortune', 'name': 'Fortune'}\nMatthew Perrone, The Associated Press\nE-cigarettes pose a new environmental dilemma—...\nCommunities are experimenting with new ways to...\nhttps://fortune.com/2023/10/19/e-cigarettes-po...\nhttps://content.fortune.com/wp-content/uploads...\n2023-10-19T14:46:55Z\nWith the growing popularity of disposable e-ci...\n\n\n15\n{'id': 'fortune', 'name': 'Fortune'}\nHannah Schoenbaum, The Associated Press\nToyota literally doubles down on EVs in North ...\nToyota's first U.S. automotive battery plant, ...\nhttps://fortune.com/2023/10/31/toyota-electric...\nhttps://content.fortune.com/wp-content/uploads...\n2023-10-31T20:37:05Z\nToyota will invest an additional $8 billion in...\n\n\n16\n{'id': 'new-scientist', 'name': 'New Scientist'}\nMatthew Sparkes\nWhat are solid-state batteries and why do we n...\nBatteries containing solid electrolytes have m...\nhttps://www.newscientist.com/article/2398896-w...\nhttps://images.newscientist.com/wp-content/upl...\n2023-10-24T11:54:42Z\nSolid-state batteries could be lighter and mor...\n\n\n17\n{'id': 'the-verge', 'name': 'The Verge'}\nAndrew J. Hawkins\nRedwood Materials is recycling its first stati...\nRedwood Materials will decommission and recycl...\nhttps://www.theverge.com/2023/11/2/23943267/re...\nhttps://cdn.vox-cdn.com/thumbor/UUXd9_9UhChk9E...\n2023-11-02T13:00:00Z\nRedwood Materials is recycling its first stati...\n\n\n18\n{'id': 'newsweek', 'name': 'Newsweek'}\nThomas Kika\n'Urgent' Warning Issued for Electric Scooter A...\nA battery malfunction in one of the scooters w...\nhttps://www.newsweek.com/urgent-warning-issued...\nhttps://d.newsweek.com/en/full/2298354/electri...\n2023-10-23T20:49:19Z\nU.S. consumers are being strongly warned again...\n\n\n19\n{'id': 'abc-news', 'name': 'ABC News'}\nHANNAH SCHOENBAUM /REPORT FOR AMERICA Associat...\nToyota more than doubles investment and job cr...\nToyota will invest an additional $8 billion in...\nhttps://abcnews.go.com/Technology/wireStory/to...\nhttps://i.abcnewsfe.com/a/5ace85e3-724d-46db-8...\n2023-10-31T13:42:59Z\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n20\n{'id': 'fortune', 'name': 'Fortune'}\nJames K. Glassman\nHow the West can help Ukraine win its economic...\n'Right now, Ukraine’s allies can help most by ...\nhttps://fortune.com/2023/10/31/west-ukraine-ec...\nhttps://content.fortune.com/wp-content/uploads...\n2023-10-31T11:15:22Z\nSince Russian President Vladimir Putin invaded...\n\n\n21\n{'id': 'breitbart-news', 'name': 'Breitbart Ne...\nJohn Hayward, John Hayward\nTaliban to Formally Join China’s Belt and Road...\nThe Taliban plans to formally join China’s Bel...\nhttps://www.breitbart.com/asia/2023/10/19/tali...\nhttps://media.breitbart.com/media/2023/10/Tali...\n2023-10-19T15:47:29Z\nHaji Nooruddin Azizi, the acting commerce mini...\n\n\n22\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nJapanese automaker Toyota's profits zoom on ch...\nToyota’s profit in the latest quarter jumped n...\nhttps://abcnews.go.com/International/wireStory...\nhttps://i.abcnewsfe.com/a/0b9f4a7b-034e-4653-9...\n2023-11-01T06:22:34Z\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n23\n{'id': 'abc-news', 'name': 'ABC News'}\nMATTHEW PERRONE AP health writer\nCommunities can't recycle or trash disposable ...\nCommunities across the U.S. are confronting a ...\nhttps://abcnews.go.com/Health/wireStory/commun...\nhttps://i.abcnewsfe.com/a/1e51aec8-b5f5-4031-b...\n2023-10-19T12:22:30Z\nWASHINGTON -- With the growing popularity of d...\n\n\n24\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nG-7 nations back strong supply chains for ener...\nTrade and economy officials from the Group of ...\nhttps://abcnews.go.com/International/wireStory...\nhttps://i.abcnewsfe.com/a/1ad16d9a-8471-4838-9...\n2023-10-29T08:06:27Z\nTOKYO -- Trade and economy officials from the ...\n\n\n25\n{'id': 'new-scientist', 'name': 'New Scientist'}\nJames Dinneen\nDesert plant collects water from air by excret...\nAn evergreen desert shrub common in the Middle...\nhttps://www.newscientist.com/article/2400273-d...\nhttps://images.newscientist.com/wp-content/upl...\n2023-10-30T19:00:25Z\nThe Athel tamarisk has ingenious ways of survi...\n\n\n26\n{'id': 'newsweek', 'name': 'Newsweek'}\nMeghan Gunn\nFive Tech Innovations That Could Help Save the...\nFrom a paint that cools buildings to a new car...\nhttps://www.newsweek.com/five-tech-innovations...\nhttps://d.newsweek.com/en/full/2294547/dr-xiul...\n2023-10-30T16:13:19Z\nAround the world, entrepreneurs are trying to ...\n\n\n27\n{'id': 'abc-news', 'name': 'ABC News'}\nHANNAH SCHOENBAUM /REPORT FOR AMERICA Associat...\nToyota more than doubles investment and job cr...\nToyota will invest an additional $8 billion in...\nhttps://abcnews.go.com/Business/wireStory/toyo...\nhttps://i.abcnewsfe.com/a/5ace85e3-724d-46db-8...\n2023-10-31T13:42:54Z\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n28\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nJapan's automakers unveil EVs galore at Tokyo ...\nToyota, Honda, Nissan and other Japanese autom...\nhttps://abcnews.go.com/Technology/wireStory/ja...\nhttps://i.abcnewsfe.com/a/946cd347-6904-4e14-9...\n2023-10-25T03:51:22Z\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n29\n{'id': 'abc-news', 'name': 'ABC News'}\nMATTHEW PERRONE AP health writer\nCommunities can't recycle or trash disposable ...\nCommunities across the U.S. are confronting a ...\nhttps://abcnews.go.com/Business/wireStory/comm...\nhttps://i.abcnewsfe.com/a/1e51aec8-b5f5-4031-b...\n2023-10-19T21:26:36Z\nWASHINGTON -- With the growing popularity of d...\n\n\n30\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nJapan's automakers unveil EVs galore at Tokyo ...\nToyota, Honda, Nissan and other Japanese autom...\nhttps://abcnews.go.com/International/wireStory...\nhttps://i.abcnewsfe.com/a/946cd347-6904-4e14-9...\n2023-10-25T03:50:32Z\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n31\n{'id': 'abc-news', 'name': 'ABC News'}\nFATIMA HUSSEIN Associated Press\nYellen calls for more US-Latin America trade, ...\nTreasury Secretary Janet Yellen wants Latin Am...\nhttps://abcnews.go.com/US/wireStory/treasury-s...\nhttps://i.abcnewsfe.com/a/7d503fb7-17af-4773-b...\n2023-11-02T12:22:28Z\nWASHINGTON -- Treasury Secretary Janet Yellen ...\n\n\n32\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nJapanese automaker Toyota's profits zoom on ch...\nToyota’s profit in the latest quarter jumped n...\nhttps://abcnews.go.com/Business/wireStory/japa...\nhttps://i.abcnewsfe.com/a/0b9f4a7b-034e-4653-9...\n2023-11-01T06:22:38Z\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n33\n{'id': 'abc-news', 'name': 'ABC News'}\nFATIMA HUSSEIN Associated Press\nTreasury Secretary Yellen calls for more US-La...\nTreasury Secretary Janet Yellen wants Latin Am...\nhttps://abcnews.go.com/Business/wireStory/trea...\nhttps://i.abcnewsfe.com/a/7d503fb7-17af-4773-b...\n2023-11-02T12:22:34Z\nWASHINGTON -- Treasury Secretary Janet Yellen ...\n\n\n34\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nG-7 nations back strong supply chains for ener...\nTrade and economy officials from the Group of ...\nhttps://abcnews.go.com/Business/wireStory/7-na...\nhttps://i.abcnewsfe.com/a/1ad16d9a-8471-4838-9...\n2023-10-29T08:22:22Z\nTOKYO -- Trade and economy officials from the ...\n\n\n35\n{'id': 'time', 'name': 'Time'}\nEliana Dockterman\nBritney Spears Is Suspended Between Girlhood a...\nIn her highly anticipated memoir, the pop star...\nhttps://time.com/6326344/britney-spears-the-wo...\nhttps://api.time.com/wp-content/uploads/2023/1...\n2023-10-20T21:39:30Z\nBritney Spears knows what it means to be depri...\n\n\n36\n{'id': 'abc-news', 'name': 'ABC News'}\nYURI KAGEYAMA AP business writer\nJapan's automakers unveil EVs galore at Tokyo ...\nToyota, Honda, Nissan and other Japanese autom...\nhttps://abcnews.go.com/Business/wireStory/japa...\nhttps://i.abcnewsfe.com/a/946cd347-6904-4e14-9...\n2023-10-25T03:50:36Z\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n37\n{'id': 'ars-technica', 'name': 'Ars Technica'}\nJonathan M. Gitlin\nHonda says making cheap electric vehicles is t...\nThe platform was to use GM's Ultium batteries.\nhttps://arstechnica.com/cars/2023/10/honda-can...\nhttps://cdn.arstechnica.net/wp-content/uploads...\n2023-10-25T15:25:55Z\nEnlarge/ A GM Ultium battery pack. \\r\\n75 with...\n\n\n38\n{'id': 'breitbart-news', 'name': 'Breitbart Ne...\nJohn Binder, John Binder\nGM Backtracks on Joe Biden's Green Energy Agen...\nAfter investing billions to adhere to Presiden...\nhttps://www.breitbart.com/economy/2023/10/31/g...\nhttps://media.breitbart.com/media/2023/10/Gett...\n2023-10-31T18:55:02Z\nAfter investing billions to adhere to Presiden...\n\n\n39\n{'id': 'breitbart-news', 'name': 'Breitbart Ne...\nJohn Binder, John Binder\nUAW Strike: 6.8K Auto Workers Shut Down Stella...\nNearly 7,000 auto workers at Stellantis' Sterl...\nhttps://www.breitbart.com/politics/2023/10/23/...\nhttps://media.breitbart.com/media/2023/10/Gett...\n2023-10-23T22:15:07Z\nNearly 7,000 auto workers at Stellantis’ Sterl...\n\n\n40\n{'id': 'fox-news', 'name': 'Fox News'}\nAssociated Press\nUS struggles to discard hazardous materials fr...\nE-cigarettes are considered hazardous waste th...\nhttps://www.foxnews.com/us/us-struggles-discar...\nhttps://static.foxnews.com/foxnews.com/content...\n2023-10-19T17:55:51Z\nWith the growing popularity of disposable e-ci...\n\n\n41\n{'id': 'techradar', 'name': 'TechRadar'}\nspace2occupy@gmail.com (James Holland)\nYeedi Cube robot vacuum and mop review: lots o...\nWith its 2-in-1 vacuuming and mopping cleaning...\nhttps://www.techradar.com/home/robot-vacuums/y...\nhttps://cdn.mos.cms.futurecdn.net/gnPf25Xz3Ui3...\n2023-10-30T13:00:29Z\nYeedi Cube: One-minute review\\r\\nThe Yeedi Cub...\n\n\n42\n{'id': 'nbc-news', 'name': 'NBC News'}\nChloe Melas\nBritney Spears reflects on her 'traumatizing' ...\nIn her upcoming memoir, Britney Spears is shed...\nhttps://www.nbcnews.com/pop-culture/pop-cultur...\nhttps://media-cldnry.s-nbcnews.com/image/uploa...\n2023-10-20T04:06:21Z\nBritney Spears is shedding new light onto what...\n\n\n43\n{'id': 'fox-news', 'name': 'Fox News'}\nPeter Aitken\nBiden hosts anti-Israel world leader to talk c...\nChile, Bolivia and Colombia all recalled their...\nhttps://www.foxnews.com/world/biden-hosts-anti...\nhttps://static.foxnews.com/foxnews.com/content...\n2023-11-02T08:00:19Z\nPresident Biden will meet with Chilean Preside...\n\n\n44\n{'id': 'breitbart-news', 'name': 'Breitbart Ne...\nJohn Binder, John Binder\nJoe Biden's Green Energy Flop: Automakers Real...\nAmerican automakers are quickly learning that ...\nhttps://www.breitbart.com/economy/2023/10/27/j...\nhttps://media.breitbart.com/media/2023/10/Gett...\n2023-10-27T22:28:20Z\nAfter investing billions into a green energy a...\n\n\n45\n{'id': 'fox-news', 'name': 'Fox News'}\nKristen Walker\nThe latest attempt to take away your gas-power...\nAnother government agency has proposed a rule ...\nhttps://www.foxnews.com/opinion/latest-attempt...\nhttps://static.foxnews.com/foxnews.com/content...\n2023-10-18T18:00:10Z\nIn yet another attempt to regulate the car mar...\n\n\n46\n{'id': 'engadget', 'name': 'Engadget'}\nSteve Dent\nThis 90-passenger school bus has nearly 300 mi...\nGreenPower has launched the Mega Beast electri...\nhttps://www.engadget.com/this-90-passenger-sch...\nhttps://s.yimg.com/ny/api/res/1.2/MZj6YDQcHvJg...\n2023-11-01T04:57:10Z\nYour EV may go a long way between charges, but...\n\n\n47\n{'id': 'reuters', 'name': 'Reuters'}\nJoseph White\nGM withdraws 2023 guidance as UAW strike costs...\nGeneral Motors &lt;a href=\"https://www.reuters.co...\nhttps://www.reuters.com/business/autos-transpo...\nhttps://www.reuters.com/resizer/2eWtjHsHlJ4x8o...\n2023-10-24T10:40:38Z\nDETROIT, Oct 24 (Reuters) - General Motors (GM...\n\n\n48\n{'id': 'usa-today', 'name': 'USA Today'}\nNada Hassanein\nAbortion ban medical exceptions often exclude ...\nPregnant women were more likely to die from me...\nhttps://www.usatoday.com/story/news/nation/202...\nhttps://www.usatoday.com/gcdn/authoring/author...\n2023-10-26T11:35:39Z\nMore than a dozen states now have near-total a...\n\n\n\n\n\n\n\n\nsources\n\n\n\n\n\n\n\n\nid\nname\ndescription\nurl\ncategory\nlanguage\ncountry\n\n\n\n\n0\nabc-news\nABC News\nYour trusted source for breaking news, analysi...\nhttps://abcnews.go.com\ngeneral\nen\nus\n\n\n3\nal-jazeera-english\nAl Jazeera English\nNews, analysis from the Middle East and worldw...\nhttp://www.aljazeera.com\ngeneral\nen\nus\n\n\n6\nars-technica\nArs Technica\nThe PC enthusiast's resource. Power users and ...\nhttp://arstechnica.com\ntechnology\nen\nus\n\n\n8\nassociated-press\nAssociated Press\nThe AP delivers in-depth coverage on the inter...\nhttps://apnews.com/\ngeneral\nen\nus\n\n\n10\naxios\nAxios\nAxios are a new media company delivering vital...\nhttps://www.axios.com\ngeneral\nen\nus\n\n\n16\nbloomberg\nBloomberg\nBloomberg delivers business and markets news, ...\nhttp://www.bloomberg.com\nbusiness\nen\nus\n\n\n17\nbreitbart-news\nBreitbart News\nSyndicated news and opinion website providing ...\nhttp://www.breitbart.com\ngeneral\nen\nus\n\n\n18\nbusiness-insider\nBusiness Insider\nBusiness Insider is a fast-growing business si...\nhttp://www.businessinsider.com\nbusiness\nen\nus\n\n\n22\ncbs-news\nCBS News\nCBS News: dedicated to providing the best in j...\nhttp://www.cbsnews.com\ngeneral\nen\nus\n\n\n23\ncnn\nCNN\nView the latest news and breaking news today f...\nhttp://us.cnn.com\ngeneral\nen\nus\n\n\n25\ncrypto-coins-news\nCrypto Coins News\nProviding breaking cryptocurrency news - focus...\nhttps://www.ccn.com\ntechnology\nen\nus\n\n\n29\nengadget\nEngadget\nEngadget is a web magazine with obsessive dail...\nhttps://www.engadget.com\ntechnology\nen\nus\n\n\n36\nfortune\nFortune\nFortune 500 Daily and Breaking Business News\nhttp://fortune.com\nbusiness\nen\nus\n\n\n38\nfox-news\nFox News\nBreaking News, Latest News and Current News fr...\nhttp://www.foxnews.com\ngeneral\nen\nus\n\n\n41\ngoogle-news\nGoogle News\nComprehensive, up-to-date news coverage, aggre...\nhttps://news.google.com\ngeneral\nen\nus\n\n\n55\nhacker-news\nHacker News\nHacker News is a social news website focusing ...\nhttps://news.ycombinator.com\ntechnology\nen\nus\n\n\n73\nmsnbc\nMSNBC\nBreaking news and in-depth analysis of the hea...\nhttp://www.msnbc.com\ngeneral\nen\nus\n\n\n76\nnational-geographic\nNational Geographic\nReporting our world daily: original nature and...\nhttp://news.nationalgeographic.com\nscience\nen\nus\n\n\n77\nnational-review\nNational Review\nNational Review: Conservative News, Opinion, P...\nhttps://www.nationalreview.com/\ngeneral\nen\nus\n\n\n78\nnbc-news\nNBC News\nBreaking news, videos, and the latest top stor...\nhttp://www.nbcnews.com\ngeneral\nen\nus\n\n\n80\nnew-scientist\nNew Scientist\nBreaking science and technology news from arou...\nhttps://www.newscientist.com/section/news\nscience\nen\nus\n\n\n82\nnewsweek\nNewsweek\nNewsweek provides in-depth analysis, news and ...\nhttps://www.newsweek.com\ngeneral\nen\nus\n\n\n83\nnew-york-magazine\nNew York Magazine\nNYMAG and New York magazine cover the new, the...\nhttp://nymag.com\ngeneral\nen\nus\n\n\n84\nnext-big-future\nNext Big Future\nCoverage of science and technology that have t...\nhttps://www.nextbigfuture.com\nscience\nen\nus\n\n\n88\npolitico\nPolitico\nPolitical news about Congress, the White House...\nhttps://www.politico.com\ngeneral\nen\nus\n\n\n91\nrecode\nRecode\nGet the latest independent tech news, reviews ...\nhttp://www.recode.net\ntechnology\nen\nus\n\n\n92\nreddit-r-all\nReddit /r/all\nReddit is an entertainment, social news networ...\nhttps://www.reddit.com/r/all\ngeneral\nen\nus\n\n\n93\nreuters\nReuters\nReuters.com brings you the latest news from ar...\nhttp://www.reuters.com\ngeneral\nen\nus\n\n\n102\ntechcrunch\nTechCrunch\nTechCrunch is a leading technology media prope...\nhttps://techcrunch.com\ntechnology\nen\nus\n\n\n104\ntechradar\nTechRadar\nThe latest technology news and reviews, coveri...\nhttp://www.techradar.com\ntechnology\nen\nus\n\n\n105\nthe-american-conservative\nThe American Conservative\nRealism and reform. A new voice for a new gene...\nhttp://www.theamericanconservative.com/\ngeneral\nen\nus\n\n\n107\nthe-hill\nThe Hill\nThe Hill is a top US political website, read b...\nhttp://thehill.com\ngeneral\nen\nus\n\n\n109\nthe-huffington-post\nThe Huffington Post\nThe Huffington Post is a politically liberal A...\nhttp://www.huffingtonpost.com\ngeneral\nen\nus\n\n\n113\nthe-next-web\nThe Next Web\nThe Next Web is one of the world’s largest onl...\nhttp://thenextweb.com\ntechnology\nen\nus\n\n\n116\nthe-verge\nThe Verge\nThe Verge covers the intersection of technolog...\nhttp://www.theverge.com\ntechnology\nen\nus\n\n\n117\nthe-wall-street-journal\nThe Wall Street Journal\nWSJ online coverage of breaking news and curre...\nhttp://www.wsj.com\nbusiness\nen\nus\n\n\n118\nthe-washington-post\nThe Washington Post\nBreaking news and analysis on politics, busine...\nhttps://www.washingtonpost.com\ngeneral\nen\nus\n\n\n119\nthe-washington-times\nThe Washington Times\nThe Washington Times delivers breaking news an...\nhttps://www.washingtontimes.com/\ngeneral\nen\nus\n\n\n120\ntime\nTime\nBreaking news and analysis from TIME.com. Poli...\nhttp://time.com\ngeneral\nen\nus\n\n\n121\nusa-today\nUSA Today\nGet the latest national, international, and po...\nhttp://www.usatoday.com/news\ngeneral\nen\nus\n\n\n122\nvice-news\nVice News\nVice News is Vice Media, Inc.'s current affair...\nhttps://news.vice.com\ngeneral\nen\nus\n\n\n123\nwired\nWired\nWired is a monthly American magazine, publishe...\nhttps://www.wired.com\ntechnology\nen\nus"
  },
  {
    "objectID": "Jupyter_Notebooks/3_data_cleaning Python.html#ibm-watson---sentiment-analysis",
    "href": "Jupyter_Notebooks/3_data_cleaning Python.html#ibm-watson---sentiment-analysis",
    "title": "Data Gathering Python",
    "section": "IBM Watson - Sentiment Analysis",
    "text": "IBM Watson - Sentiment Analysis\n\n# pip install xlsxwriter\n#pip install xlrd\n#pip install openpyxl\n\nimport json\nfrom ibm_watson import NaturalLanguageUnderstandingV1\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\nfrom ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions\n\nimport os\n\n\ndf_content = pd.DataFrame(df_articles, columns=['source', 'content', 'publishedAt'])\ndf_content['source'] = df_content['source'].apply(lambda x: x['id'])\n\n# Vieja\n#_4YE1Qj6PFjke1zYsp7Kapgfu5laaaBE1E_ZUw1IiUPa\n\n# Nueva\n#DHHnv30sML61Hn2pt6iHBdMKtj6bxV1PLvzg7j0OSQf3\n\nauthenticator = IAMAuthenticator('_4YE1Qj6PFjke1zYsp7Kapgfu5laaaBE1E_ZUw1IiUPa')\nnatural_language_understanding = NaturalLanguageUnderstandingV1(\n    version='2020-08-01',\n    authenticator=authenticator\n)\n\nnatural_language_understanding.set_service_url('https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/8b0909d1-3768-4c54-b80d-b9817610e36d')\n\n\n#IBM Watson\ni = 0\nibm_source = []\nibm_date = []\nibm_score = []\nibm_label = []\nibm_content = []\n\nfor index, row in df_content.iterrows():\n    response = natural_language_understanding.analyze(\n        text=row['content'], \n        language = 'en', \n        features=Features(sentiment=SentimentOptions())).get_result()\n\n    s = row['source']\n    ibm_source.append(s)\n    d = row['publishedAt']\n    ibm_date.append(d)\n    c = row['content']\n    ibm_content.append(c)\n    x = response['sentiment']['document']['score']\n    x = round(x, 4)\n    ibm_score.append(x)\n    y = response['sentiment']['document']['label']\n    ibm_label.append(y)\n    # print(response['sentiment']['document']['score'])\n    # print(response['sentiment']['document']['label'])\n    # print(json.dumps(response, indent=2))\n\n    i=i+1   \n\n\nresults = {\"id\": ibm_source, \"ibm_date\": ibm_date, \"ibm_score\": ibm_score, \"ibm_label\": ibm_label, \"ibm_content\": ibm_content}\n\nresults = pd.DataFrame(results)\n\n\nresults = results.merge(sources, how='left')\n\nresults = pd.DataFrame(results, columns=['name', 'category', 'ibm_score', 'ibm_label', 'ibm_date', 'ibm_content'])\n\nresults = results.rename(columns={'ibm_date': 'date'})\n\nresults['date'] = pd.to_datetime(results['date'])\n\nresults['date'] = results['date'].dt.date\n\nresults\n\n\n\n\n\n\n\n\nname\ncategory\nibm_score\nibm_label\ndate\nibm_content\n\n\n\n\n0\nWired\ntechnology\n-0.5961\nnegative\n2023-10-27\nIn lithium-ion batteries, thats no longer the ...\n\n\n1\nThe Verge\ntechnology\n-0.8783\nnegative\n2023-10-27\nFord hits the brakes on $12 billion in EV spen...\n\n\n2\nArs Technica\ntechnology\n0.2886\npositive\n2023-10-31\nEnlarge/ GreenPower has given its class-D elec...\n\n\n3\nBusiness Insider\nbusiness\n0.6452\npositive\n2023-10-22\nWuling Hongguang Mini EVs on display at the Sh...\n\n\n4\nThe Next Web\ntechnology\n0.6209\npositive\n2023-10-24\nIn a big boost to sustainable mobility, 130 mi...\n\n\n5\nThe Next Web\ntechnology\n0.9080\npositive\n2023-10-23\nNorwegian tech company AutoStore today unveile...\n\n\n6\nBusiness Insider\nbusiness\n-0.6760\nnegative\n2023-10-18\nWhen something enters Earth's atmosphere, it's...\n\n\n7\nEngadget\ntechnology\n0.0000\nneutral\n2023-10-22\nSpace isn't hard only on account of the rocket...\n\n\n8\nBusiness Insider\nbusiness\n0.6035\npositive\n2023-10-29\nThe National Spherical Torus Experiment-Upgrad...\n\n\n9\nTime\ngeneral\n0.0000\nneutral\n2023-10-19\nIn her new book, The Woman in Me, Britney Spea...\n\n\n10\nTime\ngeneral\n0.0000\nneutral\n2023-10-19\nThe rocky relationship between Britney Spears ...\n\n\n11\nCBS News\ngeneral\n-0.6290\nnegative\n2023-10-23\nRiders of Toos Elite 60-volt electric scooters...\n\n\n12\nNext Big Future\nscience\n0.0000\nneutral\n2023-10-25\nBrian Wang is a Futurist Thought Leader and a ...\n\n\n13\nNew Scientist\nscience\n0.6051\npositive\n2023-10-31\nPassive cooling could be more efficient using ...\n\n\n14\nFortune\nbusiness\n-0.8252\nnegative\n2023-10-19\nWith the growing popularity of disposable e-ci...\n\n\n15\nFortune\nbusiness\n0.0000\nneutral\n2023-10-31\nToyota will invest an additional $8 billion in...\n\n\n16\nNew Scientist\nscience\n0.8984\npositive\n2023-10-24\nSolid-state batteries could be lighter and mor...\n\n\n17\nThe Verge\ntechnology\n0.0000\nneutral\n2023-11-02\nRedwood Materials is recycling its first stati...\n\n\n18\nNewsweek\ngeneral\n-0.9520\nnegative\n2023-10-23\nU.S. consumers are being strongly warned again...\n\n\n19\nABC News\ngeneral\n0.0000\nneutral\n2023-10-31\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n20\nFortune\nbusiness\n-0.6701\nnegative\n2023-10-31\nSince Russian President Vladimir Putin invaded...\n\n\n21\nBreitbart News\ngeneral\n0.0000\nneutral\n2023-10-19\nHaji Nooruddin Azizi, the acting commerce mini...\n\n\n22\nABC News\ngeneral\n0.6487\npositive\n2023-11-01\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n23\nABC News\ngeneral\n-0.8907\nnegative\n2023-10-19\nWASHINGTON -- With the growing popularity of d...\n\n\n24\nABC News\ngeneral\n0.6301\npositive\n2023-10-29\nTOKYO -- Trade and economy officials from the ...\n\n\n25\nNew Scientist\nscience\n0.0000\nneutral\n2023-10-30\nThe Athel tamarisk has ingenious ways of survi...\n\n\n26\nNewsweek\ngeneral\n0.4480\npositive\n2023-10-30\nAround the world, entrepreneurs are trying to ...\n\n\n27\nABC News\ngeneral\n0.0000\nneutral\n2023-10-31\nRALEIGH, N.C. -- Toyota will invest an additio...\n\n\n28\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n29\nABC News\ngeneral\n-0.8907\nnegative\n2023-10-19\nWASHINGTON -- With the growing popularity of d...\n\n\n30\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n31\nABC News\ngeneral\n-0.6795\nnegative\n2023-11-02\nWASHINGTON -- Treasury Secretary Janet Yellen ...\n\n\n32\nABC News\ngeneral\n0.6487\npositive\n2023-11-01\nTOKYO -- Toyotas July-September profit jumped ...\n\n\n33\nABC News\ngeneral\n-0.6795\nnegative\n2023-11-02\nWASHINGTON -- Treasury Secretary Janet Yellen ...\n\n\n34\nABC News\ngeneral\n0.6301\npositive\n2023-10-29\nTOKYO -- Trade and economy officials from the ...\n\n\n35\nTime\ngeneral\n-0.3817\nnegative\n2023-10-20\nBritney Spears knows what it means to be depri...\n\n\n36\nABC News\ngeneral\n0.5985\npositive\n2023-10-25\nTOKYO -- TOKYO (AP) We love battery EVs.\\r\\nTa...\n\n\n37\nArs Technica\ntechnology\n-0.7175\nnegative\n2023-10-25\nEnlarge/ A GM Ultium battery pack. \\r\\n75 with...\n\n\n38\nBreitbart News\ngeneral\n0.3935\npositive\n2023-10-31\nAfter investing billions to adhere to Presiden...\n\n\n39\nBreitbart News\ngeneral\n0.0000\nneutral\n2023-10-23\nNearly 7,000 auto workers at Stellantis’ Sterl...\n\n\n40\nFox News\ngeneral\n-0.8252\nnegative\n2023-10-19\nWith the growing popularity of disposable e-ci...\n\n\n41\nTechRadar\ntechnology\n-0.5295\nnegative\n2023-10-30\nYeedi Cube: One-minute review\\r\\nThe Yeedi Cub...\n\n\n42\nNBC News\ngeneral\n-0.5513\nnegative\n2023-10-20\nBritney Spears is shedding new light onto what...\n\n\n43\nFox News\ngeneral\n-0.4456\nnegative\n2023-11-02\nPresident Biden will meet with Chilean Preside...\n\n\n44\nBreitbart News\ngeneral\n0.3316\npositive\n2023-10-27\nAfter investing billions into a green energy a...\n\n\n45\nFox News\ngeneral\n0.0000\nneutral\n2023-10-18\nIn yet another attempt to regulate the car mar...\n\n\n46\nEngadget\ntechnology\n-0.6099\nnegative\n2023-11-01\nYour EV may go a long way between charges, but...\n\n\n47\nReuters\ngeneral\n-0.3458\nnegative\n2023-10-24\nDETROIT, Oct 24 (Reuters) - General Motors (GM...\n\n\n48\nUSA Today\ngeneral\n-0.6586\nnegative\n2023-10-26\nMore than a dozen states now have near-total a..."
  },
  {
    "objectID": "Jupyter_Notebooks/4_EDA_df_resources_prices.html",
    "href": "Jupyter_Notebooks/4_EDA_df_resources_prices.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Read csv file\ndf_resources &lt;- read.csv(\"../../data/01-modified-data/clean_resources-price.csv\")\n\nnames(df_resources) &lt;- c(\"DATE\", \"Uranium\", \"Natural Gas\")\n\ndf_resources$DATE &lt;- as.Date(df_resources$DATE)\n\n\ndf_resources &lt;- pivot_longer(df_resources, \n                        cols = -DATE,  # Replace 'ID' with the name of your ID column\n                        names_to = \"Resources\",  # Name for the new variable column\n                        values_to = \"Prices\")    # Name for the new value column\n\ndf_resources\n\n\nA tibble: 264 × 3\n\n\nDATE\nResources\nPrices\n\n\n&lt;date&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n2012-01-01\nUranium\n52.31250\n\n\n2012-01-01\nNatural Gas\n3.25400\n\n\n2012-02-01\nUranium\n52.05556\n\n\n2012-02-01\nNatural Gas\n3.43600\n\n\n2012-03-01\nUranium\n51.28889\n\n\n2012-03-01\nNatural Gas\n3.71700\n\n\n2012-04-01\nUranium\n51.30000\n\n\n2012-04-01\nNatural Gas\n3.87400\n\n\n2012-05-01\nUranium\n51.88889\n\n\n2012-05-01\nNatural Gas\n3.71800\n\n\n2012-06-01\nUranium\n50.83333\n\n\n2012-06-01\nNatural Gas\n3.51800\n\n\n2012-07-01\nUranium\n50.35556\n\n\n2012-07-01\nNatural Gas\n3.29100\n\n\n2012-08-01\nUranium\n49.25000\n\n\n2012-08-01\nNatural Gas\n3.60600\n\n\n2012-09-01\nUranium\n47.72500\n\n\n2012-09-01\nNatural Gas\n3.79700\n\n\n2012-10-01\nUranium\n44.61111\n\n\n2012-10-01\nNatural Gas\n3.75000\n\n\n2012-11-01\nUranium\n41.50000\n\n\n2012-11-01\nNatural Gas\n3.40600\n\n\n2012-12-01\nUranium\n43.66667\n\n\n2012-12-01\nNatural Gas\n3.33700\n\n\n2013-01-01\nUranium\n42.75000\n\n\n2013-01-01\nNatural Gas\n3.23300\n\n\n2013-02-01\nUranium\n43.40625\n\n\n2013-02-01\nNatural Gas\n3.47100\n\n\n2013-03-01\nUranium\n42.28125\n\n\n2013-03-01\nNatural Gas\n3.69800\n\n\n...\n...\n...\n\n\n2021-10-01\nUranium\n38.47619\n\n\n2021-10-01\nNatural Gas\n3.09300\n\n\n2021-11-01\nUranium\n31.09958\n\n\n2021-11-01\nNatural Gas\n3.28000\n\n\n2021-12-01\nUranium\n36.12930\n\n\n2021-12-01\nNatural Gas\n3.20400\n\n\n2022-01-01\nUranium\n36.87292\n\n\n2022-01-01\nNatural Gas\n3.14100\n\n\n2022-02-01\nUranium\n35.83191\n\n\n2022-02-01\nNatural Gas\n3.33000\n\n\n2022-03-01\nUranium\n45.51470\n\n\n2022-03-01\nNatural Gas\n3.96300\n\n\n2022-04-01\nUranium\n48.70473\n\n\n2022-04-01\nNatural Gas\n4.02100\n\n\n2022-05-01\nUranium\n40.89163\n\n\n2022-05-01\nNatural Gas\n4.03100\n\n\n2022-06-01\nUranium\n40.33152\n\n\n2022-06-01\nNatural Gas\n4.70200\n\n\n2022-07-01\nUranium\n38.93916\n\n\n2022-07-01\nNatural Gas\n4.61900\n\n\n2022-08-01\nUranium\n39.80231\n\n\n2022-08-01\nNatural Gas\n4.03400\n\n\n2022-09-01\nUranium\n40.95039\n\n\n2022-09-01\nNatural Gas\n3.61700\n\n\n2022-10-01\nUranium\n41.30360\n\n\n2022-10-01\nNatural Gas\n3.59200\n\n\n2022-11-01\nUranium\n40.95406\n\n\n2022-11-01\nNatural Gas\n3.62800\n\n\n2022-12-01\nUranium\n39.17824\n\n\n2022-12-01\nNatural Gas\n3.26000\n\n\n\n\n\n\nuranium &lt;- ggplot(df_resources[df_resources$Resources == \"Uranium\",]) +\n                geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n                labs(x = \"Price\", y = \"Frequency\", title = \"Resource Distributions\", subtitle = \"Uranium\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nnatural_gas &lt;- ggplot(df_resources[df_resources$Resources == \"Natural Gas\",]) +\n                geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n                labs(x = \"Price\", y = \"Frequency\", subtitle = \"Natural Gas\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nplots &lt;- wrap_plots(uranium, natural_gas, nrow = 2)\n\nplots\n\n\n\n\n\nlibrary(patchwork)\n\nplot_list &lt;- lapply(unique(df_resources$Resources), function(resource) {\n  ggplot(df_resources[df_resources$Resources == resource, ]) +\n    geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n    labs(x = \"Price\", y = \"Frequency\", title = resource) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n      axis.title.x = element_text(size = 10),\n      axis.title.y = element_text(size = 10),\n      plot.margin = margin(1, 1, 1, 1, \"cm\")\n    )\n})\n\n# Arrange the plots in a grid\ngrid_plot &lt;- wrap_plots(plotlist = plot_list, nrow = 2)\n\n# Print the grid of histograms\ngrid_plot\n\n\n\n\n\nuranium &lt;- ggplot(df_resources[df_resources$Resources == \"Uranium\",]) +\n                geom_boxplot(aes(y = Prices)) +\n                labs(x = \"\", y = \"Price\", title = \"Resource Distributions\", subtitle = \"Uranium\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nggplotly(uranium)\n\n\n\n    \n        \n        \n\n\n\n\n\n\n\n    \n    \n        \n\n    \n\n\n\n\np &lt;- ggplot(df_resources) +\n        geom_line(aes(x = DATE, y = Prices), color = \"darkred\") + \n        labs(x = \"Date\", y = \"Price\", title = \"Resources Prices\") +\n        theme_minimal() +\n        theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\"))+\n        facet_grid(Resources ~ ., scales = \"free\")    \n\nggplotly(p, width = 800, height = 500)"
  },
  {
    "objectID": "Jupyter_Notebooks/4_EDA_df_resources_prices.html#df_resources_prices",
    "href": "Jupyter_Notebooks/4_EDA_df_resources_prices.html#df_resources_prices",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Read csv file\ndf_resources &lt;- read.csv(\"../../data/01-modified-data/clean_resources-price.csv\")\n\nnames(df_resources) &lt;- c(\"DATE\", \"Uranium\", \"Natural Gas\")\n\ndf_resources$DATE &lt;- as.Date(df_resources$DATE)\n\n\ndf_resources &lt;- pivot_longer(df_resources, \n                        cols = -DATE,  # Replace 'ID' with the name of your ID column\n                        names_to = \"Resources\",  # Name for the new variable column\n                        values_to = \"Prices\")    # Name for the new value column\n\ndf_resources\n\n\nA tibble: 264 × 3\n\n\nDATE\nResources\nPrices\n\n\n&lt;date&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n2012-01-01\nUranium\n52.31250\n\n\n2012-01-01\nNatural Gas\n3.25400\n\n\n2012-02-01\nUranium\n52.05556\n\n\n2012-02-01\nNatural Gas\n3.43600\n\n\n2012-03-01\nUranium\n51.28889\n\n\n2012-03-01\nNatural Gas\n3.71700\n\n\n2012-04-01\nUranium\n51.30000\n\n\n2012-04-01\nNatural Gas\n3.87400\n\n\n2012-05-01\nUranium\n51.88889\n\n\n2012-05-01\nNatural Gas\n3.71800\n\n\n2012-06-01\nUranium\n50.83333\n\n\n2012-06-01\nNatural Gas\n3.51800\n\n\n2012-07-01\nUranium\n50.35556\n\n\n2012-07-01\nNatural Gas\n3.29100\n\n\n2012-08-01\nUranium\n49.25000\n\n\n2012-08-01\nNatural Gas\n3.60600\n\n\n2012-09-01\nUranium\n47.72500\n\n\n2012-09-01\nNatural Gas\n3.79700\n\n\n2012-10-01\nUranium\n44.61111\n\n\n2012-10-01\nNatural Gas\n3.75000\n\n\n2012-11-01\nUranium\n41.50000\n\n\n2012-11-01\nNatural Gas\n3.40600\n\n\n2012-12-01\nUranium\n43.66667\n\n\n2012-12-01\nNatural Gas\n3.33700\n\n\n2013-01-01\nUranium\n42.75000\n\n\n2013-01-01\nNatural Gas\n3.23300\n\n\n2013-02-01\nUranium\n43.40625\n\n\n2013-02-01\nNatural Gas\n3.47100\n\n\n2013-03-01\nUranium\n42.28125\n\n\n2013-03-01\nNatural Gas\n3.69800\n\n\n...\n...\n...\n\n\n2021-10-01\nUranium\n38.47619\n\n\n2021-10-01\nNatural Gas\n3.09300\n\n\n2021-11-01\nUranium\n31.09958\n\n\n2021-11-01\nNatural Gas\n3.28000\n\n\n2021-12-01\nUranium\n36.12930\n\n\n2021-12-01\nNatural Gas\n3.20400\n\n\n2022-01-01\nUranium\n36.87292\n\n\n2022-01-01\nNatural Gas\n3.14100\n\n\n2022-02-01\nUranium\n35.83191\n\n\n2022-02-01\nNatural Gas\n3.33000\n\n\n2022-03-01\nUranium\n45.51470\n\n\n2022-03-01\nNatural Gas\n3.96300\n\n\n2022-04-01\nUranium\n48.70473\n\n\n2022-04-01\nNatural Gas\n4.02100\n\n\n2022-05-01\nUranium\n40.89163\n\n\n2022-05-01\nNatural Gas\n4.03100\n\n\n2022-06-01\nUranium\n40.33152\n\n\n2022-06-01\nNatural Gas\n4.70200\n\n\n2022-07-01\nUranium\n38.93916\n\n\n2022-07-01\nNatural Gas\n4.61900\n\n\n2022-08-01\nUranium\n39.80231\n\n\n2022-08-01\nNatural Gas\n4.03400\n\n\n2022-09-01\nUranium\n40.95039\n\n\n2022-09-01\nNatural Gas\n3.61700\n\n\n2022-10-01\nUranium\n41.30360\n\n\n2022-10-01\nNatural Gas\n3.59200\n\n\n2022-11-01\nUranium\n40.95406\n\n\n2022-11-01\nNatural Gas\n3.62800\n\n\n2022-12-01\nUranium\n39.17824\n\n\n2022-12-01\nNatural Gas\n3.26000\n\n\n\n\n\n\nuranium &lt;- ggplot(df_resources[df_resources$Resources == \"Uranium\",]) +\n                geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n                labs(x = \"Price\", y = \"Frequency\", title = \"Resource Distributions\", subtitle = \"Uranium\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nnatural_gas &lt;- ggplot(df_resources[df_resources$Resources == \"Natural Gas\",]) +\n                geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n                labs(x = \"Price\", y = \"Frequency\", subtitle = \"Natural Gas\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nplots &lt;- wrap_plots(uranium, natural_gas, nrow = 2)\n\nplots\n\n\n\n\n\nlibrary(patchwork)\n\nplot_list &lt;- lapply(unique(df_resources$Resources), function(resource) {\n  ggplot(df_resources[df_resources$Resources == resource, ]) +\n    geom_histogram(aes(x = Prices), color = \"pink\", bins = 30, fill = \"darkred\") +\n    labs(x = \"Price\", y = \"Frequency\", title = resource) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n      axis.title.x = element_text(size = 10),\n      axis.title.y = element_text(size = 10),\n      plot.margin = margin(1, 1, 1, 1, \"cm\")\n    )\n})\n\n# Arrange the plots in a grid\ngrid_plot &lt;- wrap_plots(plotlist = plot_list, nrow = 2)\n\n# Print the grid of histograms\ngrid_plot\n\n\n\n\n\nuranium &lt;- ggplot(df_resources[df_resources$Resources == \"Uranium\",]) +\n                geom_boxplot(aes(y = Prices)) +\n                labs(x = \"\", y = \"Price\", title = \"Resource Distributions\", subtitle = \"Uranium\") +\n                theme_minimal() +\n                theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\")\n                )\n\nggplotly(uranium)\n\n\n\n    \n        \n        \n\n\n\n\n\n\n\n    \n    \n        \n\n    \n\n\n\n\np &lt;- ggplot(df_resources) +\n        geom_line(aes(x = DATE, y = Prices), color = \"darkred\") + \n        labs(x = \"Date\", y = \"Price\", title = \"Resources Prices\") +\n        theme_minimal() +\n        theme(\n                plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n                axis.title.x = element_text(size = 10),\n                axis.title.y = element_text(size = 10),\n                plot.margin = margin(1, 1, 1, 1, \"cm\"))+\n        facet_grid(Resources ~ ., scales = \"free\")    \n\nggplotly(p, width = 800, height = 500)"
  },
  {
    "objectID": "Jupyter_Notebooks/3_data_cleaning R.html",
    "href": "Jupyter_Notebooks/3_data_cleaning R.html",
    "title": "Data Gathering R",
    "section": "",
    "text": "Libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(readxl)\nlibrary(zoo)"
  },
  {
    "objectID": "Jupyter_Notebooks/3_data_cleaning R.html#global-lithium-production",
    "href": "Jupyter_Notebooks/3_data_cleaning R.html#global-lithium-production",
    "title": "Data Gathering R",
    "section": "Global Lithium Production",
    "text": "Global Lithium Production\n\ndf_production &lt;- read.csv(\"../../data/00-raw-data/lithium-production.csv\")\n\ndf_production &lt;- df_production %&gt;% filter(nchar(Code) == 3)\n\nhead(df_production, n = 10)\n\n#summary(df_production)\n\n\nA data.frame: 10 × 4\n\n\n\nEntity\nCode\nYear\nLithium.production...kt\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\nArgentina\nARG\n1995\n8\n\n\n2\nArgentina\nARG\n1996\n8\n\n\n3\nArgentina\nARG\n1997\n8\n\n\n4\nArgentina\nARG\n1998\n1130\n\n\n5\nArgentina\nARG\n1999\n200\n\n\n6\nArgentina\nARG\n2000\n200\n\n\n7\nArgentina\nARG\n2001\n200\n\n\n8\nArgentina\nARG\n2002\n946\n\n\n9\nArgentina\nARG\n2003\n960\n\n\n10\nArgentina\nARG\n2004\n1970"
  },
  {
    "objectID": "Jupyter_Notebooks/3_data_cleaning R.html#chinese-yuan-renminbi-to-u.s.-dollar-spot-exchange-rate",
    "href": "Jupyter_Notebooks/3_data_cleaning R.html#chinese-yuan-renminbi-to-u.s.-dollar-spot-exchange-rate",
    "title": "Data Gathering R",
    "section": "Chinese Yuan Renminbi to U.S. Dollar Spot Exchange Rate",
    "text": "Chinese Yuan Renminbi to U.S. Dollar Spot Exchange Rate\n\n\nLibraries\n# Set the start and end dates\nstart_date &lt;- \"2010-01-01\"\nend_date &lt;- \"2022-12-31\"\n\n# Define the symbol for CNY to USD exchange rate\nsymbol &lt;- \"DEXCHUS\"\n\n# Use getSymbols() to fetch the data\ngetSymbols(symbol, from = start_date, to = end_date, src = \"FRED\")\n\n# Access the data as a data frame\ndf_exchange_rate &lt;- as.data.frame(DEXCHUS)\n\ndf_exchange_rate &lt;- rownames_to_column(df_exchange_rate, var = \"DATE\")\n\ndf_exchange_rate$DATE &lt;- as.Date(df_exchange_rate$DATE)\n\n# Print the first few rows of the data\nhead(df_exchange_rate)\n\nsummary(df_exchange_rate)\n\n\n'DEXCHUS'\n\n\n\nA data.frame: 6 × 2\n\n\n\nDATE\nDEXCHUS\n\n\n\n&lt;date&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n2010-01-01\nNA\n\n\n2\n2010-01-04\n6.8273\n\n\n3\n2010-01-05\n6.8258\n\n\n4\n2010-01-06\n6.8272\n\n\n5\n2010-01-07\n6.8280\n\n\n6\n2010-01-08\n6.8274\n\n\n\n\n\n      DATE               DEXCHUS     \n Min.   :2010-01-01   Min.   :6.040  \n 1st Qu.:2013-04-02   1st Qu.:6.309  \n Median :2016-07-01   Median :6.504  \n Mean   :2016-07-01   Mean   :6.548  \n 3rd Qu.:2019-10-01   3rd Qu.:6.805  \n Max.   :2022-12-30   Max.   :7.305  \n                      NA's   :140"
  },
  {
    "objectID": "Jupyter_Notebooks/3_data_cleaning R.html#commodity-price",
    "href": "Jupyter_Notebooks/3_data_cleaning R.html#commodity-price",
    "title": "Data Gathering R",
    "section": "Commodity Price",
    "text": "Commodity Price\n\nUranium\n\n\nData Cleaning Code\ndf_commodity_price &lt;- read_excel(\"../../data/00-raw-data/commodity_price.xlsx\")\n\ndf_commodity_price &lt;- df_commodity_price %&gt;%\n  pivot_longer(cols = -c('...1'), \n               names_to = \"Month_Year\",\n               values_to = \"Price\")\n\ndf_commodity_price &lt;- df_commodity_price %&gt;% filter(!is.na(Price) & Price != \"\")\n\ndf_commodity_price$Month_Year &lt;- as.yearmon(df_commodity_price$Month_Year, format = \"%b %Y\")\n\ndf_commodity_price$Month_Year &lt;- format(df_commodity_price$Month_Year, \"%m-%Y\")\n\ndf_commodity_price$Month_Year &lt;- paste(\"01-\", df_commodity_price$Month_Year, sep = \"\")\n\ndf_commodity_price$Month_Year &lt;- as.Date(df_commodity_price$Month_Year, format = \"%d-%m-%Y\")\n\nnames(df_commodity_price) &lt;- c('Commodity', 'DATE', 'Price')\n\ndf_uranium_price &lt;- df_commodity_price %&gt;%\n  filter(Commodity == \"Uranium\")\n\n#df_uranium_price &lt;- head(df_uranium_price, n = 10)\n\n\n\nhead(df_uranium_price,20)\n\n\nA tibble: 20 × 3\n\n\nCommodity\nDATE\nPrice\n\n\n&lt;chr&gt;\n&lt;date&gt;\n&lt;dbl&gt;\n\n\n\n\nUranium\n2012-01-01\n52.31250\n\n\nUranium\n2012-02-01\n52.05556\n\n\nUranium\n2012-03-01\n51.28889\n\n\nUranium\n2012-04-01\n51.30000\n\n\nUranium\n2012-05-01\n51.88889\n\n\nUranium\n2012-06-01\n50.83333\n\n\nUranium\n2012-07-01\n50.35556\n\n\nUranium\n2012-08-01\n49.25000\n\n\nUranium\n2012-09-01\n47.72500\n\n\nUranium\n2012-10-01\n44.61111\n\n\nUranium\n2012-11-01\n41.50000\n\n\nUranium\n2012-12-01\n43.66667\n\n\nUranium\n2013-01-01\n42.75000\n\n\nUranium\n2013-02-01\n43.40625\n\n\nUranium\n2013-03-01\n42.28125\n\n\nUranium\n2013-04-01\n41.41250\n\n\nUranium\n2013-05-01\n40.60500\n\n\nUranium\n2013-06-01\n39.93750\n\n\nUranium\n2013-07-01\n38.02222\n\n\nUranium\n2013-08-01\n34.99416\n\n\n\n\n\n\n\nNatural Gas\n\n# Set the start and end dates\nstart_date &lt;- \"2012-01-01\"\nend_date &lt;- \"2022-12-31\"\n\n# Define the symbol for Gas Price to USD exchange rate\nsymbol &lt;- \"GASREGCOVW\"\n\n# Use getSymbols() to fetch the data\ngetSymbols(symbol, from = start_date, to = end_date, src = \"FRED\")\n\n# Access the data as a data frame\ndf_gas_price &lt;- as.data.frame(GASREGCOVW)\n\ndf_gas_price &lt;- rownames_to_column(df_gas_price, var = \"DATE\")\n\ndf_gas_price$DATE &lt;- as.Date(df_gas_price$DATE)\n\n# Print the first few rows of the data\nhead(df_gas_price, 10)\n\n'GASREGCOVW'\n\n\n\nA data.frame: 10 × 2\n\n\n\nDATE\nGASREGCOVW\n\n\n\n&lt;date&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n2012-01-02\n3.254\n\n\n2\n2012-01-09\n3.333\n\n\n3\n2012-01-16\n3.342\n\n\n4\n2012-01-23\n3.333\n\n\n5\n2012-01-30\n3.386\n\n\n6\n2012-02-06\n3.436\n\n\n7\n2012-02-13\n3.466\n\n\n8\n2012-02-20\n3.523\n\n\n9\n2012-02-27\n3.641\n\n\n10\n2012-03-05\n3.717\n\n\n\n\n\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    mutate(Year = year(DATE)) %&gt;%\n    mutate(Month = month(DATE)) %&gt;%\n    mutate(Day = day(DATE))\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    group_by(Year, Month) %&gt;%\n    filter(Day == min(Day)) %&gt;%\n    ungroup()\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    select(GASREGCOVW, Year, Month, Day)\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    mutate(DATE = paste(Year, Month, '01', sep = \"-\"))\n\ndf_gas_price$DATE &lt;- as.Date(df_gas_price$DATE)\n\ndf_gas_price &lt;- df_gas_price %&gt;%\n    select(GASREGCOVW, DATE)\n\nhead(df_gas_price, 10)\n\n\nA tibble: 10 × 2\n\n\nGASREGCOVW\nDATE\n\n\n&lt;dbl&gt;\n&lt;date&gt;\n\n\n\n\n3.254\n2012-01-01\n\n\n3.436\n2012-02-01\n\n\n3.717\n2012-03-01\n\n\n3.874\n2012-04-01\n\n\n3.718\n2012-05-01\n\n\n3.518\n2012-06-01\n\n\n3.291\n2012-07-01\n\n\n3.606\n2012-08-01\n\n\n3.797\n2012-09-01\n\n\n3.750\n2012-10-01\n\n\n\n\n\n\n\nResources Prices\n\ndf_uranium_price &lt;- df_uranium_price %&gt;% select(DATE, Price)\n\nnames(df_uranium_price) &lt;- c('DATE', 'Uranium')\n\nnames(df_gas_price) &lt;- c('Natural Gas', 'DATE')\n\ndf_resource_price &lt;- merge(df_uranium_price, df_gas_price, by.x = 'DATE', by.y = 'DATE', all = TRUE)\n\ndf_resource_price &lt;- na.omit(df_resource_price)\n\n# head(df_resource_price, 10)\n\ndf_resource_price\n\n\nA data.frame: 132 × 3\n\n\n\nDATE\nUranium\nNatural Gas\n\n\n\n&lt;date&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n2012-01-01\n52.31250\n3.254\n\n\n2\n2012-02-01\n52.05556\n3.436\n\n\n3\n2012-03-01\n51.28889\n3.717\n\n\n4\n2012-04-01\n51.30000\n3.874\n\n\n5\n2012-05-01\n51.88889\n3.718\n\n\n6\n2012-06-01\n50.83333\n3.518\n\n\n7\n2012-07-01\n50.35556\n3.291\n\n\n8\n2012-08-01\n49.25000\n3.606\n\n\n9\n2012-09-01\n47.72500\n3.797\n\n\n10\n2012-10-01\n44.61111\n3.750\n\n\n11\n2012-11-01\n41.50000\n3.406\n\n\n12\n2012-12-01\n43.66667\n3.337\n\n\n13\n2013-01-01\n42.75000\n3.233\n\n\n14\n2013-02-01\n43.40625\n3.471\n\n\n15\n2013-03-01\n42.28125\n3.698\n\n\n16\n2013-04-01\n41.41250\n3.572\n\n\n17\n2013-05-01\n40.60500\n3.478\n\n\n18\n2013-06-01\n39.93750\n3.610\n\n\n19\n2013-07-01\n38.02222\n3.410\n\n\n20\n2013-08-01\n34.99416\n3.566\n\n\n21\n2013-09-01\n34.45329\n3.575\n\n\n22\n2013-10-01\n34.84850\n3.298\n\n\n23\n2013-11-01\n35.59200\n3.209\n\n\n24\n2013-12-01\n34.59330\n3.209\n\n\n25\n2014-01-01\n35.21499\n3.261\n\n\n26\n2014-02-01\n35.54674\n3.243\n\n\n27\n2014-03-01\n34.70286\n3.421\n\n\n28\n2014-04-01\n32.74457\n3.533\n\n\n29\n2014-05-01\n28.53986\n3.606\n\n\n30\n2014-06-01\n28.22546\n3.624\n\n\n...\n...\n...\n...\n\n\n103\n2020-07-01\n32.36087\n2.100\n\n\n104\n2020-08-01\n31.37619\n2.085\n\n\n105\n2020-09-01\n29.98182\n2.122\n\n\n106\n2020-10-01\n29.63636\n2.091\n\n\n107\n2020-11-01\n29.48571\n2.021\n\n\n108\n2020-12-01\n29.77391\n2.063\n\n\n109\n2021-01-01\n29.85714\n2.160\n\n\n110\n2021-02-01\n28.66250\n2.316\n\n\n111\n2021-03-01\n28.33478\n2.625\n\n\n112\n2021-04-01\n29.75227\n2.777\n\n\n113\n2021-05-01\n30.25714\n2.790\n\n\n114\n2021-06-01\n32.14211\n2.935\n\n\n115\n2021-07-01\n32.33864\n3.032\n\n\n116\n2021-08-01\n32.14773\n3.059\n\n\n117\n2021-09-01\n45.07812\n3.080\n\n\n118\n2021-10-01\n38.47619\n3.093\n\n\n119\n2021-11-01\n31.09958\n3.280\n\n\n120\n2021-12-01\n36.12930\n3.204\n\n\n121\n2022-01-01\n36.87292\n3.141\n\n\n122\n2022-02-01\n35.83191\n3.330\n\n\n123\n2022-03-01\n45.51470\n3.963\n\n\n124\n2022-04-01\n48.70473\n4.021\n\n\n125\n2022-05-01\n40.89163\n4.031\n\n\n126\n2022-06-01\n40.33152\n4.702\n\n\n127\n2022-07-01\n38.93916\n4.619\n\n\n128\n2022-08-01\n39.80231\n4.034\n\n\n129\n2022-09-01\n40.95039\n3.617\n\n\n130\n2022-10-01\n41.30360\n3.592\n\n\n131\n2022-11-01\n40.95406\n3.628\n\n\n132\n2022-12-01\n39.17824\n3.260"
  },
  {
    "objectID": "7_clustering.html",
    "href": "7_clustering.html",
    "title": "Data Clustering",
    "section": "",
    "text": "Libraries\n# Import Data\nimport pandas as pd\n\n# Data Cleaning\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# PCA analysis\nfrom sklearn.decomposition import PCA\n\n# t-SNE analysis\nfrom sklearn.manifold import TSNE\n\n# Visualize Results\nimport matplotlib.pyplot as plt\n\n# Clustering\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.cluster"
  },
  {
    "objectID": "7_clustering.html#introduction",
    "href": "7_clustering.html#introduction",
    "title": "Data Clustering",
    "section": "Introduction",
    "text": "Introduction\n\nThe purpose of this section is to evaluate the financial information for the lithium companies and understand how they relate to each other. As mentioned in the previous sections, the data sets consist of three types of companies, with each group differing in their manufacturing process. The companies specialize in lithium production, lithium battery production, and electric vehicle production.\nAll companies in the lithium industry have grown actively in recent years. However, their performance is correlated with the type of production they are developing.\nClustering analysis allows us to group similar companies together to evaluate their relationship and discover patterns over time. We also want to find the most important outliers and how they have changed in recent years to assess the main reasons for these changes.\n\nBelow there is the list of all the packages and libraries that were imported to do the analysis:\n\n\nLibraries\n# Import Data\nimport pandas as pd\n\n# Data Cleaning\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# PCA analysis\nfrom sklearn.decomposition import PCA\n\n# t-SNE analysis\nfrom sklearn.manifold import TSNE\n\n# Visualize Results\nimport matplotlib.pyplot as plt\n\n# Clustering\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.cluster"
  },
  {
    "objectID": "7_clustering.html#theory",
    "href": "7_clustering.html#theory",
    "title": "Data Clustering",
    "section": "Theory",
    "text": "Theory\n\nThe machine learning clustering algorithms that will be used in this section are K-Means, Density-based spatial clustering of applications with noise (DBSCAN) and Hierarchical or Agglomerative Clustering. These algorithms have a similar output but the method to get the result is different.\nTo create the k-means model that will be applied to the analysis, we will use the k-means clustering function from the Sklearn package. The most important parameter for creating this model is the number of clusters. The first centroids are randomly located in the resulting space, and the algorithms calculate the distance of each point to the nearest algorithm that assigns this new label. After a certain number of iterations, they will converge, resulting in no significant difference. To choose the optimal number of clusters for this method, we will use the silhoutte metric explained in the hyperparameter tuning section.\nTo create the DBSCAN model, we will use the dbscan function from the Sklearn package. DBSCAN model groups the data point by density in the space. Similarly to k-means, in every iteration the distance from the points to the centroids is calculated in order to assign the points to the clusters. The function has a different parameter, eps, which is the calculation of the number of clusters: \\(eps = 0.25\\) x \\((clusters - 1)\\). This model do not assign outlier points to the clusters which this is a disadvantage.\nTo create the hierarchical clustering, we will use the agglomerative function from the Sklearn package. This model focuses on creating a hierarchy of clusters and merging them on each iteration.\nThere are two methods we can use to evaluate the performance of the models and make decisions about improvements through the hyperparameter tuning technique. For k-means, we can use the elbow method, which consists of determining the optimal number of clusters by observing the inflection point. This means that the marginal value is close to 0, since adding one more cluster does not make a significant difference in performance to justify the complexity. The second method is the silhouette metric, which evaluates the quality of the clusters and the goal is to find the number of clusters that maximizes this value while minimizing the complexity."
  },
  {
    "objectID": "7_clustering.html#methods",
    "href": "7_clustering.html#methods",
    "title": "Data Clustering",
    "section": "Methods",
    "text": "Methods\n\nThere are several steps to the methodology for this section. The first is to select the data to be used for the clustering analysis. This data should not contain any labels for this analysis, as we want to find a relationship between the companies without knowing the actual relationship.\nThe next step is to find the optimal number of clusters per algorithm in the hyperparameter tuning. For this analysis we use a function developed during the course, where the input parameters are the data itself, the algorithm to be evaluated, the maximum number of clusters, and a boolean parameter to show the output or not. This step is explained in detail in the Hyperparameter Tuning section. With the optimal number of clusters, we will apply the final model to the data and plot the results to find insights and draw conclusions."
  },
  {
    "objectID": "7_clustering.html#results",
    "href": "7_clustering.html#results",
    "title": "Data Clustering",
    "section": "Results",
    "text": "Results\n\nThe first clustering method was k-means for 2019 data. Here we can observe the two clusters that were identified. The yellow points are closer together compared to the purple ones which are far from the cluster and far from each other. Comparing this plot to the 2022 data plot we can observe there is one point that was classified to the other cluster. In addition, we can observe that the location of the clusters was shifted down.\nThe second clustering method was DBSCAN applied for 2019 and 2022 data. As it was explained earlier, this clustering method is identifying only one cluster because there are two outlier points that cannot be classified as to a cluster. This approach is very useful for datasets that contain more that 2 clusters because outliers can be easily identified instead to be classified to a cluster. On the other hand, for this specific case, it was a disadvantage because there was waste of time while trying to troubleshoot the code.\nThe third method is Hierarchical clustering. When we apply the selected model to the two datasets, we can observe that the results are very similar to the k-means model, therefore the conclusions are the same.\nIn gerenal terms, both k-means and Hierarchical clustering suggests that there is a clear difference in the financial performance of the companies during this years."
  },
  {
    "objectID": "7_clustering.html#conclusions",
    "href": "7_clustering.html#conclusions",
    "title": "Data Clustering",
    "section": "Conclusions",
    "text": "Conclusions\n\nAs a summary for the analysis done, we first selected the data we wanted to use, then we used the function to maximize the silhouette metric to find the optimal number of clusters for each model and finally plotted the results and found insights.\nClustering can be very useful to identify similar data points without having existing labels. For this analysis we took an additional step increasing the complexity of the analysis as we did two different data sets with the goal to understand how the clusters have changed over time. This allows to get additional insights of the data analyzed.\nIn future analysis it would be useful to create a function that automatically builds visualizations that allow to compare the clusters over a certain period of time in order to understand how the relationship between the data changes."
  },
  {
    "objectID": "6_dimensionality_reduction.html#introduction",
    "href": "6_dimensionality_reduction.html#introduction",
    "title": "Dimensionality Reduction",
    "section": "Introduction",
    "text": "Introduction\n\nThe objective of this section is to use the dimensionality reduction technique to evaluate the financial statements of lithium companies. This allows for complex analyses and comparisons between companies to discover patterns and insights.\nThe results obtained will later be applied to clustering analysis. Since the number of features is high, dimensionality reduction allows to reduce the risk of overfitting, thus improving the performance of other machine learning algorithms.\n\nBelow there is the list of all the packages and libraries that were imported to do the analysis:\n\n\nLibraries\n# Import Data\nimport pandas as pd\n\n# Data Cleaning\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# PCA analysis\nfrom sklearn.decomposition import PCA\n\n# t-SNE analysis\nfrom sklearn.manifold import TSNE\n\n# Visualize Results\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "6_dimensionality_reduction.html#theory",
    "href": "6_dimensionality_reduction.html#theory",
    "title": "Dimensionality Reduction",
    "section": "Theory",
    "text": "Theory\n\nWrite a brief technical introduction to Dimensionality Reduction with PCA and Dimensionality Reduction with t-SNE:"
  },
  {
    "objectID": "6_dimensionality_reduction.html#methods",
    "href": "6_dimensionality_reduction.html#methods",
    "title": "Dimensionality Reduction",
    "section": "Methods",
    "text": "Methods\n\nDescribe your coding workflow\n\n\nData selection\n\nAs explained earlier, the dataset has 12 rows and 79 columns. The 12 rows correspond to the companies to be analyzed. The 79 columns contain the variables from the quarterly financial report that make up the companies’ income statement and balance sheet.\nFor this analysis, we want to compare the results of the dimensionality reduction techniques and see how they differ over time. We have filtered the data so that Q4 2019 is the first data frame and Q4 2022 is the second data frame. This analysis is important to understand how the dimensionality reduction changed and was affected by COVID.\n\nFinancial Information for 2019 Companies\n\n\n{python}\ndf_2019 = df[(df['Year'] == 2019) & (df['Quarter'] == 4)]\n\ndf_2019 = df_2019.pivot(index=['Company'], columns='KPI', values='Value')\n\ndf_2019.reset_index(inplace=True)\n\ndf_2019 = df_2019.fillna(0)\n\ndf_2019.head(15)\n\n\nKPI Company  Accounts Payable  ...  Total Long-Term Liabilities  Working Capital\n0      AEHR           2320.00  ...                 1.988000e+03         14523.00\n1       ALB         574138.00  ...                 4.358287e+06        816113.00\n2     BYDDF       36168168.00  ...                 2.501123e+07       1062342.00\n3         F       20673000.00  ...                 1.271750e+08      15915000.00\n4      LTHM          83100.00  ...                 1.931000e+05        149500.00\n5        ON         543600.00  ...                 3.283000e+06       1201600.00\n6     PCRFY        1021436.00  ...                 1.357557e+06        638028.00\n7       PLL            566.23  ...                 7.391400e+02         10571.25\n8      RIVN          27000.00  ...                 7.300000e+04       2108000.00\n9      SGML           2966.61  ...                 6.816340e+03          4887.72\n10     TSLA        3771000.00  ...                 1.553200e+07       1436000.00\n11     XPEV              0.00  ...                 3.090626e+06       1662959.00\n\n[12 rows x 79 columns]\n\n\n\n\n\nShape: (12, 79)\n\n\nFinancial Information for 2022 Companies\n\n\n{python}\ndf_2022 = df[(df['Year'] == 2022) & (df['Quarter'] == 4)]\n\ndf_2022 = df_2022.pivot(index=['Company'], columns='KPI', values='Value')\n\ndf_2022.reset_index(inplace=True)\n\ndf_2022 = df_2022.fillna(0)\n\ndf_2022.head(15)\n\n\nKPI Company  Accounts Payable  ...  Total Long-Term Liabilities  Working Capital\n0      AEHR            3949.0  ...                        130.0          54789.0\n1       ALB         2052001.0  ...                    4524660.0        2445902.0\n2     BYDDF       143765729.0  ...                   39126248.0       92541054.0\n3         F        25605000.0  ...                  115851000.0       19610000.0\n4      LTHM           81700.0  ...                     482500.0         395300.0\n5      LVWR           12788.0  ...                      10562.0         267487.0\n6        ON          852100.0  ...                    3710100.0        3668000.0\n7     PCRFY         1263813.0  ...                    1414040.0        1165724.0\n8       PLL           12862.0  ...                       4221.0          88448.0\n9      RIVN         1000000.0  ...                    1653000.0       10706000.0\n10     SGML            1936.0  ...                      88360.0          77052.0\n11     TSLA        15255000.0  ...                    9731000.0       14208000.0\n12     XPEV        14313967.0  ...                   10465488.0       19412568.0\n\n[13 rows x 79 columns]\n\n\n\n\n\nShape: (13, 79)\n\n\n\n\nPCA\nPCA Number of Components\n\n\n\n\n\nPCA Financial Information Analysis for 2019 Companies\n\n\n{python}\nnumeric_cols = df_2019.columns[df_2019.dtypes != 'object']\nX_2019 = df_2019[numeric_cols]\n\ntarget_col = df_2019.columns[df_2019.dtypes == 'object']\nY = df_2019[target_col]\nY , _ = pd.factorize(Y.Company)\nY_2019 = Y.astype(int)\n\n# Standardize the data\nscaler = StandardScaler()\nX_2019 = scaler.fit_transform(X_2019)\n\nn_components = 9\n\npca = PCA(n_components = n_components)\nX1_2019 = pca.fit_transform(X_2019)\n\nresults_2019 = pd.DataFrame({'X1': X1_2019[:, 0], 'X2': X1_2019[:, 1], 'Y': Y_2019})\n\nscatter = plt.scatter(X1_2019[:, 0], X1_2019[:, 1], c=Y_2019, cmap='tab10')\n\n# Add custom labels for each point\nfor i, txt in enumerate(df_2019[target_col].Company):\n    plt.annotate(txt, (X1_2019[i, 0], X1_2019[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n\n\n\n\nPCA Financial Information Analysis for 2022 Companies\n\n\n{python}\nnumeric_cols = df_2022.columns[df_2022.dtypes != 'object']\nX_2022 = df_2022[numeric_cols]\n\ntarget_col = df_2022.columns[df_2022.dtypes == 'object']\nY = df_2022[target_col]\nY , _ = pd.factorize(Y.Company)\nY_2022 = Y.astype(int)\n\n# Standardize the data\nscaler = StandardScaler()\nX_2022 = scaler.fit_transform(X_2022)\n\nn_components = 9\n\npca = PCA(n_components = n_components)\nX1_2022 = pca.fit_transform(X_2022)\n\nresults_2022 = pd.DataFrame({'X1': X1_2022[:, 0], 'X2': X1_2022[:, 1], 'Y': Y_2022})\n\nscatter = plt.scatter(X1_2022[:, 0], X1_2022[:, 1], c=Y_2022, cmap='tab10')\n\n# Add custom labels for each point\nfor i, txt in enumerate(df_2022[target_col].Company):\n    plt.annotate(txt, (X1_2022[i, 0], X1_2022[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n\n\n\n\n\n\nt-SNE\nt-SNE Financial Information Analysis for 2019 Companies\n\n\n\n\n\nt-SNE Financial Information Analysis for 2022 Companies\n\n\n{python}\nperplexity = 10\n\ntsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=perplexity, random_state=42)\n\ntsne_2022 = tsne.fit_transform(X_2022)\n\nscatter_tsne_2022 = plt.scatter(tsne_2022[:,0],tsne_2022[:,1], c=Y_2022, alpha=0.5)\n\n# Add custom labels for each point\nfor i, txt in enumerate(df_2022[target_col].Company):\n    plt.annotate(txt, (tsne_2022[i, 0], tsne_2022[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')"
  },
  {
    "objectID": "6_dimensionality_reduction.html#results",
    "href": "6_dimensionality_reduction.html#results",
    "title": "Dimensionality Reduction",
    "section": "Results",
    "text": "Results\n\nDiscuss, illustrate, and compare the results of your various dimensionality reduction methods"
  },
  {
    "objectID": "6_dimensionality_reduction.html#conclusions",
    "href": "6_dimensionality_reduction.html#conclusions",
    "title": "Dimensionality Reduction",
    "section": "Conclusions",
    "text": "Conclusions\n\nSummarize & wrap-up the report\nKey and important findings and how these findings affect real-life and real people\n\n\n\n\nReferences:"
  },
  {
    "objectID": "6_dimensionality_reduction.html",
    "href": "6_dimensionality_reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Libraries\n# Import Data\nimport pandas as pd\n\n# Data Cleaning\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# PCA analysis\nfrom sklearn.decomposition import PCA\n\n# t-SNE analysis\nfrom sklearn.manifold import TSNE\n\n# Visualize Results\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "6_dimensionality_reduction.html#code-implementation",
    "href": "6_dimensionality_reduction.html#code-implementation",
    "title": "Dimensionality Reduction",
    "section": "Code Implementation",
    "text": "Code Implementation\n\nTo apply dimensionality reduction, the first step was to select the dataset to use. Once the data was imported, we selected the subset based on the data science objectives.\nThe next step is to compute the optimal number of PCA components, which is explained in detail below. With this optimal value, we apply the PCA analysis to both of the datasets and evaluate the results.\nWe use the same subsets for the t-SNE algorithm to evaluate the output for the datasets.\n\n\nPCA Number of Components\n\nFor the PCA analysis, we calculated the optimal number of components where the cumulative explained variance reaches a plateau. This means that an additional component does not improve the explained variance. In this case, the value chosen for the number of components is 9.\n\nCumulative Explained Variance by Number of Components Plot\n\n\n\n\n\n\n\nPCA Analysis\n\nIn order to perform the PCA analysis on the subset data frames, we first cleaned the data. The first step was to separate the labels from the features. Since the target variable is the company name, we converted them to numerical factors.\nThe next step was to transform the X-variables using the StandardScaler() function. This is important because even though all the information is in the same unit, US dollars, they have different scales. Therefore, we use it to transform the data using the mean and standard deviation.\nWe then apply the PCA model using the calculated number of components and save the results in a data frame. This is useful for plotting the results and matching them to the original labels.\n\nPCA Financial Information Analysis for 2019 Companies\n\n\n{python}\nnumeric_cols = df_2019.columns[df_2019.dtypes != 'object']\nX_2019 = df_2019[numeric_cols]\n\ntarget_col = df_2019.columns[df_2019.dtypes == 'object']\nY = df_2019[target_col]\nY , _ = pd.factorize(Y.Company)\nY_2019 = Y.astype(int)\n\n# Standardize the data\nscaler = StandardScaler()\nX_2019 = scaler.fit_transform(X_2019)\n\nn_components = 9\n\npca = PCA(n_components = n_components)\nX1_2019 = pca.fit_transform(X_2019)\n\nresults_2019 = pd.DataFrame({'X1': X1_2019[:, 0], 'X2': X1_2019[:, 1], 'Y': Y_2019})\n\nscatter = plt.scatter(X1_2019[:, 0], X1_2019[:, 1], c=Y_2019, cmap='tab10')\n\n# Add custom labels for each point\nfor i, txt in enumerate(df_2019[target_col].Company):\n    plt.annotate(txt, (X1_2019[i, 0], X1_2019[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n\n\n\n\nPCA Financial Information Analysis for 2022 Companies\n\n\n{python}\nnumeric_cols = df_2022.columns[df_2022.dtypes != 'object']\nX_2022 = df_2022[numeric_cols]\n\ntarget_col = df_2022.columns[df_2022.dtypes == 'object']\nY = df_2022[target_col]\nY , _ = pd.factorize(Y.Company)\nY_2022 = Y.astype(int)\n\n# Standardize the data\nscaler = StandardScaler()\nX_2022 = scaler.fit_transform(X_2022)\n\nn_components = 9\n\npca = PCA(n_components = n_components)\nX1_2022 = pca.fit_transform(X_2022)\n\nresults_2022 = pd.DataFrame({'X1': X1_2022[:, 0], 'X2': X1_2022[:, 1], 'Y': Y_2022})\n\nscatter = plt.scatter(X1_2022[:, 0], X1_2022[:, 1], c=Y_2022, cmap='tab10')\n\n# Add custom labels for each point\nfor i, txt in enumerate(df_2022[target_col].Company):\n    plt.annotate(txt, (X1_2022[i, 0], X1_2022[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n\n\n\n\n\n\nt-SNE Analysis\n\nThe procedure for the t-SNE is the same, but the model used is different. This is because the hyperparameter used to tune the model is the perplexity.\nThe perplexity value is similar to the number of components in the PCA model. A low perplexity considers fewer neighbors in the analysis. A higher perplexity considers more neighbors and focuses the analysis on the local relationships.\nIn this case, we chose a perplexity of 10, which can be clearly seen in the results of the t-SNE plot, as the data points of the output are more spread out.\n\nt-SNE Financial Information Analysis for 2019 Companies\n\n\n{python}\nperplexity = 10\n\ntsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=perplexity, random_state=42)\n\ntsne_2019 = tsne.fit_transform(X_2019)\n\nscatter_tsne_2019 = plt.scatter(tsne_2019[:,0],tsne_2019[:,1], c=Y_2019, alpha=0.5)\n\n# Add custom labels for each point\nfor i, txt in enumerate(df_2019[target_col].Company):\n    plt.annotate(txt, (tsne_2019[i, 0], tsne_2019[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n\n\n\n\nt-SNE Financial Information Analysis for 2022 Companies\n\n\n{python}\nperplexity = 10\n\ntsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=perplexity, random_state=42)\n\ntsne_2022 = tsne.fit_transform(X_2022)\n\nscatter_tsne_2022 = plt.scatter(tsne_2022[:,0],tsne_2022[:,1], c=Y_2022, alpha=0.5)\n\n# Add custom labels for each point\nfor i, txt in enumerate(df_2022[target_col].Company):\n    plt.annotate(txt, (tsne_2022[i, 0], tsne_2022[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')"
  },
  {
    "objectID": "6_dimensionality_reduction.html#project-report",
    "href": "6_dimensionality_reduction.html#project-report",
    "title": "Dimensionality Reduction",
    "section": "Project Report",
    "text": "Project Report\n\nWhen interpreting the above results, it is important to keep in mind that the lithium market has evolved very rapidly in recent years and, like other manufacturing and transportation industries, has been affected by COVID.\nThe first analysis performed was the application of the PCA model to the Q4 2019 financial information for the lithium companies. We can see that the majority of the companies have very similar values for the variables, with the exception of Ford and BYD CO., LTD - China. We can also see that Tesla and XPeng Inc. are further away from the higher concentration of companies.\nThe second analysis performed was the application of the PCA model to the Q4 2022 financial information for the lithium companies. After 3 years, we can observe that the same companies seem to have more outliers, but this time the distribution has changed. This could be due to the relevant characteristics included in this analysis compared to the first one.\nThe third analysis performed was the application of the t-SNE model to the Q4 2019 financial information for the lithium companies. As previously explained, the model’s perplexity is very low and the output data points are spread out from each other. This output does not have much explanation rather than plotting the closest companies.\nThe final analysis performed was the application of the t-SNE model to the Q4 2022 financial information for the lithium companies. This analysis is very helpful to compare the results with the previous output. Here we can see that the results have been rearranged and that the local relationships between the companies have changed.\nThe t-SNE results are clearer to understand the relationship compared to PCA. On the other hand, PCA allows us to visualize the possible clusterings that can be done on the data. This analysis will be explained in the following section “Clustering”."
  },
  {
    "objectID": "8_decision_trees.html#introduction",
    "href": "8_decision_trees.html#introduction",
    "title": "Decision Trees",
    "section": "Introduction",
    "text": "Introduction\n\nTo enhance the analysis done in the Naïve Bayes section we will now apply a new machine learning algorithm to the Electric Vehicles Characteristics dataset. In this section we will also introduce Decision Trees, describe the types, advantages and disadvantages, and later dive into the application and analysis.\nFor this analysis we will use Python and below we have the list of libraries imported and the functions created:\n\n\n\nLibraries\n# Import Data\nimport pandas as pd\n\n# Data Cleaning\nimport numpy as np\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Split Dataset\nfrom sklearn.model_selection import train_test_split\n\n# Random Classifier\nfrom collections import Counter\n\n# Sklearn Decision Tree Model\nfrom sklearn import tree\n\n# Show Results\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\n\n\n\nFunction: random_classifier()\n## RANDOM CLASSIFIER \ndef random_classifier(y_data):\n    ypred=[];\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\",Counter(ypred).values()) # counts the elements' frequency\n    print(\"probability of prediction:\",np.fromiter(Counter(ypred).values(), dtype=float)/len(y_data)) # counts the elements' frequency\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data, ypred))\n\n\n\n\nFunction: confusion_plot()\ndef confusion_plot(actual, pred):\n\n    matrix = confusion_matrix(actual, pred)\n\n    TP = matrix[0,0]\n    FN = matrix[0,1]\n    FP = matrix[1,0]\n    TN = matrix[1,1]\n\n    ACCURACY = (TP+TN)/(TP+FP+FN+TN)\n    NEGATIVE_RECALL = TN/(TN+FP)\n    NEGATIVE_PRECISION = TN/(TN+FN)\n    POSITIVE_RECALL = TP/(TP+FN)\n    POSITIVE_PRECISION = TP/(TP+FP)\n\n    print('ACCURACY: ', ACCURACY)\n    print('NEGATIVE RECALL (Y=0): ', NEGATIVE_RECALL)\n    print('NEGATIVE PRECISION (Y=0): ', NEGATIVE_PRECISION)\n    print('POSITIVE RECALL (Y=1): ', POSITIVE_RECALL)\n    print('POSITIVE PRECISION (Y=1): ', POSITIVE_PRECISION)\n    \n    print(matrix)\n\n    cm = confusion_matrix(actual, pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot()\n\n\n\n\nFunction: plot_tree()\ndef plot_tree(model, X, Y, labels):\n    model_X_Y = model.fit(X, Y)\n    \n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model_X_Y, \n                        feature_names=X.columns,\n                        class_names=labels,\n                        filled=True)\n\n    plt.show()"
  },
  {
    "objectID": "8_decision_trees.html#methods",
    "href": "8_decision_trees.html#methods",
    "title": "Decision Trees",
    "section": "Methods",
    "text": "Methods\n\nIn Machine Learning, decision trees are classified as non-parametric supervised learning method used for both classification and regression models.\nThe goal of decision trees is to create a model that predicts the output value of the target variables based on different set of rules. This can be better understood as a set of questions where the first one, the root node, is a very general questions, and based on the answer the trajectory to make the next question. After several questions you can get to the final value.\nTo go from one inner node to another, there may be two possible paths, which we identify as binary split as it divides values into two subsets, or more tan two possible paths, which we identify as multi way split as it uses as many partitions as distinct values.\nThere are different types of decision trees, but in this analysis we will focus in binary classifier, where the output of each node is true or false.\nDecision trees have several advantages such as that they are simple to understand and easy to interpret. This is due to the visualization of the tree that can be created in order to understand the logic and the conditions behind it. It also required minimal data preparation compared to other models. As it was explained before, they can handle both numerical and categorical data and combine them in a way to handle multi-outut problems.\nOne of the main disadvantages of the decision trees is that they tend to overfit, but there are several techniques that can be applied to reduce this, such as pruning or setting a maximum depth. The predictions done by the decision trees are local, this means that if we want to extrapolate to new values, they might not perform well. Lastly, we need to consider that if the dataset is not well balanced, the performance might be poor too because the results will tend to be biased.\n\n\nClass distribution\n\nThe first step is to import the data we will be using for the analysis. As we can observe, the dataset contains both numeric and categoric variables which we will later drop based on the analysis. The original dataset contains 10 columns and 360 rows.\n\n\nOriginal Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrand\nCarName\nBattery\nEfficiency\nFastCharge\nPrice\nRange\nTopSpeed\nAcceleration\nContinent\n\n\n\n\nAbarth\nAbarth 500e Hatchback\n37.8\n168\n370\n37990\n225\n155\n7.0\nEurope\n\n\nAbarth\nAbarth 500e Convertible\n37.8\n168\n370\n40990\n225\n155\n7.0\nEurope\n\n\nAiways\nAiways U6\n60.0\n171\n430\n47588\n350\n160\n7.0\nEurope\n\n\nAiways\nAiways U5\n60.0\n190\n380\n39563\n315\n150\n7.5\nEurope\n\n\nAudi\nAudi Q8 e-tron Sportback 50 quattro\n89.0\n200\n690\n76650\n445\n200\n6.0\nEurope\n\n\n\n\n\n\n\n(360, 10)\n\n\n\n\nData Cleaning\n\nAs it was mentioned in previous sections, there are some cases where we need to apply additional data cleaning steps to satisfy the models’ requirements. In this case we need to filter out the rows that contain NAs. After cleaning the data, we can see that the number of rows was reduced to 307 data points.\n\n\n\n{python}\ndf_vehicles = df_vehicles.dropna()\n\ndf_vehicles.head(5).to_csv('../../data/02-visualization-data/df_vehicles_4.csv', index=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrand\nCarName\nBattery\nEfficiency\nFastCharge\nPrice\nRange\nTopSpeed\nAcceleration\nContinent\n\n\n\n\nAbarth\nAbarth 500e Hatchback\n37.8\n168\n370\n37990\n225\n155\n7.0\nEurope\n\n\nAbarth\nAbarth 500e Convertible\n37.8\n168\n370\n40990\n225\n155\n7.0\nEurope\n\n\nAiways\nAiways U6\n60.0\n171\n430\n47588\n350\n160\n7.0\nEurope\n\n\nAiways\nAiways U5\n60.0\n190\n380\n39563\n315\n150\n7.5\nEurope\n\n\nAudi\nAudi Q8 e-tron Sportback 50 quattro\n89.0\n200\n690\n76650\n445\n200\n6.0\nEurope\n\n\n\n\n\n\n\n(307, 10)\n\n\n\n\nData Exploration\n\nThe next step is to divide the target column from the original data set. In this case, the target column is the continent where the car was manufactured. The numeric columns are the features of the analysis used to construct the decision tree.\nWe can see that there are more records for Europe compared to North America and Asia. This can be a limitation for the decision tree because the classes are unbalanced. Considering that we don’t have many records, we will not downsample the data because we will lose a lot of data points.\n\n\n\n{python}\nlabels = df_vehicles['Continent'].unique()\n\ndf_vehicles['target'] = df_vehicles['Continent']\n\nlength_0 = len(df_vehicles[df_vehicles['target'] == 'North America'])\nlength_1 = len(df_vehicles[df_vehicles['target'] == 'Europe'])\nlength_2 = len(df_vehicles[df_vehicles['target'] == 'Asia'])\n\ntotal = df_vehicles.shape[0]\n\nprint('\\n', 'Number of points with target = North America: ', length_0, length_0/total, '\\n', 'Number of points with target = Europe: ', length_1, length_1/total, '\\n', 'Number of points with target = Asia: ', length_2, length_2/total)\n\n\n\n Number of points with target = North America:  38 0.1237785016286645 \n Number of points with target = Europe:  195 0.6351791530944625 \n Number of points with target = Asia:  74 0.24104234527687296\n\n\n\nTo explore the data we are going to use, we created a correlation plot between the variables. We can see the Pearson correlation between all the variables. There are some that are highly positively correlated and some that don’t have any correlation at all.\nFor example, Acceleration is inversely correlated with all the variables because the Pearson correlation coefficient is close to -1. We can also see that there is a high correlation between Range and Battery.\n\n\n\n{python}\ntarget_column = 'target'\n\nY = df_vehicles[target_column]\nY = pd.Categorical(df_vehicles['Continent']).codes\n\nX = df_vehicles[['Battery', 'Efficiency', 'FastCharge', 'Price', 'Range', 'TopSpeed', 'Acceleration']]\n\ndf_vehicles = df_vehicles[['Battery', 'Efficiency', 'FastCharge', 'Price', 'Range', 'TopSpeed', 'Acceleration']]\n\ncorr = df_vehicles.corr()\n\nsns.set_theme(style=\"white\")\nf, ax = plt.subplots(figsize=(20, 20))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.show()"
  },
  {
    "objectID": "8_decision_trees.html#final-results",
    "href": "8_decision_trees.html#final-results",
    "title": "Decision Trees",
    "section": "Final results",
    "text": "Final results\n\nAfter running the final results, we can see that the shape of the decision tree has changed and the performance metrics have also changed. We can observe an accuracy of 0.86 for the training set and 0.92 for the test set. The accuracy for the test set has increased and the training set has decreased. Considering that both values are high, we will keep the model and conclude that we have obtained very good performance.\n\n\n\n{python}\nbest_max_depth = 5\n\nmodel = tree.DecisionTreeClassifier(max_depth=best_max_depth)\nmodel = model.fit(x_train,y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\nprint(\"------TRAINING------\")\n\n\n------TRAINING------\n\n\n{python}\nconfusion_plot(y_train,yp_train)\n\n\nACCURACY:  0.8647342995169082\nNEGATIVE RECALL (Y=0):  0.9473684210526315\nNEGATIVE PRECISION (Y=0):  0.8780487804878049\nPOSITIVE RECALL (Y=1):  0.6363636363636364\nPOSITIVE PRECISION (Y=1):  0.813953488372093\n[[ 35  20   3]\n [  8 144   2]\n [  0  10  23]]\n\n\n{python}\nprint(\"------TEST------\")\n\n\n------TEST------\n\n\n{python}\nconfusion_plot(y_test,yp_test)\n\n\nACCURACY:  0.9183673469387755\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0):  0.8918918918918919\nPOSITIVE RECALL (Y=1):  0.75\nPOSITIVE PRECISION (Y=1):  1.0\n[[12  4  0]\n [ 0 33  8]\n [ 0  0  5]]\n\n\n\n\n\n\n\n{python}\nplot_tree(model,X,Y, labels)"
  },
  {
    "objectID": "8_decision_trees.html#conclusions",
    "href": "8_decision_trees.html#conclusions",
    "title": "Decision Trees",
    "section": "Conclusions",
    "text": "Conclusions\n\nThe conclusions we can draw from this analysis are very similar to those of Naive Bayes. Based on the results obtained, we can see that the values for the analyzed features are useful to determine in which continent each car was manufactured. Considering the small number of data points, it would be useful in the future to include additional data points to make the analysis more balanced. It would also be useful to add another target label to understand, for example, if we can target the country where the electric vehicles are manufactured.\nCompared to the Naive Bayes model, we can see that a big advantage of decision trees is that they are easy to interpret and understand the decisions made to arrive at the final value."
  },
  {
    "objectID": "3_data_cleaning.html#global-lithium-production",
    "href": "3_data_cleaning.html#global-lithium-production",
    "title": "Data Cleaning",
    "section": "Global Lithium Production",
    "text": "Global Lithium Production\n\nThe Global Lithium Production dataset was downloaded from the Our World in Data website as a csv file. Very few steps were applied to this dataset due to the quality of the original data. The cleaned dataset contains 224 rows and 4 columns.\nThe access link to the cleaned dataset can be found below the table.\n\n\n\nData Cleaning\n# Read csv file\ndf_production &lt;- read.csv(\"../../data/00-raw-data/lithium-production.csv\")\n\n# Filter code column \ndf_production &lt;- df_production %&gt;% filter(nchar(Code) == 3)\n\n# Change column names\nnames(df_production) &lt;- c('Country', 'Code', 'Year', 'Production')\n\n# Save dataframe as a new file\nwrite.csv(df_production, '../../data/01-modified-data/clean_lithium-production.csv', row.names = FALSE)\n\n# Read cleaned file\ndf_production &lt;- read.csv('../../data/01-modified-data/clean_lithium-production.csv')\n\n# View data\nknitr::kable(head(df_production, 5))\n\n\n\n\n\nCountry\nCode\nYear\nProduction\n\n\n\n\nArgentina\nARG\n1995\n8\n\n\nArgentina\nARG\n1996\n8\n\n\nArgentina\nARG\n1997\n8\n\n\nArgentina\nARG\n1998\n1130\n\n\nArgentina\nARG\n1999\n200\n\n\n\n\n\n\n\nDataset - Github"
  },
  {
    "objectID": "3_data_cleaning.html#lithium-companies-financial-reports",
    "href": "3_data_cleaning.html#lithium-companies-financial-reports",
    "title": "Data Cleaning",
    "section": "Lithium Companies Financial Reports",
    "text": "Lithium Companies Financial Reports\n\nWe collected stock data for relevant lithium-related companies from January 1, 2000 to December 31, 2022. Using the stocks_cleaning() and merge_dataframes() functions defined at the beginning of this section facilitated the systematic cleaning and organization of the data. This process resulted in the creation of a consolidated dataset that set the stage for subsequent analyses. The cleaned dataset consists of 8398 rows and 15 columns.\nThe link to the cleaned dataset can be found below the table.\n\n\n\nData Cleaning - Stocks Data\ndf_list &lt;- list()\n\nstart &lt;- \"2000-01-01\"\nend &lt;- \"2022-12-31\"\n\ntickers_Lithium &lt;- c(\"ALB\", \"LTHM\", \"SGML\", \"PLL\")\ntickers_EV &lt;- c(\"TSLA\", \"F\", \"LI\", \"ON\", \"RIVN\", \"XPEV\", \"LVWR\", \"AEHR\")\ntickers_Batteries &lt;- c(\"BYDDF\", \"6752.T\")\n\ntickers &lt;- append(tickers_Lithium, tickers_EV)\n\ntickers &lt;- append(tickers, tickers_Batteries)\n\nfor(i in tickers){\n    df &lt;- tq_get(i, get = \"stock.prices\", from = start, to = end)\n\n    df &lt;- stocks_cleaning(df)\n\n    df_list &lt;- append(df_list, list(df))\n}\n\n# Merge datasets\ndf_companies &lt;- merge_dataframes(df_list)\n\n# Save dataframe as a new file\nwrite.csv(df_companies, '../../data/01-modified-data/clean_lithium-companies.csv', row.names = FALSE)\n\n# Read cleaned file\ndf_companies &lt;- read.csv('../../data/01-modified-data/clean_lithium-companies.csv')\n\n# View data\nknitr::kable(tail(df_companies, 5))\n\n\n\n\n\n\ndate\nALB\nLTHM\nSGML\nPLL\nTSLA\nF\nLI\nON\nRIVN\nXPEV\nLVWR\nAEHR\nBYDDF\nX6752.T\n\n\n\n\n8394\n2022-12-26\n215.4762\n19.95778\n28.78222\n42.82167\n114.5617\n10.162896\n18.855\n61.48278\n18.15444\n9.811667\n4.672222\n20.30778\n24.36444\n1105.407\n\n\n8395\n2022-12-27\n212.9210\n19.94000\n29.00000\n42.39000\n109.1000\n10.135747\n18.540\n61.36000\n17.74000\n9.800000\n4.760000\n20.22000\n24.28041\n1097.588\n\n\n8396\n2022-12-28\n212.8017\n19.52000\n27.42000\n41.64000\n112.7100\n9.909502\n18.430\n60.28000\n17.74000\n9.390000\n4.200000\n19.90000\n24.23066\n1096.610\n\n\n8397\n2022-12-29\n216.2119\n19.80000\n27.49000\n43.61000\n121.8200\n10.443438\n19.490\n62.71000\n18.73000\n9.880000\n4.650000\n20.63000\n24.67845\n1094.656\n\n\n8398\n2022-12-30\n215.6054\n19.87000\n28.22000\n44.02000\n123.1800\n10.524886\n20.400\n62.37000\n18.43000\n9.940000\n4.850000\n20.10000\n24.44958\n1085.371\n\n\n\n\n\n\n\nDataset - Github\n\nThe data cleaning process involved obtaining financial data from Yahoo Finance for prominent lithium-related companies. Income statements and balance sheets for these companies were downloaded, compiled into an Excel spreadsheet, and subjected to several cleaning steps. These steps included identifying sheet names, importing and merging data frames, and renaming columns. In addition, the data was transformed into a wide format for better visualization. The resulting cleaned dataset contains 382 rows and 82 columns, ready for further analysis and insight.\nThe link to the cleaned dataset can be found below the table.\n\n\n\nData Cleaning - Financial Data\n# Read csv file\ndf_companies2 &lt;- read_excel('../../data/00-raw-data/Lithium_Companies.xlsx', sheet = \"Sheet1\")\n\n# Remove NAs\ndf_companies2 &lt;- na.omit(df_companies2)\n\n# Read xlsx file\nfile &lt;- '../../data/00-raw-data/Lithium_Companies.xlsx'\n\nsheet_names &lt;- excel_sheets(file)\n\nsheet_names &lt;- sheet_names[2:length(sheet_names)]\n\ncompanies_list &lt;- list()\n\n# Print the list of sheet names\nfor(i in (2:length(sheet_names))){\n  name &lt;- unlist(strsplit(sheet_names[i], \" \"))\n  companies_list &lt;- append(companies_list, name[1])\n}\n\n# Function to read and merge data for a company\nread_and_merge &lt;- function(company_name) {\n  # Read Income Statement and Balance Sheet sheets\n  income_sheet &lt;- read_excel(file, sheet = paste0(company_name, \" Income Statement\"))\n  balance_sheet &lt;- read_excel(file, sheet = paste0(company_name, \" Balance Sheet\"))\n\n  income_sheet$'Quarter Ended' &lt;- as.Date(income_sheet$'Quarter Ended')\n  balance_sheet$'Quarter Ended' &lt;- as.Date(balance_sheet$'Quarter Ended')\n\n  # Merge by \"Quarter Ended\"\n  merged_data &lt;- merge(income_sheet, balance_sheet, by = \"Quarter Ended\")\n\n  merged_data &lt;- merged_data %&gt;% mutate(Company = company_name)\n\n  return(merged_data)\n\n}\n\n# Initialize an empty list to store dataframes\nmerged_data_list &lt;- list()\n\n# Loop through each company, read and merge data, and store in the list\nfor (i in companies_list) {\n  merged_data &lt;- read_and_merge(i)\n  merged_data_list[[i]] &lt;- merged_data\n}\n\ndf &lt;- as.data.frame(matrix(NA, 1, 4))\n\nnames(df) &lt;- c('Quarter Ended', 'Company', 'KPI', 'Value')\n\nfor(i in merged_data_list){\n  temp &lt;- gather(i, key = \"KPI\", value = \"Value\", c(-'Quarter Ended',-Company))\n\n  df &lt;- union(df, temp)\n}\n\ndf$'Quarter Ended' &lt;- as.Date(df$'Quarter Ended')\n\ndf$Year &lt;- year(df$'Quarter Ended')\n\ndf$Quarter &lt;- quarter(df$'Quarter Ended')\n\ndf$Value &lt;- as.numeric(sub(\"-\", \"0\", df$Value))\n\ndf &lt;- df %&gt;%\n  group_by(Company, KPI, Year, Quarter) %&gt;%\n  summarise(Value = sum(Value, na.rm = TRUE))\n\n# Save dataframe as a new file\nwrite.csv(df, '../../data/01-modified-data/clean_companies_finance.csv', row.names = FALSE)\n\n# Read cleaned file\ndf_companies_2 &lt;- read.csv('../../data/01-modified-data/clean_companies_finance.csv')\n\n# Pivot the data from long to wide format\ndf_companies_2 &lt;- df_companies_2 %&gt;%\n  pivot_wider(\n    id_cols = c(Company, Year, Quarter),\n    names_from = KPI,\n    values_from = Value\n  )\n\n# View data\nknitr::kable(head(df_companies_2, 5))\n\n\n\n\n\nCompany\nYear\nQuarter\nAccounts Payable\nBook Value Per Share\nCash & Cash Equivalents\nCash & Equivalents\nCash Growth\nCommon Stock\nComprehensive Income\nCost of Revenue\nCurrent Debt\nDebt Growth\nDeferred Revenue\nDepreciation & Amortization\nEBIT\nEBIT Margin\nEBITDA\nEBITDA Margin\nEPS (Basic)\nEPS (Diluted)\nEPS Growth\nEffective Tax Rate\nFree Cash Flow\nFree Cash Flow Margin\nFree Cash Flow Per Share\nGross Margin\nGross Profit\nIncome Tax\nInterest Expense / Income\nInventory\nLong-Term Debt\nNet Cash / Debt\nNet Cash / Debt Growth\nNet Cash Per Share\nNet Income\nNet Income Growth\nOperating Expenses\nOperating Income\nOperating Margin\nOther Current Assets\nOther Current Liabilities\nOther Expense / Income\nOther Long-Term Assets\nOther Long-Term Liabilities\nOther Operating Expenses\nPretax Income\nProfit Margin\nProperty, Plant & Equipment\nReceivables\nResearch & Development\nRetained Earnings\nRevenue\nRevenue Growth (YoY)\nSelling, General & Admin\nShareholders' Equity\nShares Change\nShares Outstanding (Basic)\nShares Outstanding (Diluted)\nShort-Term Investments\nTotal Assets\nTotal Current Assets\nTotal Current Liabilities\nTotal Debt\nTotal Liabilities\nTotal Long-Term Assets\nTotal Long-Term Liabilities\nWorking Capital\nDividend Growth\nDividend Per Share\nGoodwill and Intangibles\nLong-Term Investments\nGeneral & Administrative\nGoodwill\nIntangible Assets\nInterest Expense\nInterest Income\nSelling & Marketing\nTotal Liabilities and Equity\nNet Income Common\nPreferred Dividends\nNA\n\n\n\n\nAEHR\n2013\n4\n1892\n0.54\n2494\n2494\n1.0923\n51499\n2488\n2456\n476\n0.7175\n2058\n42\n149\n0.0301\n191\n0.0386\n0.01\n0.01\n0\n0.0144\n217\n0.0438\n0.02\n0.5038\n2494\n2\n10\n5650\n0\n2018\n0.0000\n0.17\n137\n0\n2315\n179\n0.0362\n420\n1225\n30\n78\n152\n0\n139\n0.0277\n249\n2735\n798\n48143\n4950\n0.0206\n1517\n5844\n0.2730\n10806\n11839\n0\n11626\n11299\n5651\n476\n5803\n327\n152\n5648\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAEHR\n2014\n1\n2265\n0.58\n1846\n1846\n0.3367\n51813\n2502\n2742\n406\n0.6658\n1353\n24\n241\n0.0429\n265\n0.0472\n0.02\n0.02\n0\n0.0940\n656\n0.1169\n0.06\n0.5114\n2870\n22\n7\n5931\n0\n1440\n7.6747\n0.12\n212\n0\n2608\n262\n0.0467\n399\n1439\n21\n78\n117\n0\n234\n0.0378\n311\n3378\n907\n47931\n5612\n0.6802\n1701\n6384\n0.3112\n10982\n12277\n0\n11943\n11554\n5463\n406\n5580\n389\n117\n6091\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAEHR\n2014\n2\n1892\n0.64\n1809\n1809\n0.2216\n52254\n2488\n2456\n777\n0.2943\n1058\n22\n234\n0.0436\n256\n0.0477\n0.02\n0.02\n0\n0.0437\n623\n0.1160\n0.06\n0.5426\n2914\n10\n5\n6148\n0\n1032\n0.1562\n0.08\n239\n0\n2701\n213\n0.0397\n326\n1390\n21\n78\n79\n0\n229\n0.0445\n474\n3390\n1016\n47692\n5370\n0.6462\n1685\n7050\n0.2352\n11087\n12807\n0\n12225\n11673\n5117\n777\n5196\n552\n79\n6556\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAEHR\n2014\n3\n1586\n0.58\n2075\n2075\n0.1478\n52793\n2451\n1948\n1000\n0.0560\n761\n29\n942\n0.2648\n913\n0.2566\n0.08\n0.08\n0\n0.0000\n239\n0.0672\n0.02\n0.4525\n1610\n49\n14\n6304\n0\n1075\n0.2776\n0.09\n907\n0\n2583\n973\n0.2735\n351\n1435\n31\n95\n8\n0\n956\n0.2549\n492\n2098\n959\n48599\n3558\n0.0517\n1624\n6645\n0.0711\n11391\n11391\n0\n11415\n10828\n4782\n1000\n4790\n587\n8\n6046\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAEHR\n2014\n4\n1790\n0.64\n3596\n3596\n0.4419\n55852\n2373\n1921\n642\n0.3487\n792\n32\n2087\n0.7981\n2055\n0.7859\n0.18\n0.18\n0\n0.0000\n709\n0.2711\n0.06\n0.2654\n694\n19\n8\n6035\n0\n2954\n0.4638\n0.25\n2114\n0\n2841\n2147\n0.8210\n359\n1248\n60\n94\n8\n0\n2095\n0.8084\n538\n1352\n1105\n50713\n2615\n0.4717\n1736\n7512\n0.0097\n11724\n11724\n0\n11974\n11342\n4472\n642\n4480\n632\n8\n6870\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nDataset - Github"
  },
  {
    "objectID": "3_data_cleaning.html#resources-prices",
    "href": "3_data_cleaning.html#resources-prices",
    "title": "Data Cleaning",
    "section": "Resources Prices",
    "text": "Resources Prices\n\nTo clean the resource price data, we imported an Excel file from the IMF. The steps were straightforward and included filtering out NA values, formatting the date column, renaming columns, and filtering for relevant variables. The resulting cleaned dataset of 127 rows and 7 columns is now ready for further analysis and comparison, providing a refined and accurate representation of commodity price information.\nThe access link to the cleaned dataset can be found below the table.\n\n\n\nData Cleaning\ndf_commodity_price &lt;- read_excel(\"../../data/00-raw-data/commodity_price.xlsx\")\n\ndf_commodity_price &lt;- df_commodity_price %&gt;%\n  pivot_longer(cols = -c('...1'), \n               names_to = \"Month_Year\",\n               values_to = \"Price\")\n\ndf_commodity_price &lt;- df_commodity_price %&gt;% filter(!is.na(Price) & Price != \"\")\n\ndf_commodity_price$Month_Year &lt;- as.yearmon(df_commodity_price$Month_Year, format = \"%b %Y\")\n\ndf_commodity_price$Month_Year &lt;- format(df_commodity_price$Month_Year, \"%m-%Y\")\n\ndf_commodity_price$Month_Year &lt;- paste(\"01-\", df_commodity_price$Month_Year, sep = \"\")\n\ndf_commodity_price$Month_Year &lt;- as.Date(df_commodity_price$Month_Year, format = \"%d-%m-%Y\")\n\nnames(df_commodity_price) &lt;- c('Commodity', 'DATE', 'Price')\n\ndf_commodity_price &lt;- df_commodity_price %&gt;% filter(Commodity %in% c(\"Lithium\", \"Aluminum\", \"Cobalt\", \"Copper\", \"Nickel\", \"Zinc\"))\n\ndf_commodity_price &lt;- pivot_wider(df_commodity_price, id_cols = DATE, names_from = Commodity, values_from = Price)\n\n#df_commodity_price &lt;- df_commodity_price %&gt;% select(Commodity, DATE, Price)\n\n#names(df_commodity_price) &lt;- c('DATE', 'Price')\n\ndf_commodity_price &lt;- na.omit(df_commodity_price)\n\n# Save dataframe as a new file\nwrite.csv(df_commodity_price, '../../data/01-modified-data/clean_resources-price.csv', row.names = FALSE)\n\n# Save dataframe as a new file\ndf_resources &lt;- read.csv('../../data/01-modified-data/clean_resources-price.csv')\n\nknitr::kable(head(df_resources, 5))\n\n\n\n\n\nDATE\nAluminum\nCobalt\nCopper\nNickel\nZinc\nLithium\n\n\n\n\n2012-06-01\n1885.513\n28831.91\n7428.289\n16603.68\n1855.934\n63643.22\n\n\n2012-07-01\n1876.250\n28394.08\n7584.261\n16128.41\n1847.750\n63568.83\n\n\n2012-08-01\n1843.327\n29050.36\n7510.432\n15703.99\n1816.318\n63682.98\n\n\n2012-09-01\n2064.120\n29221.25\n8087.743\n17287.96\n2009.850\n64069.57\n\n\n2012-10-01\n1974.304\n26896.74\n8062.033\n17168.74\n1903.959\n64650.80\n\n\n\n\n\n\n\nDataset - Github"
  },
  {
    "objectID": "3_data_cleaning.html#lithium-news-sentiment-analysis",
    "href": "3_data_cleaning.html#lithium-news-sentiment-analysis",
    "title": "Data Cleaning",
    "section": "Lithium News Sentiment Analysis",
    "text": "Lithium News Sentiment Analysis\n\nIn the data cleaning process for the news article text data, we carefully edited our dataset. After extracting available sources from the News API, we applied filters to retain articles that were in English, had the U.S. as a country, and fell into the general, business, technology, and science categories. We then used the News API to collect articles containing the terms “lithium,” “lithium batteries,” and “electric vehicles”. The resulting dataframe included key details such as name, author, title, description, URL, content, and date.\nUsing the IBM Watson API, we performed sentiment analysis, which required an IBM developer key. The output provided valuable information such as source, date, score, label, and content. We merged this sentiment-analyzed data with the original source dataframe, adding the name and category of each source. The final dataset was then saved to a CSV file, setting the stage for comprehensive future analysis.\nThe access link to the cleaned dataset can be found below the table.\n\n\n\nData Cleaning - News Sources\ndate = date.today()\ndate_past = date - timedelta(days=20)\n\nf = open('./auth.k','r', encoding=\"utf-8\")\nak = f.readlines()\nf.close()\n\nnewsapi = NewsApiClient(api_key=ak[0])\n\nsources = newsapi.get_sources()\n\nsources = pd.DataFrame(sources['sources'])\n\nsources = sources[(sources['language'] == 'en') & (sources['country'] == 'us') & ~sources['category'].isin(['sports', 'entertainment', 'health'])]\n\n\n\n\nData Cleaning - News Articles\ndf_sources = ', '.join(sources['id'].astype(str))\n\ndf_domains = ', '.join(sources['url'].astype(str))\n\nall_articles_1 = newsapi.get_everything(q='lithium',\n                                      sources=str(df_sources),\n                                      domains=str(df_domains),\n                                      from_param=date_past,\n                                      to=date,\n                                      language='en',\n                                      sort_by='relevancy')\n\nall_articles_2 = newsapi.get_everything(q='\"lithium batteries\"',\n                                      sources=str(df_sources),\n                                      domains=str(df_domains),\n                                      from_param=date_past,\n                                      to=date,\n                                      language='en',\n                                      sort_by='relevancy')\n\nall_articles_3 = newsapi.get_everything(q='\"electric vehicles\"',\n                                      sources=str(df_sources),\n                                      domains=str(df_domains),\n                                      from_param=date_past,\n                                      to=date,\n                                      language='en',\n                                      sort_by='relevancy')\n\ndf_articles_1 = pd.DataFrame(all_articles_1['articles'])\n\ndf_articles_2 = pd.DataFrame(all_articles_2['articles'])\n\ndf_articles_3 = pd.DataFrame(all_articles_3['articles'])\n\n\ndf_articles = pd.concat([df_articles_1, df_articles_2, df_articles_3], ignore_index=True)\n\ndf_articles[['id', 'name']] = df_articles['source'].apply(lambda x: pd.Series([x['id'], x['name']]))\n\ndf_articles_save = pd.DataFrame(df_articles, columns=[\n                                                'id', \n                                                'name', \n                                                'author', \n                                                'title',\n                                                'description',\n                                                'url',\n                                                'urlToImage', \n                                                'content', \n                                                'publishedAt'])\n\n# We have previously save the results\n# df_articles_save.to_csv('../../data/01-modified-data/clean_articles.csv', index=False)\n\n\n\n\nData Cleaning - News Articles Sentiment Analysis\ndf_content = pd.DataFrame(df_articles, columns=['source', 'content', 'publishedAt'])\ndf_content['source'] = df_content['source'].apply(lambda x: x['id'])\n\nf = open('./auth2.k','r', encoding=\"utf-8\")\nak2 = f.readlines()\nf.close()\n\nak2 = ak2[0]\n\nauthenticator = IAMAuthenticator(apikey = ak2)\nnatural_language_understanding = NaturalLanguageUnderstandingV1(\n    version='2020-08-01',\n    authenticator=authenticator\n)\n\nnatural_language_understanding.set_service_url('https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/8b0909d1-3768-4c54-b80d-b9817610e36d')\n\n#IBM Watson\ni = 0\nibm_source = []\nibm_date = []\nibm_score = []\nibm_label = []\nibm_content = []\n\nfor index, row in df_content.iterrows():\n    response = natural_language_understanding.analyze(\n    text=row['content'], language = 'en',\n    features=Features(sentiment=SentimentOptions())).get_result()\n\n    s = row['source']\n    ibm_source.append(s)\n    d = row['publishedAt']\n    ibm_date.append(d)\n    c = row['content']\n    ibm_content.append(c)\n    x = response['sentiment']['document']['score']\n    x = round(x, 4)\n    ibm_score.append(x)\n    y = response['sentiment']['document']['label']\n    ibm_label.append(y)\n    # print(response['sentiment']['document']['score'])\n    # print(response['sentiment']['document']['label'])\n    # print(json.dumps(response, indent=2))\n\n    i=i+1   \n\nresults = {\"id\": ibm_source, \"ibm_date\": ibm_date, \"ibm_score\": ibm_score, \"ibm_label\": ibm_label, \"ibm_content\": ibm_content}\n\nresults = pd.DataFrame(results)\n\ndate = date.today()\n\nname = '../' + str(date) + '_results.csv'\n\nresults.to_csv(name, index=False)\n\nresults = results.merge(sources, how='left')\n\nresults = pd.DataFrame(results, columns=['name', 'category', 'ibm_score', 'ibm_label', 'ibm_date', 'ibm_content'])\n\nresults = results.rename(columns={'ibm_date': 'date'})\n\nresults['date'] = pd.to_datetime(results['date'])\n\nresults['date'] = results['date'].dt.date\n\n# We have previously save the results\n# results.to_csv('../../data/01-modified-data/clean_sentiment_analysis.csv', index=False)\n\n\n\n\n\n\n\nname\ncategory\nibm_score\nibm_label\ndate\nibm_content\n\n\n\n\nEngadget\ntechnology\n0.9650\npositive\n2023-12-01\nIf you recently moved into a new place or are just looking to update your home's security, now's a good time to do so. Though Black Friday has come and gone, Blink's video doorbell and two fourth-gen… [+1310 chars]\n\n\nThe Verge\ntechnology\n0.0000\nneutral\n2023-11-30\nTesla Cybertruck will usher in a new Powershare bidirectional charging feature Tesla Cybertruck will usher in a new Powershare bidirectional charging feature / The EV maker finally jumps on the ve… [+2497 chars]\n\n\nWired\ntechnology\n0.7650\npositive\n2023-11-29\nThe robotic line cooks were deep in their recipe, toiling away in a room tightly packed with equipment. In one corner, an articulated arm selected and mixed ingredients, while another slid back and f… [+3678 chars]\n\n\nThe Next Web\ntechnology\n0.5425\npositive\n2023-12-04\nRenewable energies like wind and solar are clean, abundant, and cheap but notoriously unpredictable. Thats why so much time and money has been pumped into scaling energy storage solutions: we need to… [+4764 chars]\n\n\nABC News\ngeneral\n-0.8755\nnegative\n2023-12-04\nNEW YORK -- One person was killed and six others were injured when a fire blamed on an electric bicycle battery tore through a New York City apartment, officials said Monday. The fire started at aro… [+884 chars]\n\n\n\n\n\n\n\nDataset - Github"
  },
  {
    "objectID": "3_data_cleaning.html#electric-vehicles",
    "href": "3_data_cleaning.html#electric-vehicles",
    "title": "Data Cleaning",
    "section": "Electric Vehicles",
    "text": "Electric Vehicles\n\nIn the data cleaning process for the electric vehicle dataset, we imported the CSV file and performed simple steps. This included renaming columns, creating a new column to store the brand name, and selecting relevant columns. To enhance the dataset, we created a new dataframe that associated each brand with its continent. We then merged the datasets, creating a refined dataset that encapsulates relevant information for future analysis. The cleaned dataset contains 360 rows and 10 columns.\nThe access link to the cleaned dataset can be found below the table.\n\n\n\nData Cleaning\n# Read csv file\ndf_vehicles &lt;- read.csv(\"../../data/00-raw-data/EV_cars.csv\")\n\n# Rename columns\nnames(df_vehicles) &lt;- c('Battery', 'CarName', 'Car_name_link', 'Efficiency', 'FastCharge', 'Price', 'Range', 'TopSpeed', 'Acceleration')\n\n# Extract Car Brand\ndf_vehicles$Brand &lt;- str_extract(df_vehicles$CarName, \"^[^\\\\s]+\")\n\n# Select relevant columns\ndf_vehicles &lt;- df_vehicles %&gt;% dplyr::select('CarName', 'Brand', 'Battery', 'Efficiency', 'FastCharge', 'Price', 'Range', 'TopSpeed', 'Acceleration')\n\ndf_brands &lt;- data.frame(\n  Brand = c(\"Tesla\", \"BYD\", \"MG\", \"BMW\", \"Volvo\", \"Citroen\", \"Renault\", \"Hyundai\", \"Kia\", \"Rolls-Royce\",\n            \"Hongqi\", \"Fiat\", \"CUPRA\", \"Dacia\", \"Opel\", \"Audi\", \"Toyota\", \"Smart\", \"Volkswagen\", \"Peugeot\",\n            \"Skoda\", \"Mini\", \"Jeep\", \"Nissan\", \"Mercedes\", \"Zeekr\", \"Polestar\", \"Lucid\", \"Honda\", \"Lotus\",\n            \"Subaru\", \"Fisker\", \"Mazda\", \"Maxus\", \"Lexus\", \"Ford\", \"XPENG\", \"ORA\", \"NIO\", \"VinFast\",\n            \"Jaguar\", \"SsangYong\", \"Genesis\", \"Aiways\", \"Porsche\", \"Maserati\", \"DS\", \"Seres\", \"e.Go\", \"Elaris\", \"Abarth\"),\n  Continent = c(\"North America\", \"Asia\", \"Europe\", \"Europe\", \"Europe\", \"Europe\", \"Europe\", \"Asia\", \"Asia\", \"Europe\",\n                \"Asia\", \"Europe\", \"Europe\", \"Europe\", \"Europe\", \"Europe\", \"Asia\", \"Europe\", \"Europe\", \"Europe\",\n                \"Europe\", \"Europe\", \"Europe\", \"Asia\", \"Europe\", \"Asia\", \"North America\", \"Europe\", \"North America\", \"Asia\", \"Europe\",\n                \"North America\", \"North America\", \"Asia\", \"Europe\", \"Asia\", \"Asia\", \"Asia\", \"North America\", \"Europe\", \"Asia\",\n                \"North America\", \"North America\", \"Europe\", \"Asia\", \"North America\", \"Europe\", \"Europe\", \"Europe\", \"Europe\", \"Europe\")\n)\n\ndf_vehicles &lt;- merge(df_vehicles, df_brands)\n\n# Save dataframe as a new file\nwrite.csv(df_vehicles, '../../data/01-modified-data/clean_vehicles.csv', row.names = FALSE)\n\n# Save dataframe as a new file\ndf_vehicles &lt;- read.csv('../../data/01-modified-data/clean_vehicles.csv')\n\nknitr::kable(head(df_vehicles, 5))\n\n\n\n\n\nBrand\nCarName\nBattery\nEfficiency\nFastCharge\nPrice\nRange\nTopSpeed\nAcceleration\nContinent\n\n\n\n\nAbarth\nAbarth 500e Hatchback\n37.8\n168\n370\n37990\n225\n155\n7.0\nEurope\n\n\nAbarth\nAbarth 500e Convertible\n37.8\n168\n370\n40990\n225\n155\n7.0\nEurope\n\n\nAiways\nAiways U6\n60.0\n171\n430\n47588\n350\n160\n7.0\nEurope\n\n\nAiways\nAiways U5\n60.0\n190\n380\n39563\n315\n150\n7.5\nEurope\n\n\nAudi\nAudi Q8 e-tron Sportback 50 quattro\n89.0\n200\n690\n76650\n445\n200\n6.0\nEurope\n\n\n\n\n\n\n\nDataset - Github"
  },
  {
    "objectID": "2_data_gathering.html#lithium-companies",
    "href": "2_data_gathering.html#lithium-companies",
    "title": "Data Gathering",
    "section": "Lithium Companies",
    "text": "Lithium Companies\n\n\n\nLithium-related companies play a key role in the performance of the industry. Using Yahoo Finance and other financial sources, we have compiled a dataset of market and financial information for the key companies in the industry. For this analysis, we focus on stock market data, income statements and balance sheets for the most recent dates available.\nThis analysis will focus on 3 main production models; lithium production, electric vehicle production and lithium-ion batteries. We will look at Albemarle Corporation, Livent Corporation, Piedmont Lithium Inc. and Sigma Lithium Corporation. These companies are four of the most important companies in the world within the lithium production model. The first three companies were founded in the USA, while the last one was founded in Canada. Other major companies are located in Australia, Chile and China. The second model is electric vehicle production, and within this market we have well-known companies and some new ones. The six companies from the US that will be considered are Tesla, Inc, Ford Motor Company, ON Semiconductor Corp, Rivian Automotive, Inc, LiveWire Group Inc and AEHR Test Systems. The emerging market in this production model is China and in this case we will consider Li Auto Inc. and XPeng Inc. The third manufacturing companies we will analyze are BYD CO., LTD - China and Panasonic Holdings Corporation - Japan. Both companies are located in Asia and their main activity is lithium-ion batteries.\nSource: Yahoo Finance"
  },
  {
    "objectID": "2_data_gathering.html#resources-prices",
    "href": "2_data_gathering.html#resources-prices",
    "title": "Data Gathering",
    "section": "Resources Prices",
    "text": "Resources Prices\n\n\n\nThe International Monetary Fund is a useful resource with a variety of economic and market datasets, and for this project we will use it to collect data on the key resources needed to produce lithium-ion batteries. These include aluminum, cobalt, copper, nickel, zinc, and lithium. The emergence of mining operations for the different resources is a key aspect that drives the prices of each resource. By analyzing this data, we can be able to identify patterns and trends, correlations between resources, and compare how demand for each resource is driving prices.\nThe composition of the lithium-ion battery depends on the manufacturer. In general terms, the lithium cells consist of a 5-20% of cobalt, 5-10% nickel, 15% aluminum, 10% copper, 5-10% lithium, 15% organic chemicals, and 7% of plastics. The composition also depends on the end use of the battery as it affects the performance and life of the battery.\nSource: Primary Commodity Price System"
  },
  {
    "objectID": "2_data_gathering.html#electric-vehicles",
    "href": "2_data_gathering.html#electric-vehicles",
    "title": "Data Gathering",
    "section": "Electric Vehicles",
    "text": "Electric Vehicles\n\n\n\nAs the popularity of electric vehicles continues to grow, understanding their specifications becomes critical. Using a Kaggle dataset, we focus on key quantitative attributes that are essential for a comprehensive understanding of electric vehicle characteristics. The dataset includes details such as the vehicle make and model listed in car name. The most important quantitative characteristics are the capacity of the vehicle’s battery, the energy efficiency rating, the fast-charging capability in minutes, the price of the vehicle in Germany, the driving range on a single charge, the maximum speed per hour, and the acceleration time from 0 to 100 kilometers per hour. The source of the information is listed in the car name link. The ability to analyze and compare these attributes enables both consumers and industry stakeholders to make informed decisions. This dataset facilitates research to understand which are the most influential factors and if there is a relationship between them.\nSource: Electric Vehicle Specifications and Prices"
  },
  {
    "objectID": "1_introduction.html#goal",
    "href": "1_introduction.html#goal",
    "title": "Introduction",
    "section": "Goal",
    "text": "Goal\n\nGiven the growing industry and appeal of this resource, it is important to understand how the industry has evolved over the past few years from a variety of perspectives. This includes evaluating the major producers, key companies in the market, resource performance, and news.\nThe goal of this project is to analyze the most recent data collected from various aspects of the industry and find insights and future points of improvement. This analysis is necessary to conduct a study of environmental impacts and actions that can improve the quality of life from various perspectives."
  },
  {
    "objectID": "1_introduction.html#questions",
    "href": "1_introduction.html#questions",
    "title": "Introduction",
    "section": "Questions:",
    "text": "Questions:\n\nHow has the distribution of the total global lithium production among the main lithium producing countries evolved over the past two decades?\nHow have the main lithium related companies performed in the stocks market over the past two decades and how do they compare between each other?\nHow does the lithium price correlate with the complementary resources used to produce lithium batteries?\nCan we use machine learning to predict sentiment analysis in text data and recognize categories?\nDo lithium related companies have a similar financial performance and is there a clear relationship between them? How did COVID affect these companies and relationships?\nDoes the financial data of the main lithium related companies differentiate enough to distinct them?\nIs there a pattern in the electrical vehicles characteristics in order to predict where they were manufactured?\nHow has public sentiment and opinion evolved regarding the increasing demand for lithium, and can text analysis reveal whether there is a pattern of sentiment by source of data?\nAre there specific terminologies that drive the sentiment of publications?\nWhat conclusions can we draw based on the industry analysis and are there specific concerns arising?\n\n\n\nReferences:\n\n\nDorn, Felix M, and Fernando Ruiz Peyré. 2020. “Lithium as a Strategic Resource: Geopolitics, Industrialization, and Mining in Argentina.” Journal of Latin American Geography 19 (4): 68–90.\n\n\nPetavratzi, E, D Sanchez-Lopez, A Hughes, J Stacey, J Ford, and A Butcher. 2022. “The Impacts of Environmental, Social and Governance (ESG) Issues in Achieving Sustainable Lithium Supply in the Lithium Triangle.” Mineral Economics 35 (3-4): 673–99.\n\n\nStevens, Pippa. n.d. “Inside the Only Lithium Producer in the u.s., Which Provides the Critical Mineral Used in Batteries by Tesla, EV Makers.” Accessed 2022. https://www.cnbc.com/2022/10/14/lithium-for-tesla-evs-batteries-touring-silver-peak-nevada-.html."
  },
  {
    "objectID": "4_data_exploration.html#global-lithium-production",
    "href": "4_data_exploration.html#global-lithium-production",
    "title": "Data Exploration",
    "section": "Global Lithium Production",
    "text": "Global Lithium Production\n\nIn the first visualization we can observe a map plot done using Tableau Software. Using the controls to change the years of visualization we can observe the evolution of the global lithium production and how the total production is distributed along the main producing countries. As it was explained earlier, the data includes information since 1995 as this resource is very innovative.\nThe global lithium production over the past two decades reveals a notable evolution in the distribution among the major producing countries. In 1995, Australia, the United States, and Chile were prominent lithium producers, each with a modest contribution of less than 3,500 tons. Fast forward to 2015, Australia jumped to about 12,000 tons, Chile reached nearly 10,000 tons, and Argentina contributed around 3,600 tons. The most recent data from 2022 shows a significant increase in lithium production, with Australia leasing with 61,000 tons, Chile following closely with about 38,000 tons, and China producing 19000 tons. This analysis underscores the remarkable growth in the lithium industry, indicating a persistent upward trend in production over the years."
  },
  {
    "objectID": "4_data_exploration.html#lithium-companies",
    "href": "4_data_exploration.html#lithium-companies",
    "title": "Data Exploration",
    "section": "Lithium Companies",
    "text": "Lithium Companies\n\nIn examining the stock market performance of lithium producing companies over the past two decades, we focus on time series data. Albemarle Corporation (ALB) stands out as a market participant that has shown consistent growth since 2000. ALB’s stock price has exceeded $37 per share since 2015, with a significant peak of over $300 in 2022, followed by post-peak fluctuations around $200. In particular, the pandemic-related slowdown in 2020 temporarily impacted ALB’s value. In contrast, Lithium Americas Corp (LTHM) and Sigma Lithium Corp (SGML), which were introduced in 2018, show increasing values, approaching $30 per share. The comparison highlights ALB’s significant market presence and robust performance, with a marked difference in valuation trends compared to its newer competitors.\n\n\n\nVisualization\n# Read cleaned file\ndf_companies &lt;- read.csv('../../data/01-modified-data/clean_lithium-companies.csv')\n\ndf_companies_1 &lt;- df_companies %&gt;% dplyr::select('date', 'ALB', 'LTHM', 'SGML')\n\ndf_companies_1 &lt;- gather(df_companies_1, key = \"stock\", value = \"price\", -date)\n\n# Change data type\ndf_companies_1$date &lt;- as.Date(df_companies_1$date)\n\n# Create ggplot line plot\nviz_companies_1 &lt;- ggplot(df_companies_1, aes(x = date, y = price, color = stock)) +\n  geom_line() +\n  labs(title = \"Stock Prices - Lithium Production Companies\",\n       x = \"Date\",\n       y = \"Stock Price\") +\n  scale_color_discrete(name = \"Stock\")\n\n# Show plot\nviz_companies_1 %&gt;% ggplotly()\n\n\n\n\n\n\n\n\nWe look at the stock market performance of the major electric vehicle manufacturers over the past two decades. In the early 2000s, AEHR, ON, and F were the major players. Around 2010, Tesla came on the scene and changed the landscape. It wasn’t until around 2019 that other companies emerged. Notably, in 2020, Tesla experienced a significant peak in the stock market, with prices spiking above $200 and reaching a remarkable $400. This spike and subsequent volatility positioned Tesla as a market leader relative to its competitors, demonstrating its dominance in the electric vehicle manufacturing sector.\n\n\n\nVisualization\ndf_companies_2 &lt;- df_companies %&gt;% dplyr::select('date', 'TSLA', 'F', 'LI', 'ON', 'RIVN', 'XPEV', 'LVWR', 'AEHR')\n\ndf_companies_2 &lt;- gather(df_companies_2, key = \"stock\", value = \"price\", -date)\n\n# Change data type\ndf_companies_2$date &lt;- as.Date(df_companies_2$date)\n\n# Create ggplot line plot\nviz_companies_2 &lt;- ggplot(df_companies_2, aes(x = date, y = price, color = stock)) +\n  geom_line() +\n  labs(title = \"Stock Prices - Electric Vehicles Production Companies\",\n       x = \"Date\",\n       y = \"Stock Price\") +\n  scale_color_discrete(name = \"Stock\")\n\n# Show plot\nviz_companies_2 %&gt;% ggplotly()\n\n\n\n\n\n\n\n\nIn analyzing the stock market performance of lithium battery manufacturing companies over the past two decades, we focus on Panasonic Holdings Corporation of Japan and BYD CO., LTD of China. Panasonic has been around since 2000, with initial values near $2000 per share. However, they have experienced considerable volatility, dropping below $1000 per share and currently hovering around $1200 per share, indicating a challenging period for the company. In contrast, BYD CO., LTD entered the market in 2009 and has shown a remarkable upward trend, exceeding $20 per share. Despite this positive trajectory, BYD’s market presence remains comparatively smaller than Panasonic’s. This comparison highlights the contrasting fortunes of these two companies in the lithium battery manufacturing sector.\n\n\n\nVisualization\ndf_companies_3 &lt;- df_companies %&gt;% dplyr::select('date', 'BYDDF', 'X6752.T')\n\ndf_companies_3 &lt;- gather(df_companies_3, key = \"stock\", value = \"price\", -date)\n\n# Change data type\ndf_companies_3$date &lt;- as.Date(df_companies_3$date)\n\n# Create ggplot line plot\nviz_companies_3 &lt;- ggplot(df_companies_3, aes(x = date, y = price, color = stock)) +\n  geom_line() +\n  labs(title = \"Stock Prices - Lithium Batteries Production Companies\",\n       x = \"Date\",\n       y = \"Stock Price\") +\n  scale_color_discrete(name = \"Stock\")\n\n# Show plot\nviz_companies_3 %&gt;% ggplotly()"
  },
  {
    "objectID": "4_data_exploration.html#resources-prices",
    "href": "4_data_exploration.html#resources-prices",
    "title": "Data Exploration",
    "section": "Resources Prices",
    "text": "Resources Prices\n\nIn this analysis, we created a correlation plot heatmap using the Pearson correlation coefficient to visually represent the relationships between these resources. The results show that lithium has a high correlation with zinc, nickel and aluminum. However, the correlation with copper and cobalt is less pronounced. This observation provides valuable insight into the interdependencies among these critical resources and provides a basis for understanding their co-variation patterns in the context of lithium battery production.\n\n\n\nVisualization\n# Read csv file\ndf_resources &lt;- read.csv(\"../../data/01-modified-data/clean_resources-price.csv\")\n\n# Edit datatypes\ndf_resources$DATE &lt;- as.Date(df_resources$DATE)\n\n# Create correlation matrix\ncorrelation_matrix &lt;- cor(df_resources[, -1], use = \"complete.obs\")\n\ncorrelation_matrix &lt;- reshape2::melt(correlation_matrix)\n\nggplot(correlation_matrix, aes(x = Var1, y = Var2, fill = value, label = sprintf(\"%.2f\", value))) +\n  geom_tile(color = \"white\") +\n  geom_text(size = 3, color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", mid = \"yellow\", high = \"red\", midpoint = 0) +\n  theme_minimal() +\n  labs(title = \"Resources Correlation Plot\")"
  },
  {
    "objectID": "4_data_exploration.html#electric-vehicles",
    "href": "4_data_exploration.html#electric-vehicles",
    "title": "Data Exploration",
    "section": "Electric Vehicles",
    "text": "Electric Vehicles\n\nTo assess the relationship between the different characteristics of electric vehicles, we calculated the Pearson correlation coefficient. We analyze these values using a heat map. The results show that there are some variables that are positively correlated, inversely correlated and weakly correlated. The most relevant cases are the price which is highly correlated with the top speed and the battery. The battery is also highly correlated with range. Acceleration is inversely highly correlated with top speed, range, fast charge and acceleration.\n\n\n\nVisualization\n# Save dataframe as a new file\ndf_vehicles &lt;- read.csv('../../data/01-modified-data/clean_vehicles.csv')\n\ndf_vehicles &lt;- df_vehicles %&gt;% dplyr::select('Battery', 'Efficiency', 'FastCharge', 'Price', 'Range', 'TopSpeed', 'Acceleration')\n\n# Create correlation matrix\ncorrelation_matrix2 &lt;- cor(df_vehicles, use = \"complete.obs\")\n\ncorrelation_matrix2 &lt;- reshape2::melt(correlation_matrix2)\n\nggplot(correlation_matrix2, aes(x = Var1, y = Var2, fill = value, label = sprintf(\"%.2f\", value))) +\ngeom_tile(color = \"white\") +\ngeom_text(size = 3, color = \"white\") +\nscale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +\ntheme_minimal() +\nlabs(title = \"Electric Vehicles Characteristics Correlation Plot\")"
  },
  {
    "objectID": "4_data_exploration.html#lithium-news-sentiment-analysis",
    "href": "4_data_exploration.html#lithium-news-sentiment-analysis",
    "title": "Data Exploration",
    "section": "Lithium News Sentiment Analysis",
    "text": "Lithium News Sentiment Analysis\n\nUsing the sentiment analysis labels assigned to lithium-related news articles, word clouds were created for each sentiment category. The most recurring words across sentiments included “new,” “electric,” “vehicle,” “car,” and “batteries.” Notably, the consistency of the most common words across sentiments suggests a commonality in the language used, regardless of the sentiment assigned to the news article.\n\nWordCloud - Positive Sentiment\n\n\nVisualization\n# Save dataframe as a new file\ndf_sentiment = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\ndf_sentiment = df_sentiment[df_sentiment['ibm_label'] == \"positive\"]\n\ncomplete_text = \"\"\n\nfor i in df_sentiment['ibm_content']:\n    text=i\n    if not pd.isna(text):\n\n        text = clean(text,\n                fix_unicode=True,               # fix various unicode errors\n                to_ascii=True,                  # transliterate to closest ASCII representation\n                lower=True,                     # lowercase text\n                no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n                no_urls=True,                  # replace all URLs with a special token\n                no_emails=True,                # replace all email addresses with a special token\n                no_phone_numbers=True,         # replace all phone numbers with a special token\n                no_numbers=True,               # replace all numbers with a special token\n                no_digits=True,                # replace all digits with a special token\n                no_currency_symbols=True,      # replace all currency symbols with a special token\n                no_punct=False,                 # remove punctuations\n                replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n                replace_with_url=\"\",\n                replace_with_email=\"\",\n                replace_with_phone_number=\"\",\n                replace_with_number=\"\",\n                replace_with_digit=\"0\",\n                replace_with_currency_symbol=\"\",\n                lang=\"en\"                       # set to 'de' for German special handling\n            )\n        \n        text = text.replace('... [ chars]', ' ')\n        text = text.replace('&lt;ul&gt;', ' ')\n        text = text.replace('&lt;li&gt;', ' ')\n\n        complete_text = complete_text+text\n\ngenerate_word_cloud(complete_text, \"WordCloud - Positive Sentiment\\n\")\n\n\n\n\n\nWordCloud - Neutral Sentiment\n\n\nVisualization\n# Save dataframe as a new file\ndf_sentiment = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\ndf_sentiment = df_sentiment[df_sentiment['ibm_label'] == \"neutral\"]\n\ncomplete_text = \"\"\n\nfor i in df_sentiment['ibm_content']:\n    text=i\n    if not pd.isna(text):\n\n        text = clean(text,\n                fix_unicode=True,               # fix various unicode errors\n                to_ascii=True,                  # transliterate to closest ASCII representation\n                lower=True,                     # lowercase text\n                no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n                no_urls=True,                  # replace all URLs with a special token\n                no_emails=True,                # replace all email addresses with a special token\n                no_phone_numbers=True,         # replace all phone numbers with a special token\n                no_numbers=True,               # replace all numbers with a special token\n                no_digits=True,                # replace all digits with a special token\n                no_currency_symbols=True,      # replace all currency symbols with a special token\n                no_punct=False,                 # remove punctuations\n                replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n                replace_with_url=\"\",\n                replace_with_email=\"\",\n                replace_with_phone_number=\"\",\n                replace_with_number=\"\",\n                replace_with_digit=\"0\",\n                replace_with_currency_symbol=\"\",\n                lang=\"en\"                       # set to 'de' for German special handling\n            )\n        \n        text = text.replace('... [ chars]', ' ')\n        text = text.replace('&lt;ul&gt;', ' ')\n        text = text.replace('&lt;li&gt;', ' ')\n\n        complete_text = complete_text+text\n\ngenerate_word_cloud(complete_text, \"WordCloud - Neutral Sentiment\\n\")\n\n\n\n\n\nWordCloud - Negative Sentiment\n\n\nVisualization\n# Save dataframe as a new file\ndf_sentiment = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\ndf_sentiment = df_sentiment[df_sentiment['ibm_label'] == \"negative\"]\n\ncomplete_text = \"\"\n\nfor i in df_sentiment['ibm_content']:\n    text=i\n    if not pd.isna(text):\n\n        text = clean(text,\n                fix_unicode=True,               # fix various unicode errors\n                to_ascii=True,                  # transliterate to closest ASCII representation\n                lower=True,                     # lowercase text\n                no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n                no_urls=True,                  # replace all URLs with a special token\n                no_emails=True,                # replace all email addresses with a special token\n                no_phone_numbers=True,         # replace all phone numbers with a special token\n                no_numbers=True,               # replace all numbers with a special token\n                no_digits=True,                # replace all digits with a special token\n                no_currency_symbols=True,      # replace all currency symbols with a special token\n                no_punct=False,                 # remove punctuations\n                replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n                replace_with_url=\"\",\n                replace_with_email=\"\",\n                replace_with_phone_number=\"\",\n                replace_with_number=\"\",\n                replace_with_digit=\"0\",\n                replace_with_currency_symbol=\"\",\n                lang=\"en\"                       # set to 'de' for German special handling\n            )\n        \n        text = text.replace('... [ chars]', ' ')\n        text = text.replace('&lt;ul&gt;', ' ')\n        text = text.replace('&lt;li&gt;', ' ')\n\n        complete_text = complete_text+text\n\ngenerate_word_cloud(complete_text, \"WordCloud - Negative Sentiment\\n\")"
  },
  {
    "objectID": "5_2_NaiveBayes_Record.html",
    "href": "5_2_NaiveBayes_Record.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Python Libraries\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report\n\n\n\n\nNaive Bayes\n# Save dataframe as a new file\ndf_vehicles = pd.read_csv('../../data/01-modified-data/clean_vehicles.csv')\n\ndf_vehicles = df_vehicles.dropna()\n\nx = df_vehicles.columns[df_vehicles.dtypes != 'object']\nx = df_vehicles[x]\n\ny = df_vehicles['Continent']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nmodel = GaussianNB()\n\n\n\n\nResults\nmodel.fit(x_train, y_train)\n\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\nResults\ny_pred = model.predict(x_test)\n\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Test Set Accuracy: ', accuracy, \"\\n\", \"\\n\", classification_report(y_test, y_pred))\n\n\nTest Set Accuracy:  0.6290322580645161 \n \n                precision    recall  f1-score   support\n\n         Asia       0.60      0.35      0.44        17\n       Europe       0.76      0.72      0.74        39\nNorth America       0.33      0.83      0.48         6\n\n     accuracy                           0.63        62\n    macro avg       0.56      0.63      0.55        62\n weighted avg       0.67      0.63      0.63        62"
  },
  {
    "objectID": "5_NaiveBayes.html#naïve-bayes-nb-with-labeled-record-data",
    "href": "5_NaiveBayes.html#naïve-bayes-nb-with-labeled-record-data",
    "title": "Naive Bayes",
    "section": "Naïve Bayes (NB) with Labeled Record Data",
    "text": "Naïve Bayes (NB) with Labeled Record Data\n\nFor this section…\n\n\nData Selection\n\n\n{python}\n# Save dataframe as a new file\ndf_vehicles = pd.read_csv('../../data/01-modified-data/clean_vehicles.csv')\n\ndf_vehicles = df_vehicles.dropna()\n\nx = df_vehicles.columns[df_vehicles.dtypes != 'object']\nx = df_vehicles[x]\n\ny = df_vehicles['Continent']\n\n\n\n\nSplit Data\n\n\n{python}\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nmodel = GaussianNB()\n\n\n\n\nRun Model\n\n\nResults\nmodel.fit(x_train, y_train)\n\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\nResults\ny_train_pred = model.predict(x_train)\n\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\n\nprint('Train Set Accuracy: ', train_accuracy, \"\\n\", \"\\n\", classification_report(y_train, y_train_pred))\n\n\nTrain Set Accuracy:  0.5714285714285714 \n \n                precision    recall  f1-score   support\n\n         Asia       0.51      0.37      0.43        57\n       Europe       0.73      0.63      0.68       156\nNorth America       0.30      0.66      0.41        32\n\n     accuracy                           0.57       245\n    macro avg       0.51      0.55      0.51       245\n weighted avg       0.62      0.57      0.58       245\n\n\nResults\ny_pred = model.predict(x_test)\n\ntest_accuracy = accuracy_score(y_test, y_pred)\n\nprint('Test Set Accuracy: ', test_accuracy, \"\\n\", \"\\n\", classification_report(y_test, y_pred))\n\n\nTest Set Accuracy:  0.6290322580645161 \n \n                precision    recall  f1-score   support\n\n         Asia       0.60      0.35      0.44        17\n       Europe       0.76      0.72      0.74        39\nNorth America       0.33      0.83      0.48         6\n\n     accuracy                           0.63        62\n    macro avg       0.56      0.63      0.55        62\n weighted avg       0.67      0.63      0.63        62"
  },
  {
    "objectID": "5_NaiveBayes.html#naïve-bayes-nb-with-labeled-text-data",
    "href": "5_NaiveBayes.html#naïve-bayes-nb-with-labeled-text-data",
    "title": "Naive Bayes",
    "section": "Naïve Bayes (NB) with Labeled Text Data",
    "text": "Naïve Bayes (NB) with Labeled Text Data"
  },
  {
    "objectID": "5_NaiveBayes.html#labeled-record-data",
    "href": "5_NaiveBayes.html#labeled-record-data",
    "title": "Naive Bayes",
    "section": "Labeled Record Data",
    "text": "Labeled Record Data\n\nFor this section…\n\n\nData Selection\n\n\n{python}\n# Save dataframe as a new file\ndf_vehicles = pd.read_csv('../../data/01-modified-data/clean_vehicles.csv')\n\ndf_vehicles = df_vehicles.dropna()\n\nx = df_vehicles.columns[df_vehicles.dtypes != 'object']\nx = df_vehicles[x]\n\ny = df_vehicles['Continent']\n\n\n\n\nSplit Data\n\n\n{python}\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nmodel = GaussianNB()\n\nmodel.fit(x_train, y_train)\n\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\n\nRun Model\n\n\nResults\ny_pred = model.predict(x_test)\n\ntest_accuracy = accuracy_score(y_test, y_pred)\n\nprint('Test Set Accuracy: ', test_accuracy, \"\\n\", \"\\n\", classification_report(y_test, y_pred))\n\n\nTest Set Accuracy:  0.6290322580645161 \n \n                precision    recall  f1-score   support\n\n         Asia       0.60      0.35      0.44        17\n       Europe       0.76      0.72      0.74        39\nNorth America       0.33      0.83      0.48         6\n\n     accuracy                           0.63        62\n    macro avg       0.56      0.63      0.55        62\n weighted avg       0.67      0.63      0.63        62"
  },
  {
    "objectID": "5_NaiveBayes.html#labeled-text-data",
    "href": "5_NaiveBayes.html#labeled-text-data",
    "title": "Naive Bayes",
    "section": "Labeled Text Data",
    "text": "Labeled Text Data"
  },
  {
    "objectID": "5_NaiveBayes.html#electric-vehicles-characteristics",
    "href": "5_NaiveBayes.html#electric-vehicles-characteristics",
    "title": "Naive Bayes",
    "section": "Electric Vehicles Characteristics",
    "text": "Electric Vehicles Characteristics\n\nFor this section…\n\n\nData Selection\n\n\n{python}\n# Save dataframe as a new file\ndf_vehicles = pd.read_csv('../../data/01-modified-data/clean_vehicles.csv')\n\ndf_vehicles = df_vehicles.dropna()\n\nx = df_vehicles.columns[df_vehicles.dtypes != 'object']\nx = df_vehicles[x]\n\ny = df_vehicles['Continent']\n\n\n\n\nSplit Data\n\n\n{python}\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nmodel = GaussianNB()\n\nmodel.fit(x_train, y_train)\n\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\n\nRun Model\n\n\nResults\ny_pred = model.predict(x_test)\n\ntest_accuracy = accuracy_score(y_test, y_pred)\n\nprint('Test Set Accuracy: ', test_accuracy, \"\\n\", \"\\n\", classification_report(y_test, y_pred))\n\n\nTest Set Accuracy:  0.6290322580645161 \n \n                precision    recall  f1-score   support\n\n         Asia       0.60      0.35      0.44        17\n       Europe       0.76      0.72      0.74        39\nNorth America       0.33      0.83      0.48         6\n\n     accuracy                           0.63        62\n    macro avg       0.56      0.63      0.55        62\n weighted avg       0.67      0.63      0.63        62"
  },
  {
    "objectID": "5_NaiveBayes.html#lithium-news-sentiment-analysis",
    "href": "5_NaiveBayes.html#lithium-news-sentiment-analysis",
    "title": "Naive Bayes",
    "section": "Lithium News Sentiment Analysis",
    "text": "Lithium News Sentiment Analysis"
  },
  {
    "objectID": "8_decision_trees_Financial.html#introduction",
    "href": "8_decision_trees_Financial.html#introduction",
    "title": "Decision Trees",
    "section": "Introduction",
    "text": "Introduction\n\nTo continue with the previous analysis, we will build a decision tree model for the lithium companies financial information.\nBelow we can observe the libraries imported and the functions created that will be used in this section.\n\n\n\nLibraries\n# Import Data\nimport pandas as pd\n\n# Data Cleaning\nimport numpy as np\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Split Dataset\nfrom sklearn.model_selection import train_test_split\n\n# Random Classifier\nfrom collections import Counter\n\n# Sklearn Decision Tree Model\nfrom sklearn import tree\n\n# Show Results\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\nnp.random.seed(5000)\n\n\n\n\nFunction: random_classifier()\n## RANDOM CLASSIFIER \ndef random_classifier(y_data):\n    ypred=[];\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\",Counter(ypred).values()) # counts the elements' frequency\n    print(\"probability of prediction:\",np.fromiter(Counter(ypred).values(), dtype=float)/len(y_data)) # counts the elements' frequency\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data, ypred))\n\n\n\n\nFunction: confusion_plot()\ndef confusion_plot(actual, pred):\n\n    matrix = confusion_matrix(actual, pred)\n\n    TP = matrix[0,0]\n    FN = matrix[0,1]\n    FP = matrix[1,0]\n    TN = matrix[1,1]\n\n    ACCURACY = (TP+TN)/(TP+FP+FN+TN)\n    NEGATIVE_RECALL = TN/(TN+FP)\n    NEGATIVE_PRECISION = TN/(TN+FN)\n    POSITIVE_RECALL = TP/(TP+FN)\n    POSITIVE_PRECISION = TP/(TP+FP)\n\n    print('ACCURACY: ', ACCURACY)\n    print('NEGATIVE RECALL (Y=0): ', NEGATIVE_RECALL)\n    print('NEGATIVE PRECISION (Y=0): ', NEGATIVE_PRECISION)\n    print('POSITIVE RECALL (Y=1): ', POSITIVE_RECALL)\n    print('POSITIVE PRECISION (Y=1): ', POSITIVE_PRECISION)\n    \n    print(matrix)\n\n    cm = confusion_matrix(actual, pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot()\n\n\n\n\nFunction: plot_tree()\ndef plot_tree(model, X, Y):\n    model_X_Y = model.fit(X, Y)\n    \n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model_X_Y, \n                        feature_names=X.columns,\n                        #class_names=list(str(Y.keys())),\n                        filled=True)\n\n    plt.show()"
  },
  {
    "objectID": "8_decision_trees_Financial.html#methods",
    "href": "8_decision_trees_Financial.html#methods",
    "title": "Decision Trees",
    "section": "Methods",
    "text": "Methods\n\nIn Machine Learning, decision trees are classified as non-parametric supervised learning method used for both classification and regression models.\nThe goal of decision trees is to create a model that predicts the output value of the target variables based on different set of rules. This can be better understood as a set of questions where the first one, the root node, is a very general questions, and based on the answer the trajectory to make the next question. After several questions you can get to the final value.\nTo go from one inner node to another, there may be two possible paths, which we identify as binary split as it divides values into two subsets, or more tan two possible paths, which we identify as multi way split as it uses as many partitions as distinct values.\nThere are different types of decision trees, but in this analysis we will focus in binary classifier, where the output of each node is true or false.\nDecision trees have several advantages such as that they are simple to understand and easy to interpret. This is due to the visualization of the tree that can be created in order to understand the logic and the conditions behind it. It also required minimal data preparation compared to other models. As it was explained before, they can handle both numerical and categorical data and combine them in a way to handle multi-outut problems.\nOne of the main disadvantages of the decision trees is that they tend to overfit, but there are several techniques that can be applied to reduce this, such as pruning or setting a maximum depth. The predictions done by the decision trees are local, this means that if we want to extrapolate to new values, they might not perform well. Lastly, we need to consider that if the dataset is not well balanced, the performance might be poor too because the results will tend to be biased.\n\n\nClass distribution\n\nThe data that will be used for this analysis is the financial information for lithium companies. Considering the number of companies per specialization, we decided to filter out the dataset to keep the lithium producers and electric vehicle producers.\n\nOriginal Data\n\n\n  Company               KPI    Year  Quarter   Value\n0    AEHR  Accounts Payable  2013.0      4.0  1892.0\n1    AEHR  Accounts Payable  2014.0      1.0  2265.0\n2    AEHR  Accounts Payable  2014.0      2.0  1892.0\n3    AEHR  Accounts Payable  2014.0      3.0  1586.0\n4    AEHR  Accounts Payable  2014.0      4.0  1790.0\n\n\n(24874, 5)\n\n\nData Cleaning\n\nAfter cleaning the data, we can see that we have all the financial information for the companies and 71 variables that will be used for the analysis, which all are in float format. The data has been reshaped to be in a wide format instead of a long format. Below there is a list of the companies by type of production.\nLithium Production: - ALB - LTGM - SGML - PLL\nElectric Vehicles Production: - TSLA - F - LI - ON - RIVN - XPEV - LVWR - AEHR\n\n\n\n{python}\ndf = df[(df['Year'] == 2022) & (df['Quarter'] == 4)]\n\ndf.drop(['Year','Quarter'], axis=1, inplace=True)\n\n# - - -\n\n# df['Month'] = df['Quarter'] * 3\n\n# df['Day'] = 1\n\n# df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n\n# df.drop(['Year','Quarter','Day', 'Month'], axis=1, inplace=True)\n\ndf = df[df['Company'].isin(['ALB', 'LTHM', 'SGML', 'PLL', 'TSLA', 'F', 'LI', 'ON', 'RIVN', 'XPEV', 'LVWR', 'AEHR'])]\n\ndf = df.pivot(index=['Company'], columns='KPI', values='Value')\n\n# df = df.pivot(index=['Company', 'Date'], columns='KPI', values='Value')\n\ndf.reset_index(inplace=True)\n\n# df['Date'] = pd.to_datetime(df['Date'])\n\n# df.set_index('Date', inplace=True)\n\ndf.head(5)\n\n\nKPI Company  Accounts Payable  ...  Total Long-Term Liabilities  Working Capital\n0      AEHR            3949.0  ...                        130.0          54789.0\n1       ALB         2052001.0  ...                    4524660.0        2445902.0\n2         F        25605000.0  ...                  115851000.0       19610000.0\n3      LTHM           81700.0  ...                     482500.0         395300.0\n4      LVWR           12788.0  ...                      10562.0         267487.0\n\n[5 rows x 72 columns]\n\n\n{python}\ndf.shape\n\n\n(11, 72)\n\n\nData Exploration\n\n\n{python}\nsummary = df.describe().transpose()\nsummary['dtypes'] = df.dtypes\nsummary = summary[['dtypes', 'min', 'mean', 'max']]\nsummary\n\n\n                              dtypes        min          mean           max\nKPI                                                                        \nAccounts Payable             float64   1936.000  5.381028e+06  2.560500e+07\nBook Value Per Share         float64      1.520  1.760091e+01  6.813000e+01\nCash & Cash Equivalents      float64  36584.000  1.402432e+07  8.279000e+07\nCash & Equivalents           float64  18874.000  6.614173e+06  2.513400e+07\nCash Growth                  float64      0.008  9.495900e+00  9.841530e+01\n...                              ...        ...           ...           ...\nTotal Debt                   float64    616.000  1.470467e+07  1.389690e+08\nTotal Liabilities            float64  10876.000  2.742564e+07  2.127170e+08\nTotal Long-Term Assets       float64   2008.000  2.109271e+07  1.394080e+08\nTotal Long-Term Liabilities  float64    130.000  1.332009e+07  1.158510e+08\nWorking Capital              float64  54789.000  6.448504e+06  1.961000e+07\n\n[71 rows x 4 columns]\n\n\n\nThe next step is to divide the target column from the original data set. The target column is binary and has a value of 1 for the lithium manufacturing companies and 0 for the electric vehicle manufacturing companies.\n\n\n\n{python}\ndf['target'] = df['Company'].isin(['ALB', 'LTHM', 'SGML', 'PLL']).astype(int)\n\ndf = df.drop(columns=['Company'])\n\nlength_0 = len(df[df['target'] == 0])\nlength_1 = len(df[df['target'] == 1])\n\ntotal = df.shape[0]\n\nprint('Number of points with target=0: ', length_0, length_0/total)\n\n\nNumber of points with target=0:  7 0.6363636363636364\n\n\n{python}\nprint('Number of points with target=1: ', length_1, length_1/total)\n\n\nNumber of points with target=1:  4 0.36363636363636365\n\n\n\nIn the correlation plot below, we can see the Pearson correlation between all the variables. There are some that are highly positively correlated and some that don’t have any correlation at all.\nThe dividend per share variable is the only one that has a negative correlation with the rest of the variables.\n\n\n\n{python}\ncorr = df.corr()\n\nsns.set_theme(style=\"white\")\nf, ax = plt.subplots(figsize=(20, 20))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.show()\n\n\n\n\n\n\n\nRandom Classifier Model\n\nBefore building a decision tree model, we decided to test a baseline model to compare results in the next steps. The baseline model that will be used is a random classifier, since there are no criteria for assigning labels. The goal of comparing the random classifier with the decision tree is to see if the performance of our model is better than the performance of assigning a random label.\nWe can see that we get an accuracy of about 0.518, which is very low.\n\n\n\n{python}\ntarget_column = 'target'\n\nX = df.drop(columns=[target_column])\n\nY = df[target_column]\n\nX_array = X\nY_array = Y\n\n\nx_train, x_test, y_train, y_test = train_test_split(X_array, Y_array, test_size=0.2, random_state=0)\n\n\n\n\n\nBINARY CLASS: NON UNIFORM LOAD\n\n\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([6, 5])\nprobability of prediction: [0.54545455 0.45454545]\naccuracy 0.5454545454545454\npercision, recall, fscore, (array([0.66666667, 0.4       ]), array([0.57142857, 0.5       ]), array([0.61538462, 0.44444444]), array([7, 4]))\n\n\n\n\nDecision Tree Model\n\nNow that we have the random classifier model, we can build the decision tree model. To evaluate the results, we use the confusion matrix function that we built at the beginning of this section.\nFor the training set, we get very good results because the accuracy is equal to 1, so the other performance metrics, positive and negative recalls and precisions, are also equal to 1. For the test set, we get an accuracy of 0.66 approximately, which compared to the random classifier is performing better but it could be improved.\n\nTrain Results\n\n\nACCURACY:  1.0\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0):  1.0\nPOSITIVE RECALL (Y=1):  1.0\nPOSITIVE PRECISION (Y=1):  1.0\n[[4 0]\n [0 4]]\n\n\n\n\n\nTest Results\n\n\nACCURACY:  0.6666666666666666\nNEGATIVE RECALL (Y=0):  nan\nNEGATIVE PRECISION (Y=0):  0.0\nPOSITIVE RECALL (Y=1):  0.6666666666666666\nPOSITIVE PRECISION (Y=1):  1.0\n[[2 1]\n [0 0]]\n\n\n\n\n\nTree Plot\n\n\n{python}\nplot_tree(tree.DecisionTreeClassifier(), x_train, y_train)\n\n\n\n\n\n\n\nModel tuning:\n\nTo get the best decision tree model, we can tune the hyperparameters of the model. There are two types of hyperparameters that can be tuned. First, the min_samples, which is the minimum number of samples needed to split an internal node. The second is the max_depth, which is the maximum depth of the tree.\nBelow, we apply the hypertuning process to the max_depth parameter. Considering the low number of data points for the analysis the test and train metric values vary through the number of layers in decision tree. Therefore, for this analysis we choose a maximum depth equal to 4.\n\n\n\n{python}\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train,y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=0),recall_score(y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label=0),recall_score(y_train, yp_train,pos_label=1)])\n\ntest_results = pd.DataFrame(test_results, columns= (\"num_layer\", \"accuracy_score\", \"recall_score_0\", \"recall_score_1\"))\ntrain_results = pd.DataFrame(train_results, columns= (\"num_layer\", \"accuracy_score\", \"recall_score_0\", \"recall_score_1\"))\n\n# ACCURACY\n\nsns.scatterplot(data=test_results, x='num_layer', y='accuracy_score', label='Test Results', color='red')\nsns.lineplot(data=test_results, x='num_layer', y='accuracy_score', color='red')\n\nsns.scatterplot(data=train_results, x='num_layer', y='accuracy_score', label='Train Results', color='blue')\nsns.lineplot(data=train_results, x='num_layer', y='accuracy_score', color='blue')\n\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('ACCURACY (Y=0): Training (blue) and Test (red)')\n\nplt.show()\n\n\n\n\n\n{python}\n# RECALL (Y=0)\n\nsns.scatterplot(data=test_results, x='num_layer', y='recall_score_0', label='Test Results', color='red')\nsns.lineplot(data=test_results, x='num_layer', y='recall_score_0', color='red')\n\nsns.scatterplot(data=train_results, x='num_layer', y='recall_score_0', label='Train Results', color='blue')\nsns.lineplot(data=train_results, x='num_layer', y='recall_score_0', color='blue')\n\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=0): Training (blue) and Test (red)')\n\nplt.show()\n\n\n\n\n\n{python}\n# RECALL (Y=1)\n\nsns.scatterplot(data=test_results, x='num_layer', y='recall_score_1', label='Test Results', color='red')\nsns.lineplot(data=test_results, x='num_layer', y='recall_score_1', color='red')\n\nsns.scatterplot(data=train_results, x='num_layer', y='recall_score_1', label='Train Results', color='blue')\nsns.lineplot(data=train_results, x='num_layer', y='recall_score_1', color='blue')\n\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=1): Training (blue) and Test (red)')\n\nplt.show()"
  },
  {
    "objectID": "8_decision_trees_Financial.html#final-results",
    "href": "8_decision_trees_Financial.html#final-results",
    "title": "Decision Trees",
    "section": "Final results",
    "text": "Final results\n\nAfer running the final results we can observe that the shape of the decision tree has changed, but the performance metric values are the same as the original model.\n\n\n\n{python}\nbest_max_depth = 4\n\nmodel = tree.DecisionTreeClassifier(max_depth=best_max_depth)\nmodel = model.fit(x_train,y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\nprint(\"------TRAINING------\")\n\n\n------TRAINING------\n\n\n{python}\nconfusion_plot(y_train,yp_train)\n\n\nACCURACY:  1.0\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0):  1.0\nPOSITIVE RECALL (Y=1):  1.0\nPOSITIVE PRECISION (Y=1):  1.0\n[[4 0]\n [0 4]]\n\n\n{python}\nprint(\"------TEST------\")\n\n\n------TEST------\n\n\n{python}\nconfusion_plot(y_test,yp_test)\n\n\nACCURACY:  0.6666666666666666\nNEGATIVE RECALL (Y=0):  nan\nNEGATIVE PRECISION (Y=0):  0.0\nPOSITIVE RECALL (Y=1):  0.6666666666666666\nPOSITIVE PRECISION (Y=1):  1.0\n[[2 1]\n [0 0]]\n\n\n{python}\nplot_tree(model,X,Y)"
  },
  {
    "objectID": "8_decision_trees_Financial.html#conclusions",
    "href": "8_decision_trees_Financial.html#conclusions",
    "title": "Decision Trees",
    "section": "Conclusions",
    "text": "Conclusions\n\nAs a conclusion, we can observe that the variables used to define the output of the tree are Effective Tax Rate and Total Assets. Considering the manufacturing activity of each company and how different they are, these variables are clear decision makers for the decision tree.\nAnyway, for future analysis, additional companies and manufacturing activities have to be added in order to obtain better results and perform a multiclassifier decision tree."
  },
  {
    "objectID": "5_NaiveBayes.html#introduction",
    "href": "5_NaiveBayes.html#introduction",
    "title": "Naive Bayes",
    "section": "Introduction",
    "text": "Introduction\n\nNaive Bayes Classification is a category within Bayes’ Theorem that contains a collection of algorithms. The main purpose of using this type of algorithms is to classify data into predefined classes or categories. For this reason, Naive Bayes is considered a type of supervised learning, as the models are trained on labeled data, meaning that the data has been pre-classified. The main advantages of these algorithms are that they are simple, efficient, and useful in various applications, including record label data and text-labeled data. As the name implies, these methods are “naive”, meaning that they assume that the features used are independent in order to simplify the models and be computationally efficient. This assumption is usually not true in real-world cases, but the models perform well in practice.\nNaive Bayes is based on the principles of probability and statistics, therefore, to understand how the algorithms work, we first need an introduction to Bayes’ theorem. “Bayes’ theorem finds the probability of an event occurring given the probability of another event that has already occurred.” To understand the function, we try to find the probability of the event \\(A\\), given that the event \\(B\\), which is the evidence, is true. The probability of \\(A\\) is the prior of \\(A\\), that is, the probability of the event before the evidence is seen.\nBayes’ Thorem:\n\\(P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\)\n\\(P(A)\\): Probability of \\(A\\)\n\\(P(A|B)\\): Poterior probability of \\(B\\)\nThe algorithm uses the theorem to compute the probability that a data point belongs to a particular class given its features. It combines the prior knowledge with the new evidence. In summary, Naive Bayes classifies data by evaluating the likelihood of each class based on the observed features."
  },
  {
    "objectID": "5_NaiveBayes.html#objectives",
    "href": "5_NaiveBayes.html#objectives",
    "title": "Naive Bayes",
    "section": "Objectives",
    "text": "Objectives\n\nThrough this section we will focus in two analysis. First, we will use the record labeled data we have collected, cleaned, and explored in the previous sections. The data we will use is the Electric Vehicle Specifications dataset. With this analysis the goal is to understand if there is an existing pattern or relationship between different characteristics in order to classify them into the continent of origin. This is very useful to learn the manufacturing specifications and gain insights to draw conclusions about the desires for each market.\nThe second dataset we will use is the sentiment analysis results on the Lithium News data. The goal of this analysis is to understand if Naive Bayes classification algorithms are able to perform a similar procedure to identify sentiment analysis labels based on a reduced data set. This is very useful because sentiment analysis algorithms can be biased based on the connotations of certain words depending on the environment. By using Naive Bayes with trained data from a specific topic, the bias is reduced and the predictions should be more accurate. A third analysis is performed on the same dataset. The goal is to see if we can classify the articles into specific categories based on the vocabulary used, which are originally the publisher’s categories. Within these categories we find General, Business, Science and Technology."
  },
  {
    "objectID": "5_NaiveBayes.html#objectives-1",
    "href": "5_NaiveBayes.html#objectives-1",
    "title": "Naive Bayes",
    "section": "Objectives",
    "text": "Objectives\n\nDefine the objectives of what you are trying to do.\nWhat you aim to achieve through Naive Bayes classification.\nDescribe different variants of Naive Bayes, such as Gaussian, Multinomial, and Bernoulli Naive Bayes, and explain when to use each.\n\n\n\nPython Libraries\n\nimport pandas as pd\nimport numpy as np \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nimport sklearn \nfrom scipy.stats import spearmanr\n\n# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\n# Multinomial Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\n\n\n# Split the data\nfrom sklearn.model_selection import train_test_split\n\n# Performance Metrics\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport random\n\n# Features Text Data\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn import metrics\n\n\n\n\nFuction print_model_summary()\ndef print_model_summary(y_test, y_pred):\n    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n    print(\"Classification Report:\")\n    print(\"\\n\")\n    print(metrics.classification_report(y_test, y_pred))\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n\n\n\nEV Characteristics\n\n\nLithium News Sentiments\n\n\nLithium News Categories\n\n\n\n\nData Selection\n\nFor this section…\n\n\n\nFuction train_GNB_model()\ndef train_GNB_model(x_train,y_train,x_test,y_test,i_print=False):\n\n    # INSERT CODE HERE  \n    gnb_model = GaussianNB()\n\n    gnb_model.fit(x_train, y_train)\n\n    y_train_pred = gnb_model.predict(x_train)\n    y_test_pred = gnb_model.predict(x_test)\n\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n\n    if i_print == True:\n        print(train_accuracy*100, test_accuracy*100)\n\n    return train_accuracy, test_accuracy\n\n\n\n\n{python}\n# Save dataframe as a new file\ndf_vehicles = pd.read_csv('../../data/01-modified-data/clean_vehicles.csv')\n\ndf_vehicles = df_vehicles.dropna()\n\nx = df_vehicles.columns[df_vehicles.dtypes != 'object']\n\nx = df_vehicles[x]\n\ny = pd.Categorical(df_vehicles['Continent']).codes\n\nprint(pd.Categorical(df_vehicles['Continent']).categories)\n\n\nIndex(['Asia', 'Europe', 'North America'], dtype='object')\n\n\n\n\nResults\n\nFor this section…\n\n\n\n{python}\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nprint(\"Train Set Accuracy:\", train_GNB_model(x_train,y_train,x_test,y_test,i_print=False)[0])\nprint(\"Test Set Accuracy:\", train_GNB_model(x_train,y_train,x_test,y_test,i_print=False)[1])\n\n\n\n\n57.14285714285714 62.903225806451616\nTrain Set Accuracy: 0.5714285714285714\n\n\n57.14285714285714 62.903225806451616\nTest Set Accuracy: 0.6290322580645161\n\n\n\n\n\n\nData Selection\n\nFor this section…\n\n\n\n{python}\ndf_sentiment = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\n# Rename columns\ndf_sentiment = df_sentiment.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf_sentiment = df_sentiment[['label', 'text']]\n\nvectorizer = CountVectorizer()\n\nx1 = vectorizer.fit_transform(df_sentiment['text'])\ny1 = df_sentiment['label']\n\nx1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.2, random_state=42)\n\nmodel = MultinomialNB()\n\nmodel.fit(x1_train, y1_train)\n\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n{python}\n\ny1_pred = model.predict(x1_test)\n\n\n\n\nResults\n\nFor this section…\n\n\n\n{python}\n# Show summary\nprint_model_summary(y1_test, y1_pred)\n\n\n\n\n\n\n\n\n\nData Selection\n\nFor this section…\n\n\n\n{python}\ndf_sentiment = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\n# Rename columns\ndf_sentiment = df_sentiment.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf_sentiment = df_sentiment[['category', 'text']]\n\nvectorizer = CountVectorizer()\n\nx2 = vectorizer.fit_transform(df_sentiment['text'])\ny2 = df_sentiment['category']\n\nx2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.2, random_state=42)\n\nmodel = MultinomialNB()\n\nmodel.fit(x2_train, y2_train)\n\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n{python}\n\ny2_pred = model.predict(x2_test)\n\n\n\n\nResults\n\nFor this section…\n\n\n\n{python}\n# Show summary\nprint_model_summary(y2_test, y2_pred)\n\n\nAccuracy: 0.6052631578947368\nClassification Report:\n\n              precision    recall  f1-score   support\n\n    business       0.60      0.60      0.60        10\n     general       0.73      0.80      0.76        20\n     science       1.00      0.25      0.40         4\n  technology       0.00      0.00      0.00         4\n\n    accuracy                           0.61        38\n   macro avg       0.58      0.41      0.44        38\nweighted avg       0.65      0.61      0.60        38"
  },
  {
    "objectID": "5_NaiveBayes.html#methods",
    "href": "5_NaiveBayes.html#methods",
    "title": "Naive Bayes",
    "section": "Methods",
    "text": "Methods\n\nWithin the Naive Bayes algorithms we can find Gaussian, Multinomial and Bernoulli. Each one is different from the others because they can handle specific characteristics of the data and have different applications. The choice of the appropriate model is crucial because it improves the results, the performance and the accuracy of the classifier. The following explanations aim to summarize the main differences between the algorithms for a better understanding of the following application and analysis.\nGaussian Naive Bayes: This first model is usually used when the features in the data set are continuous. The features typically follow a normal (Gaussian) distribution, and statistics such as the mean and standard deviation for each class are used to model the distribution.\nMultinomial Naive Bayes: This second model is used when the features in the data set are discrete. The algorithm considers the frequency of occurrence of the feature within each class. An example of application is text data to perform natural language processing tasks such as text classification.\nBernoulli Naive Bayes: The third model is used for features that have a binary value. The algorithm focuses on whether the classification is yes or no, or true or false. This model is usually used in document classification tasks."
  },
  {
    "objectID": "5_NaiveBayes.html#methods-1",
    "href": "5_NaiveBayes.html#methods-1",
    "title": "Naive Bayes",
    "section": "Methods",
    "text": "Methods\n\nDescribe different variants of Naive Bayes, such as Gaussian, Multinomial, and Bernoulli Naive Bayes, and explain when to use each.\n\n\n\nPython Libraries\n\nimport pandas as pd\nimport numpy as np \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nimport sklearn \nfrom scipy.stats import spearmanr\n\n# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\n# Multinomial Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\n\n\n# Split the data\nfrom sklearn.model_selection import train_test_split\n\n# Performance Metrics\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport random\n\n# Features Text Data\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn import metrics\n\n\n\n\nFuction print_model_summary()\ndef print_model_summary(y_test, y_pred):\n    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n    print(\"Classification Report:\")\n    print(\"\\n\")\n    print(metrics.classification_report(y_test, y_pred))\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n\n\n\nEV Characteristics\n\n\nLithium News Sentiments\n\n\nLithium News Categories\n\n\n\n\nData Selection\n\nFor this section…\n\n\n\nFuction train_GNB_model()\ndef train_GNB_model(x_train,y_train,x_test,y_test,i_print=False):\n\n    # INSERT CODE HERE  \n    gnb_model = GaussianNB()\n\n    gnb_model.fit(x_train, y_train)\n\n    y_train_pred = gnb_model.predict(x_train)\n    y_test_pred = gnb_model.predict(x_test)\n\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n\n    if i_print == True:\n        print(train_accuracy*100, test_accuracy*100)\n\n    return train_accuracy, test_accuracy\n\n\n\n\n{python}\n# Save dataframe as a new file\ndf_vehicles = pd.read_csv('../../data/01-modified-data/clean_vehicles.csv')\n\ndf_vehicles = df_vehicles.dropna()\n\nx = df_vehicles.columns[df_vehicles.dtypes != 'object']\n\nx = df_vehicles[x]\n\ny = pd.Categorical(df_vehicles['Continent']).codes\n\nprint(pd.Categorical(df_vehicles['Continent']).categories)\n\n\nIndex(['Asia', 'Europe', 'North America'], dtype='object')\n\n\n\n\nResults\n\nFor this section…\n\n\n\n{python}\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nprint(\"Train Set Accuracy:\", train_GNB_model(x_train,y_train,x_test,y_test,i_print=False)[0])\nprint(\"Test Set Accuracy:\", train_GNB_model(x_train,y_train,x_test,y_test,i_print=False)[1])\n\n\n\n\n57.14285714285714 62.903225806451616\nTrain Set Accuracy: 0.5714285714285714\n\n\n57.14285714285714 62.903225806451616\nTest Set Accuracy: 0.6290322580645161\n\n\n\n\n\n\nData Selection\n\nFor this section…\n\n\n\n{python}\ndf_sentiment = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\n# Rename columns\ndf_sentiment = df_sentiment.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf_sentiment = df_sentiment[['label', 'text']]\n\nvectorizer = CountVectorizer()\n\nx1 = vectorizer.fit_transform(df_sentiment['text'])\ny1 = df_sentiment['label']\n\nx1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.2, random_state=42)\n\nmodel = MultinomialNB()\n\nmodel.fit(x1_train, y1_train)\n\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n{python}\n\ny1_pred = model.predict(x1_test)\n\n\n\n\nResults\n\nFor this section…\n\n\n\n{python}\n# Show summary\nprint_model_summary(y1_test, y1_pred)\n\n\n\n\n\n\n\n\n\nData Selection\n\nFor this section…\n\n\n\n{python}\ndf_sentiment = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\n# Rename columns\ndf_sentiment = df_sentiment.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf_sentiment = df_sentiment[['category', 'text']]\n\nvectorizer = CountVectorizer()\n\nx2 = vectorizer.fit_transform(df_sentiment['text'])\ny2 = df_sentiment['category']\n\nx2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.2, random_state=42)\n\nmodel = MultinomialNB()\n\nmodel.fit(x2_train, y2_train)\n\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n{python}\n\ny2_pred = model.predict(x2_test)\n\n\n\n\nResults\n\nFor this section…\n\n\n\n{python}\n# Show summary\nprint_model_summary(y2_test, y2_pred)\n\n\nAccuracy: 0.6052631578947368\nClassification Report:\n\n              precision    recall  f1-score   support\n\n    business       0.60      0.60      0.60        10\n     general       0.73      0.80      0.76        20\n     science       1.00      0.25      0.40         4\n  technology       0.00      0.00      0.00         4\n\n    accuracy                           0.61        38\n   macro avg       0.58      0.41      0.44        38\nweighted avg       0.65      0.61      0.60        38"
  },
  {
    "objectID": "5_NaiveBayes.html#analysis",
    "href": "5_NaiveBayes.html#analysis",
    "title": "Naive Bayes",
    "section": "Analysis",
    "text": "Analysis\n\nThe analysis for this section includes three tabs, which we will observe below. As explained above, we will perform Gaussian Naive Bayes Classification on the Electric Vehicles Characteristics dataset, Multinimial Naive Bayes Classification on the Lithium News sentiment analysis results, and Lithium News publisher categories.\nIn this case, we used Python libraries to develop the analysis and effective visualizations to show the results. The libraries required and functions created for the analysis are listed below.\n\n\n\nPython Libraries\n\nimport pandas as pd\nimport numpy as np \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nimport sklearn \nfrom scipy.stats import spearmanr\n\n# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\n# Multinomial Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\n\n\n# Split the data\nfrom sklearn.model_selection import train_test_split\n\n# Performance Metrics\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport random\n\n# Features Text Data\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn import metrics\n\n\n\n\nFuction print_model_summary()\ndef print_model_summary(y_test, y_pred):\n    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n    print(\"\\n\")\n    print(\"Classification Report:\")\n    print(\"\\n\")\n    print(metrics.classification_report(y_test, y_pred))\n    \n    cm = metrics.confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n\n\n\n\nEV Characteristics\n\n\nLithium News Sentiments\n\n\nLithium News Categories\n\n\n\n\nElectric Vehicles Characteristics ~ Record Labeled Data\n\n\nData Selection\n\nBased on the data available for analysis, we will use the numeric features; Battery, Efficiency, FastCharge, Price, Range, TopSpeed and Acceleration to predict the Continent where they were manufactured. For practical purposes, we converted the target feature; Continent, to be numeric.\n\nAsia = 0\nEurope = 2\nNorth America = 3\n\n\n\n\n{python}\n# Save dataframe as a new file\ndf_vehicles = pd.read_csv('../../data/01-modified-data/clean_vehicles.csv')\n\ndf_vehicles = df_vehicles.dropna()\n\nx = df_vehicles.columns[df_vehicles.dtypes != 'object']\n\nx = df_vehicles[x]\n\ny = pd.Categorical(df_vehicles['Continent']).codes\n\ndf_vehicles.head(5).to_csv('../../data/02-visualization-data/df_vehicles.csv', index=False)\n\n\n\n\n\n\n\nBrand\nCarName\nBattery\nEfficiency\nFastCharge\nPrice\nRange\nTopSpeed\nAcceleration\nContinent\n\n\n\n\nAbarth\nAbarth 500e Hatchback\n37.8\n168\n370\n37990\n225\n155\n7.0\nEurope\n\n\nAbarth\nAbarth 500e Convertible\n37.8\n168\n370\n40990\n225\n155\n7.0\nEurope\n\n\nAiways\nAiways U6\n60.0\n171\n430\n47588\n350\n160\n7.0\nEurope\n\n\nAiways\nAiways U5\n60.0\n190\n380\n39563\n315\n150\n7.5\nEurope\n\n\nAudi\nAudi Q8 e-tron Sportback 50 quattro\n89.0\n200\n690\n76650\n445\n200\n6.0\nEurope\n\n\n\n\n\n\n\n\n\nResults\n\nWith the goal of having an efficient and reproducible analysis, we created a function to train the Gaussian Naive Bayes model. This is useful for creating the model, fitting it, creating the test sets, and observing the performance of each set.\nTo split the data, we considered 20% of the total records for the test set and the remaining 80% for training. After applying the function to the analysis, we can see that we get an accuracy value for the training set and the test set. The accuracy value for the training set is approximately 0.57 while the test set is approximately 0.63.\n\n\n\nFuction train_GNB_model()\ndef train_GNB_model(x_train,y_train,x_test,y_test,i_print=False):\n\n    # INSERT CODE HERE  \n    gnb_model = GaussianNB()\n\n    gnb_model.fit(x_train, y_train)\n\n    y_train_pred = gnb_model.predict(x_train)\n    y_test_pred = gnb_model.predict(x_test)\n\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n\n    if i_print == True:\n        print(train_accuracy*100, test_accuracy*100)\n\n    return train_accuracy, test_accuracy\n\n\n\n\n{python}\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\nprint(\"Train Set Accuracy:\", train_GNB_model(x_train,y_train,x_test,y_test,i_print=False)[0])\nprint(\"Test Set Accuracy:\", train_GNB_model(x_train,y_train,x_test,y_test,i_print=False)[1])\n\n\n\n\nTrain Set Accuracy: 0.5714285714285714\n\n\nTest Set Accuracy: 0.6290322580645161\n\n\n\n\nConclusion\n\nBased on the results obtained, we can observe that the accuracy of the training set is lower than the accuracy of the test set. Considering the small number of data points used for this analysis, the results are considered good. So far, we can observe that the values for the analyzed features are useful to determine the class to which they belong.\nFor future analysis, it would be useful to include additional data points so that the analysis is well balanced. We can also consider adding another target label to understand, for example, if we can target the country where the electric vehicles are manufactured.\n\n\n\n\n\nLithium News Sentiments ~ Text Labeled Data\n\n\nData Selection\n\nThe relevant data for this analysis is the article content which appears as text and the label assigned by the IBM sentiment analysis algorithm. This data requires additional data cleaning as the text needs to be vectorized. This means that we will produce a sparse representation of the counts for the tokens. The next step is to divide the feature array and target array in the train and test set. With the sets created we run the model to obtain results.\n\n\n\n{python}\n# Read csv file\ndf_sentiment = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\n# Rename columns\ndf_sentiment = df_sentiment.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf_sentiment = df_sentiment[['label', 'text']]\n\nvectorizer = CountVectorizer()\n\nx1 = vectorizer.fit_transform(df_sentiment['text'])\ny1 = df_sentiment['label']\n\nx1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.2, random_state=42)\n\nmodel = MultinomialNB()\n\nmodel = model.fit(x1_train, y1_train)\n\ny1_pred = model.predict(x1_test)\n\ndf_sentiment.head(5).to_csv('../../data/02-visualization-data/df_sentiment.csv', index=False)\n\n\n\n\n\n\n\nlabel\ntext\n\n\n\n\npositive\nIf you recently moved into a new place or are just looking to update your home's security, now's a good time to do so. Though Black Friday has come and gone, Blink's video doorbell and two fourth-gen… [+1310 chars]\n\n\nneutral\nTesla Cybertruck will usher in a new Powershare bidirectional charging feature Tesla Cybertruck will usher in a new Powershare bidirectional charging feature / The EV maker finally jumps on the ve… [+2497 chars]\n\n\npositive\nThe robotic line cooks were deep in their recipe, toiling away in a room tightly packed with equipment. In one corner, an articulated arm selected and mixed ingredients, while another slid back and f… [+3678 chars]\n\n\npositive\nRenewable energies like wind and solar are clean, abundant, and cheap but notoriously unpredictable. Thats why so much time and money has been pumped into scaling energy storage solutions: we need to… [+4764 chars]\n\n\nnegative\nNEW YORK -- One person was killed and six others were injured when a fire blamed on an electric bicycle battery tore through a New York City apartment, officials said Monday. The fire started at aro… [+884 chars]\n\n\n\n\n\n\n\n\n\nResults\n\nBelow we can observe that after running the model we obtain an accuracy of approximately 0.61. We can also observe the Classification Report which show additional relevant metric used to evaluate the performance of the model. The F1 metric measures the model’s accuracy by combining the precision and recall scores by label. The negative sentiment is the one that has the higher performance compared to neutral and positive.\n\n\n\n{python}\n# Show summary\nprint_model_summary(y1_test, y1_pred)\n\n\nAccuracy: 0.6052631578947368\n\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n    negative       0.62      0.73      0.67        11\n     neutral       0.71      0.53      0.61        19\n    positive       0.45      0.62      0.53         8\n\n    accuracy                           0.61        38\n   macro avg       0.59      0.63      0.60        38\nweighted avg       0.63      0.61      0.61        38\n\n\n\n\n\n\n\nConclusions\n\nThe most important to take into consideration based on the results obtained are the False Positives and False Negatives for the negative and positive sentiments. This means that the cases we want to minimize are classifying a positive as a negative or a negative as a positive. That will be misleading to make future decisions.\n\n\n\n\n\nLithium News Categories ~ Text Labeled Data\n\n\nData Selection\n\nThe relevant data for this analysis is the article content which appears as text and the category the publisher belongs to. Below we can see some examples of three articles that belong to a Technology source and one article that belongs to a general source. We applied the same steps for the text data cleaning as in the Lithium News Sentiments Analysis\n\n\n\n{python}\ndf_sentiment = pd.read_csv('../../data/01-modified-data/clean_sentiment_analysis.csv')\n\n# Rename columns\ndf_sentiment = df_sentiment.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n\ndf_sentiment = df_sentiment[['category', 'text']]\n\nvectorizer = CountVectorizer()\n\nx2 = vectorizer.fit_transform(df_sentiment['text'])\ny2 = df_sentiment['category']\n\nx2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.2, random_state=42)\n\nmodel = MultinomialNB()\n\nmodel = model.fit(x2_train, y2_train)\n\ny2_pred = model.predict(x2_test)\n\ndf_sentiment.head(5).to_csv('../../data/02-visualization-data/df_sentiment_2.csv', index=False)\n\n\n\n\n\n\n\ncategory\ntext\n\n\n\n\ntechnology\nIf you recently moved into a new place or are just looking to update your home's security, now's a good time to do so. Though Black Friday has come and gone, Blink's video doorbell and two fourth-gen… [+1310 chars]\n\n\ntechnology\nTesla Cybertruck will usher in a new Powershare bidirectional charging feature Tesla Cybertruck will usher in a new Powershare bidirectional charging feature / The EV maker finally jumps on the ve… [+2497 chars]\n\n\ntechnology\nThe robotic line cooks were deep in their recipe, toiling away in a room tightly packed with equipment. In one corner, an articulated arm selected and mixed ingredients, while another slid back and f… [+3678 chars]\n\n\ntechnology\nRenewable energies like wind and solar are clean, abundant, and cheap but notoriously unpredictable. Thats why so much time and money has been pumped into scaling energy storage solutions: we need to… [+4764 chars]\n\n\ngeneral\nNEW YORK -- One person was killed and six others were injured when a fire blamed on an electric bicycle battery tore through a New York City apartment, officials said Monday. The fire started at aro… [+884 chars]\n\n\n\n\n\n\n\n\n\nResults\n\nBelow we can observe that after running the model we obtain an accuracy of approximately 0.61. We can also observe the Classification Report which show additional relevant metric used to evaluate the performance of the model. The F1 metric measures the model’s accuracy by combining the precision and recall scores by category. The general category is the one with the highest score, followed by the business one.\n\n\n\n{python}\n# Show summary\nprint_model_summary(y2_test, y2_pred)\n\n\nAccuracy: 0.6052631578947368\n\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n    business       0.60      0.60      0.60        10\n     general       0.73      0.80      0.76        20\n     science       1.00      0.25      0.40         4\n  technology       0.00      0.00      0.00         4\n\n    accuracy                           0.61        38\n   macro avg       0.58      0.41      0.44        38\nweighted avg       0.65      0.61      0.60        38\n\n\n\n\n\n\n\nConclusions\n\nThe results obtained are useful to better understand the use of vocabulary for certain publications. This can be applied to improve writing and use the correct vocabulary to attract future readers for the publishers. In this investigation, we can dive in the classifications done in order to understand which terms are more frequent by category while including the sentiment analysis label to detect patterns."
  },
  {
    "objectID": "6_dimensionality_reduction.html#data-selection",
    "href": "6_dimensionality_reduction.html#data-selection",
    "title": "Dimensionality Reduction",
    "section": "Data selection",
    "text": "Data selection\n\nAs explained earlier, the dataset has 12 rows and 79 columns. The 12 rows correspond to the companies to be analyzed. The 79 columns contain the variables from the quarterly financial report that make up the companies’ income statement and balance sheet.\nFor this analysis, we want to compare the results of the dimensionality reduction techniques and see how they differ over time. We have filtered the data so that Q4 2019 is the first data frame and Q4 2022 is the second data frame. This analysis is important to understand how the dimensionality reduction changed and was affected by COVID.\n\nFinancial Information for 2019 Companies\n\n\n{python}\ndf_2019 = df[(df['Year'] == 2019) & (df['Quarter'] == 4)]\n\ndf_2019 = df_2019.pivot(index=['Company'], columns='KPI', values='Value')\n\ndf_2019.reset_index(inplace=True)\n\ndf_2019 = df_2019.fillna(0)\n\ndf_2019.head(5).to_csv('../../data/02-visualization-data/df_2019.csv', index=False)\n\n\n\n\n\n\n\nCompany\nAEHR\nALB\nBYDDF\nF\nLTHM\n\n\nAccounts.Payable\n2320\n574138\n36168168\n20673000\n83100\n\n\nBook.Value.Per.Share\n0.70\n37.08\n9.48\n8.38\n3.73\n\n\nCash…Cash.Equivalents\n5302\n613110\n12684428\n88302000\n16800\n\n\nCash…Equivalents\n5302\n613110\n12650083\n17504000\n16800\n\n\n\n\n\n\n\n\nShape: (12, 79)\n\n\nFinancial Information for 2022 Companies\n\n\n{python}\ndf_2022 = df[(df['Year'] == 2022) & (df['Quarter'] == 4)]\n\ndf_2022 = df_2022.pivot(index=['Company'], columns='KPI', values='Value')\n\ndf_2022.reset_index(inplace=True)\n\ndf_2022 = df_2022.fillna(0)\n\ndf_2022.head(5).to_csv('../../data/02-visualization-data/df_2022.csv', index=False)\n\n\n\n\n\n\n\nCompany\nAEHR\nALB\nBYDDF\nF\nLTHM\n\n\nAccounts.Payable\n3949\n2052001\n143765729\n25605000\n81700\n\n\nBook.Value.Per.Share\n2.06\n68.13\n38.14\n10.80\n8.05\n\n\nCash…Cash.Equivalents\n36584\n1499142\n72098193\n82790000\n189000\n\n\nCash…Equivalents\n18874\n1499142\n51471263\n25134000\n189000\n\n\n\n\n\n\n\n\nShape: (13, 79)"
  },
  {
    "objectID": "8_decision_trees.html#random-classifier-model",
    "href": "8_decision_trees.html#random-classifier-model",
    "title": "Decision Trees",
    "section": "Random Classifier Model",
    "text": "Random Classifier Model\n\nBefore building a decision tree model, we decided to test a baseline model to compare results in the next steps. The baseline model that will be used is a random classifier, since there are no criteria for assigning labels. The goal of comparing the random classifier with the decision tree is to see if the performance of our model is better than the performance of assigning a random label.\nWe can see that we get an accuracy of about 0.3, which is very low.\n\n\n\n{python}\nX_array = X\nY_array = Y\n\nx_train, x_test, y_train, y_test = train_test_split(X_array, Y_array, test_size=0.2, random_state=0)\n\n\n\n\n\nMULTI-CLASS:  NON-UNIFORM LOAD\n\n\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([123, 74, 110])\nprobability of prediction: [0.40065147 0.24104235 0.35830619]\naccuracy 0.3257328990228013\npercision, recall, fscore, (array([0.24324324, 0.59090909, 0.13821138]), array([0.24324324, 0.33333333, 0.44736842]), array([0.24324324, 0.42622951, 0.21118012]), array([ 74, 195,  38]))"
  },
  {
    "objectID": "8_decision_trees.html#decision-tree-model",
    "href": "8_decision_trees.html#decision-tree-model",
    "title": "Decision Trees",
    "section": "Decision Tree Model",
    "text": "Decision Tree Model\n\nNow that we have the random classifier model, we can build the decision tree model. To evaluate the results, we use the confusion matrix function that we created at the beginning of this section.\nFor the training set, we get very good results because the accuracy is equal to 1, so the other performance metrics, positive and negative recalls and precisions, are also equal to 1. For the test set, we get an accuracy of about 0.83, which is higher compared to the random classifier and we consider it a good performance for this model.\nLooking at the decision tree, we can see that there is a high number of branches and a high depth. So, the next step is to apply model tuning techniques in order to get a simpler version without sacrificing the performance.\n\nTrain Results\n\n\nACCURACY:  1.0\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0):  1.0\nPOSITIVE RECALL (Y=1):  1.0\nPOSITIVE PRECISION (Y=1):  1.0\n[[ 58   0   0]\n [  0 154   0]\n [  0   0  33]]\n\n\n\n\n\nTest Results\n\n\nACCURACY:  0.8367346938775511\nNEGATIVE RECALL (Y=0):  0.8484848484848485\nNEGATIVE PRECISION (Y=0):  0.9032258064516129\nPOSITIVE RECALL (Y=1):  0.8125\nPOSITIVE PRECISION (Y=1):  0.7222222222222222\n[[13  3  0]\n [ 5 28  8]\n [ 0  1  4]]\n\n\n\n\n\nTree Plot\n\n\n{python}\nplot_tree(tree.DecisionTreeClassifier(), x_train, y_train, labels)"
  },
  {
    "objectID": "8_decision_trees.html#model-tuning",
    "href": "8_decision_trees.html#model-tuning",
    "title": "Decision Trees",
    "section": "Model tuning:",
    "text": "Model tuning:\n\nTo get the best decision tree model, we can tune the hyperparameters of the model. There are two types of hyperparameters that can be tuned. First, the min_samples, which is the minimum number of samples needed to split an internal node. The second is the max_depth, which is the maximum depth of the tree.\nBelow, we apply the hypertuning process to the max_depth parameter. Considering the low number of data points for the analysis the test and train metric values vary through the number of layers in decision tree. Therefore, for this analysis we choose a maximum depth equal to 5.\n\n\n\n{python}\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train,y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,average='micro')])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,average='micro')])\n\ntest_results = pd.DataFrame(test_results, columns= (\"num_layer\", \"accuracy_score\", \"recall_score\"))\ntrain_results = pd.DataFrame(train_results, columns= (\"num_layer\", \"accuracy_score\", \"recall_score\"))\n\n# ACCURACY\n\nsns.scatterplot(data=test_results, x='num_layer', y='accuracy_score', label='Test Results', color='red')\nsns.lineplot(data=test_results, x='num_layer', y='accuracy_score', color='red')\n\nsns.scatterplot(data=train_results, x='num_layer', y='accuracy_score', label='Train Results', color='blue')\nsns.lineplot(data=train_results, x='num_layer', y='accuracy_score', color='blue')\n\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('ACCURACY: Training (blue) and Test (red)')\n\nplt.show()\n\n\n\n\n\n{python}\n# RECALL\n\nsns.scatterplot(data=test_results, x='num_layer', y='recall_score', label='Test Results', color='red')\nsns.lineplot(data=test_results, x='num_layer', y='recall_score', color='red')\n\nsns.scatterplot(data=train_results, x='num_layer', y='recall_score', label='Train Results', color='blue')\nsns.lineplot(data=train_results, x='num_layer', y='recall_score', color='blue')\n\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL : Training (blue) and Test (red)')\n\nplt.show()"
  },
  {
    "objectID": "10_conclusions.html#final-results",
    "href": "10_conclusions.html#final-results",
    "title": "Conclusions",
    "section": "Final results",
    "text": "Final results\n::: {.justify}"
  },
  {
    "objectID": "10_conclusions.html",
    "href": "10_conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "::: {.justify}"
  },
  {
    "objectID": "10_conclusions.html#limitations",
    "href": "10_conclusions.html#limitations",
    "title": "Conclusions",
    "section": "Limitations",
    "text": "Limitations"
  },
  {
    "objectID": "10_conclusions.html#future-analysis",
    "href": "10_conclusions.html#future-analysis",
    "title": "Conclusions",
    "section": "Future Analysis",
    "text": "Future Analysis\n\nThe main limitations were overcome at the beginning of the study. Data collection can be challenging in such a young industry. Although data paradigms are different than in the past, the availability of sources has increased, lithium is considered white gold, and information plays a key role in the decision-making process.\nThe main limitations are related to the production of lithium batteries. Composition to minimize environmental impact, while maintaining low cost and high efficiency, is what differentiates a producer from others. Future points of analysis are needed to draw more significant conclusions. Assessing the impact of resource variability in the lithium battery production model may be key to understanding the most advantaged and disadvantaged players.\nThe inclusion of other data sources allows the investigation to draw different conclusions. Evaluate data to identify environmental impacts using Geographic Information Systems (GIS) to locate key mining areas. Based on these locations, incorporate weather data such as rainfall and temperature, air quality, and concentrations of key air pollutants, among others. In addition, we can incorporate other approaches to enhance the investigation, such as time series analysis to identify seasonality, trends, autocorrelations, stationarity models to build predictive models and provide accurate results."
  },
  {
    "objectID": "10_conclusions.html#future-analysis-1",
    "href": "10_conclusions.html#future-analysis-1",
    "title": "Conclusions",
    "section": "Future Analysis",
    "text": "Future Analysis\n\nThe main limitations were overcome at the beginning of the study. Data collection can be challenging in such a young industry. Although data paradigms are different than in the past, the availability of sources has increased, lithium is considered white gold, and information plays a key role in the decision-making process.\nThe main limitations are related to the production of lithium batteries. Composition to minimize environmental impact, while maintaining low cost and high efficiency, is what differentiates a producer from others. Future points of analysis are needed to draw more significant conclusions. Assessing the impact of resource variability in the lithium battery production model may be key to understanding the most advantaged and disadvantaged players.\nThe inclusion of other data sources allows the investigation to draw different conclusions. Evaluate data to identify environmental impacts using Geographic Information Systems (GIS) to locate key mining areas. Based on these locations, incorporate weather data such as rainfall and temperature, air quality, and concentrations of key air pollutants, among others."
  },
  {
    "objectID": "10_conclusions.html#key-findings",
    "href": "10_conclusions.html#key-findings",
    "title": "Conclusions",
    "section": "Key Findings",
    "text": "Key Findings\n\nIn summary, the goal of this project was to comprehensively examine the latest industry data to gain valuable insights and identify potential opportunities for future improvement. The process began with the examination of six data sets using various machine learning algorithms to extract meaningful insights. Throughout the process, essential steps such as data cleaning, exploration, and the application of algorithms such as Naive Bayes, dimensionality reduction, clustering, and decision tree analysis were critical. With these analyses complete, we can now focus on answering the initial data science questions. The findings from the project provide the basis for informed decisions and possible directions for improvement.\n\n\n\n1- How has the distribution of the total global lithium production among the main lithium producing countries evolved over the past two decades?\n\n\nAs we previously analyzed in the exploratory data analysis, there has been a major evolution in global lithium production over the last two decades. In 1995, Australia, the United States and Chile were the major players, and today Australia is a major player, followed by Chile and China with very low values in comparison. This analysis highlights the extraordinary growth of the lithium industry, indicating the robust expansion of the industry thanks to the demand for products that use lithium as a primary resource.\n\n\n\n                   \n\n\n\n2- How have the main lithium related companies performed in the stocks market over the past two decades and how do they compare between each other?\n5- Do lithium related companies have a similar financial performance and is there a clear relationship between them? How did COVID affect these companies and relationships?\n6- Does the financial data of the main lithium related companies differentiate enough to distinct them?\n\n\nBased on the analysis of major lithium-related companies in the stock market over the past two decades, several important conclusions can be drawn. First, Panasonic has proven its market potential by becoming a major player in the lithium battery manufacturing sector. Second, Tesla’s market leadership in the electric vehicle segment is evident, with a significant rise in stock prices reflecting its dominant position. In addition, the entry of numerous competitors in recent years suggests a growing industry. It’s important to note that the focus has been on major U.S.-based companies, and there are other globally significant players that should be considered for a more complete view of the industry. This is indicative of the growth and expansion of the lithium market.\nFrom the dimensionality reduction and clustering analysis comparing the financial information of lithium-related companies, a notable observation appears. From this perspective, the companies are more closely related, with some exceptions such as BYD, F, TSLA and XPEV, which stand out as outliers. Understanding the key differences between these companies becomes critical as it highlights the most influential variables to consider when evaluating their relationships. This detailed exploration contributes to a broader understanding of the dynamics within the lithium-related industry and helps identify the key factors influencing the financial performance of these companies.\n\n\n\n3- How does the lithium price correlate with the complementary resources used to produce lithium batteries?\n\n\nTo assess the correlation between lithium prices and the complementary resources used in lithium battery production, we used Pearson’s correlation coefficient. The results show a significant correlation between lithium prices and zinc, nickel and aluminum. Conversely, the correlation with copper and cobalt is less pronounced. This finding contributes to a deeper understanding of the relationship between these vital resources and highlights their co-variation dynamics in the context of lithium battery production. These results are relevant as battery manufacturing companies are highly dependent on their primary resources and their costs. An increase in production costs leads to an increase in the price of the final product.\n\n\n\n4- Can we use machine learning to predict sentiment analysis in text data and recognize categories?\n\n\nTo address the question of whether machine learning can predict sentiment analysis and recognize categories in text data, we used the IBM Watson API to label news articles related to lithium topics. The articles were labeled as positive with a score close to 1, neutral with a score close to 0, or negative with a score close to -1. The labeled data was then used to construct a Naive Bayes Multinomial Classifier model to assign sentiment labels. The performance of the model, with an accuracy of about 0.60, was affected by the constraint of a relatively small sample size. In particular, the evaluation focused on minimizing false positives and false negatives for positive and negative sentiments. It is important to avoid misclassifying positive sentiments as negative or vice versa, as such misclassifications could lead to misleading decisions in future analyses. In conclusion, while the model demonstrated moderate accuracy, ongoing efforts to refine and expand the dataset could improve its predictive capabilities by adding additional relevant data and by training and testing the model.\n\n\n\n7- Is there a pattern in the electrical vehicles characteristics in order to predict where they were manufactured?\n\n\nA Gaussian Naive Bayes model and a decision tree were used to explore the patterns in the characteristics of electric vehicles to predict their manufacturing origin. The analyses showed the usefulness of the analyzed features in determining the class to which the vehicles belonged. The results obtained for the decision tree highlight the effectiveness of the analyzed features in determining the continent of manufacture.\nWhile both models demonstrated the need for a larger dataset, the advantage of the decision tree lies in its interpretability, providing transparency into the decision-making process. In conclusion, further refinement and expansion of the dataset will contribute to a more comprehensive understanding of electric vehicle manufacturing patterns.\n\n\n\n8- How has public sentiment and opinion evolved regarding the increasing demand for lithium, and can text analysis reveal whether there is a pattern of sentiment by source of data?\n\n\nTo assess the evolution of public sentiment regarding the increasing demand for lithium and whether text analysis reveal sentiment patterns by data source, sentiment labels were analyzed across various article publishers. Notably, some sources showed a bias toward negative sentiment, such as ABC News and TechRadar. However, no clear pattern emerged that correlated sentiment with specific categories. Additional data collection and in-depth analysis will be required to draw more comprehensive conclusions about the evolution of sentiment over time. It’s important to note a limitation of the News API, which restricts data collection beyond a limited timeframe from the current date.\n\n\n\n                   \n\n\n\n9- Are there specific terminologies that drive the sentiment of publications?\n\n\nWhile the word clouds generated from the sentiment analysis labels reveal common terminology across sentiments and provide insight into overall language consistency, it is important to note that there may not be enough evidence to definitively answer the question of whether specific terminology drives the sentiment of publications. Notably, terms such as “geothermal,” “sustainable,” and “recycling” appeared in the positive word cloud, reflecting concepts that have gained increased interest in recent years, particularly in the context of studies exploring geothermal energy storage through lithium mining. Conversely, the negative sentiment word cloud included terms such as “company,” “president,” “federal,” and “fire”. While these words may contribute to a more negative sentiment, it is important to recognize that further in-depth analysis is required to establish a concrete correlation between specific terms and sentiment.\n\n\n\n10- What conclusions can we draw based on the industry analysis and are there specific concerns arising?\n\n\nIn conclusion, the comprehensive analysis of the lithium industry reveals a significant and rapid growth trajectory, driven by high demand and positive expectations regarding environmental benefits. However, it is critical to acknowledge concerns related to resource exploitation. Despite the limited data available for analysis, this study serves as a fundamental step in understanding environmental impacts and formulating actionable strategies to improve the overall quality of life. As the industry expands, there is a need for responsible practices and sustainable approaches to mitigate potential environmental drawbacks. This analysis lays the groundwork for future research and initiatives aimed at achieving a balance between industry growth and environmental stewardship."
  },
  {
    "objectID": "7_clustering.html#data-selection",
    "href": "7_clustering.html#data-selection",
    "title": "Data Clustering",
    "section": "Data selection",
    "text": "Data selection\n\nFor this analysis, we will use the financial data of lithium companies. Considering the high number of features in this dataset, we decide to use the output dataset obtained from the dimensionality reduction analysis. This is very helpful because we have reduced the variability while maintaining the quality of the data.\n\nCompanies Financial Data\n\n\n{python}\n# Read csv\ndf = pd.read_csv('../../data/01-modified-data/clean_companies_finance.csv')\n\ndf_2019 = df[(df['Year'] == 2019) & (df['Quarter'] == 4)]\n\ndf_2019 = df_2019.pivot(index=['Company'], columns='KPI', values='Value')\n\ndf_2019.reset_index(inplace=True)\n\ndf_2019 = df_2019.fillna(0)\n\n# df_2019.head(15)\n\nnumeric_cols = df_2019.columns[df_2019.dtypes != 'object']\nX_2019 = df_2019[numeric_cols]\n\ntarget_col = df_2019.columns[df_2019.dtypes == 'object']\nY = df_2019[target_col]\nY , _ = pd.factorize(Y.Company)\nY_2019 = Y.astype(int)\n\n# Standardize the data\nscaler = StandardScaler()\nX_2019 = scaler.fit_transform(X_2019)\n\nn_components = 9\n\npca = PCA(n_components = n_components)\nX1_2019 = pca.fit_transform(X_2019)\n\nresults_2019 = pd.DataFrame({'X1': X1_2019[:, 0], 'X2': X1_2019[:, 1], 'Y': Y_2019})\n\nresults_2019.head(5).to_csv('../../data/02-visualization-data/results_2019.csv', index=False)\n\n\n\n\n\n\n\nX1\nX2\nY\n\n\n\n\n-3.656204\n-0.0398362\n0\n\n\n-2.706101\n-0.0844159\n1\n\n\n12.248762\n10.7453347\n2\n\n\n14.219546\n-9.4016094\n3\n\n\n-3.637187\n0.4928705\n4\n\n\n\n\n\n\n\n{python}\ndf_2022 = df[(df['Year'] == 2022) & (df['Quarter'] == 4)]\n\ndf_2022 = df_2022.pivot(index=['Company'], columns='KPI', values='Value')\n\ndf_2022.reset_index(inplace=True)\n\ndf_2022 = df_2022.fillna(0)\n\n#df_2022.head(15)\n\nnumeric_cols = df_2022.columns[df_2022.dtypes != 'object']\nX_2022 = df_2022[numeric_cols]\n\ntarget_col = df_2022.columns[df_2022.dtypes == 'object']\nY = df_2022[target_col]\nY , _ = pd.factorize(Y.Company)\nY_2022 = Y.astype(int)\n\n# Standardize the data\nscaler = StandardScaler()\nX_2022 = scaler.fit_transform(X_2022)\n\nn_components = 9\n\npca = PCA(n_components = n_components)\nX1_2022 = pca.fit_transform(X_2022)\n\nresults_2022 = pd.DataFrame({'X1': X1_2022[:, 0], 'X2': X1_2022[:, 1], 'Y': Y_2022})\n\nresults_2022.head(5).to_csv('../../data/02-visualization-data/results_2022.csv', index=False)\n\n\n\n\n\n\n\nX1\nX2\nY\n\n\n\n\n-3.138441\n-1.1104079\n0\n\n\n-2.478826\n-0.6589369\n1\n\n\n20.455135\n-4.4606098\n2\n\n\n4.981983\n11.7004406\n3\n\n\n-2.871790\n-1.4558437\n4"
  },
  {
    "objectID": "7_clustering.html#hyper-parameter-tuning",
    "href": "7_clustering.html#hyper-parameter-tuning",
    "title": "Data Clustering",
    "section": "Hyper-parameter tuning",
    "text": "Hyper-parameter tuning\n\nAs mentioned earlier, hyperparameter tuning is a technique in some machine learning algorithms, such as clustering, where the goal is to optimize the parameters to improve performance. This can be done by iterating different parameters and comparing the results in terms of performance metrics. The hyperparameter we are tuning for clustering is the number of clusters used to group the data points.\nFor this analysis, we created a function that aims to maximize the silhouette metric. After running the function on the 2019 and 2022 data and the k-means model, we can see that the optimal number of clusters is 2. When running the function for the DBSCAN, there is a warning, and the output is not displayed. After some research, we find that this is happening because this type of model does not assign outlier values to clusters, causing that the number of clusters identified is equal to 1. The function allows to assess the maximum silhouette for a minimum of two clusters. After running the function using the Hierarchical clustering model, we observe that for both 2019 and 2022 the optimal number of clusters is 2.\nTherefore, based on the results we obtained from evaluating the silhouette function for each clustering method we can proceed to run the final models.\n\n\n\nFunction maximize_silhouette()\ndef maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X)\n\n    # LOOP OVER HYPER-PARAM\n    params=[]; \n    sil_scores=[]\n    sil_max=-10\n\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.25*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue\n\n        if(i_print): print(param,sil_scores[-1])\n\n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")\n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\nK-MEANS\n\n\n{python}\nopt_labels_kmeans = maximize_silhouette(X1_2019, algo=\"kmeans\", nmax=9, i_plot=True)\n\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\n\n\n{python}\nopt_labels_kmeans = maximize_silhouette(X1_2022, algo=\"kmeans\", nmax=9, i_plot=True)\n\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\nDBSCAN\n\n\n{python}\n#opt_labels_dbscan = maximize_silhouette(X1_2019, algo=\"dbscan\", nmax=9, i_plot=True)\n\n\n\n\n{python}\n#opt_labels_dbscan = maximize_silhouette(X1_2022, algo=\"dbscan\", nmax=9, i_plot=True)\n\n\nHierarchical\n\n\n{python}\nopt_labels_ag = maximize_silhouette(X1_2019, algo=\"ag\", nmax=9, i_plot=True)\n\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\n\n\n{python}\nopt_labels_ag = maximize_silhouette(X1_2022, algo=\"ag\", nmax=9, i_plot=True)\n\n\nOPTIMAL PARAMETER = 2"
  },
  {
    "objectID": "7_clustering.html#final-models",
    "href": "7_clustering.html#final-models",
    "title": "Data Clustering",
    "section": "Final Models",
    "text": "Final Models\n\nBelow, we can find the plots for each clustering method and for each dataset being analyzed. In the results section we will analyze the visualizations and insights we can observe.\n\nK-MEANS\n\n\n{python}\nclusters_kmeans = 2\n\nmodel_kmeans = sklearn.cluster.KMeans(n_clusters=clusters_kmeans).fit(X1_2019)\n\nlabels_kmeans = model_kmeans.predict(X1_2019)\n\nplot(X1_2019,labels_kmeans)\n\n\n\n\n\n\n\n{python}\nclusters_kmeans = 2\n\nmodel_kmeans = sklearn.cluster.KMeans(n_clusters=clusters_kmeans).fit(X1_2022)\n\nlabels_kmeans = model_kmeans.predict(X1_2022)\n\nplot(X1_2022, labels_kmeans)\n\n\n\n\n\nDBSCAN\n\n\n{python}\nmodel_DBSCAN = sklearn.cluster.DBSCAN(eps=2).fit(X1_2019)\n\nlabels_DBSCAN = model_DBSCAN.labels_\n\nplot(X1_2019, labels_DBSCAN)\n\n\n\n\n\n\n\n{python}\nmodel_DBSCAN = sklearn.cluster.DBSCAN(eps=2).fit(X1_2022)\n\nlabels_DBSCAN = model_DBSCAN.labels_\n\nplot(X1_2022, labels_DBSCAN)\n\n\n\n\n\nHIERARCHICAL\n\n\n{python}\nclusters_ag = 2\n\nmodel_ag = sklearn.cluster.AgglomerativeClustering(n_clusters=clusters_ag).fit(X1_2019)\n\nlabels_ag = model_ag.labels_\n\nplot(X1_2019, labels_ag)\n\n\n\n\n\n\n\n{python}\nclusters_ag = 2\n\nmodel_ag = sklearn.cluster.AgglomerativeClustering(n_clusters=clusters_ag).fit(X1_2022)\n\nlabels_ag = model_ag.labels_\n\nplot(X1_2022, labels_ag)"
  },
  {
    "objectID": "5_NaiveBayes.html",
    "href": "5_NaiveBayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "’, sep = “”)"
  }
]