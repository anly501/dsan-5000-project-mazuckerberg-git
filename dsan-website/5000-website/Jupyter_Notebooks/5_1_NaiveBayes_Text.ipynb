{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "## News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mariaagustinazuckerberg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk;\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download([\n",
    "    \"names\",\n",
    "    \"stopwords\",\n",
    "    \"state_union\",\n",
    "    \"twitter_samples\",\n",
    "    \"movie_reviews\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"vader_lexicon\",\n",
    "    \"punkt\",])\n",
    "\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Verge</td>\n",
       "      <td>technology</td>\n",
       "      <td>0.6783</td>\n",
       "      <td>positive</td>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>Redwood Materials will recycle stationary stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>technology</td>\n",
       "      <td>0.2886</td>\n",
       "      <td>positive</td>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>Enlarge/ GreenPower has given its class-D elec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>technology</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2023-11-13</td>\n",
       "      <td>Enlarge/ These are piles of lithium harvested ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Insider</td>\n",
       "      <td>business</td>\n",
       "      <td>-0.5678</td>\n",
       "      <td>negative</td>\n",
       "      <td>2023-11-03</td>\n",
       "      <td>BP's EV charging arm has bought $100 million w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Time</td>\n",
       "      <td>general</td>\n",
       "      <td>-0.7521</td>\n",
       "      <td>negative</td>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>A lithium-ion battery recycler and a program t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>The Verge</td>\n",
       "      <td>technology</td>\n",
       "      <td>-0.9134</td>\n",
       "      <td>negative</td>\n",
       "      <td>2023-11-16</td>\n",
       "      <td>The effort to climate-proof our housing is run...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>general</td>\n",
       "      <td>-0.5973</td>\n",
       "      <td>negative</td>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>ROME, Nov 6 (Reuters) - Italy's Industrie De N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>general</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2023-11-13</td>\n",
       "      <td>WASHINGTON, Nov 13 (Reuters) - Democratic Sena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Engadget</td>\n",
       "      <td>technology</td>\n",
       "      <td>-0.6099</td>\n",
       "      <td>negative</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>Your EV may go a long way between charges, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Politico</td>\n",
       "      <td>general</td>\n",
       "      <td>0.5868</td>\n",
       "      <td>positive</td>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>The administration is well aware of the high s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                name    category  sentiment     label        date  \\\n",
       "0          The Verge  technology     0.6783  positive  2023-11-07   \n",
       "1       Ars Technica  technology     0.2886  positive  2023-10-31   \n",
       "2       Ars Technica  technology     0.0000   neutral  2023-11-13   \n",
       "3   Business Insider    business    -0.5678  negative  2023-11-03   \n",
       "4               Time     general    -0.7521  negative  2023-11-07   \n",
       "..               ...         ...        ...       ...         ...   \n",
       "80         The Verge  technology    -0.9134  negative  2023-11-16   \n",
       "81           Reuters     general    -0.5973  negative  2023-11-06   \n",
       "82           Reuters     general     0.0000   neutral  2023-11-13   \n",
       "83          Engadget  technology    -0.6099  negative  2023-11-01   \n",
       "84          Politico     general     0.5868  positive  2023-11-06   \n",
       "\n",
       "                                                 text  \n",
       "0   Redwood Materials will recycle stationary stor...  \n",
       "1   Enlarge/ GreenPower has given its class-D elec...  \n",
       "2   Enlarge/ These are piles of lithium harvested ...  \n",
       "3   BP's EV charging arm has bought $100 million w...  \n",
       "4   A lithium-ion battery recycler and a program t...  \n",
       "..                                                ...  \n",
       "80  The effort to climate-proof our housing is run...  \n",
       "81  ROME, Nov 6 (Reuters) - Italy's Industrie De N...  \n",
       "82  WASHINGTON, Nov 13 (Reuters) - Democratic Sena...  \n",
       "83  Your EV may go a long way between charges, but...  \n",
       "84  The administration is well aware of the high s...  \n",
       "\n",
       "[85 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../../data/01-modified-data/clean_sentiment_analysis.csv')\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df = df.rename(columns={'ibm_label': 'label', 'ibm_content': 'text', 'ibm_score': 'sentiment'})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text=\"\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for character in df.at[index, 'text']:\n",
    "        if character in string.printable:\n",
    "            new_text+=character\n",
    "    df.at[index, 'text'] = new_text\n",
    "    new_text=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0 : label = positive\n",
      "index = 1 : label = neutral\n",
      "index = 2 : label = negative\n",
      "number of text chunks =  85\n",
      "['Redwood Materials will recycle stationary storage batteries as it expands its scope\\r\\nRedwood Materials will recycle stationary storage batteries as it expands its scope\\r\\n / The company is recycling b [+3842 chars]', 'Enlarge/ GreenPower has given its class-D electric school bus a big battery bump.\\r\\n39 with \\r\\nOn Tuesday morning, the West Virginia-based GreenPower Motor Company debuted its latest electric vehicle.  [+4366 chars]', \"Enlarge/ These are piles of lithium harvested in Bolivia; Exxon's site in Arkansas will look almost entirely unlike this as it will use direct lithium extraction, not evaporation, to harvest the mine [+1611 chars]\"]\n"
     ]
    }
   ],
   "source": [
    "#CONVERT FROM STRING LABELS TO INTEGERS \n",
    "labels=[]; #y1=[]; y2=[]\n",
    "y1=[]\n",
    "for label in df[\"label\"]:\n",
    "    if label not in labels:\n",
    "        labels.append(label)\n",
    "        print(\"index =\",len(labels)-1,\": label =\",label)\n",
    "    for i in range(0,len(labels)):\n",
    "        if(label==labels[i]):\n",
    "            y1.append(i)\n",
    "y1=np.array(y1)\n",
    "\n",
    "# CONVERT DF TO LIST OF STRINGS \n",
    "corpus=df[\"text\"].to_list()\n",
    "y2=df[\"sentiment\"].to_numpy()\n",
    "\n",
    "print(\"number of text chunks = \",len(corpus))\n",
    "print(corpus[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 1184) (85,) (85,)\n",
      "DATA POINT-0: [0 0 0 0 0 0 0 0 0 0] y1 = 0   y2 = 0.6783\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE COUNT VECTORIZER\n",
    "# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n",
    "# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "vectorizer=CountVectorizer(min_df=0.0001)   \n",
    "\n",
    "# RUN COUNT VECTORIZER ON OUR COURPUS \n",
    "Xs  =  vectorizer.fit_transform(corpus)   \n",
    "X = np.array(Xs.todense())\n",
    "\n",
    "#CONVERT TO ONE-HOT VECTORS\n",
    "# maxs=np.max(X,axis=0)\n",
    "# X=np.ceil(X/maxs)\n",
    "\n",
    "# DOUBLE CHECK \n",
    "print(X.shape,y1.shape,y2.shape)\n",
    "print(\"DATA POINT-0:\",X[0,0:10],\"y1 =\",y1[0],\"  y2 =\",y2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mariaagustinazuckerberg/Documents/Georgetown University/1. Fall 2023/DSAN 5000/dsan-5000-project-mazuckerberg-git/dsan-website/5000-website/4_1_NaiveBayes_R.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mariaagustinazuckerberg/Documents/Georgetown%20University/1.%20Fall%202023/DSAN%205000/dsan-5000-project-mazuckerberg-git/dsan-website/5000-website/4_1_NaiveBayes_R.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mariaagustinazuckerberg/Documents/Georgetown%20University/1.%20Fall%202023/DSAN%205000/dsan-5000-project-mazuckerberg-git/dsan-website/5000-website/4_1_NaiveBayes_R.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_ratio\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mariaagustinazuckerberg/Documents/Georgetown%20University/1.%20Fall%202023/DSAN%205000/dsan-5000-project-mazuckerberg-git/dsan-website/5000-website/4_1_NaiveBayes_R.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m x_train, x_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y1, test_size\u001b[39m=\u001b[39mtest_ratio, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mariaagustinazuckerberg/Documents/Georgetown%20University/1.%20Fall%202023/DSAN%205000/dsan-5000-project-mazuckerberg-git/dsan-website/5000-website/4_1_NaiveBayes_R.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_train\u001b[39m=\u001b[39my_train\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mariaagustinazuckerberg/Documents/Georgetown%20University/1.%20Fall%202023/DSAN%205000/dsan-5000-project-mazuckerberg-git/dsan-website/5000-website/4_1_NaiveBayes_R.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_test\u001b[39m=\u001b[39my_test\u001b[39m.\u001b[39mflatten()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#INSERT CODE TO PARTITION DATASET INTO TRAINING-TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "test_ratio=0.2\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y1, test_size=test_ratio, random_state=0)\n",
    "y_train=y_train.flatten()\n",
    "y_test=y_test.flatten()\n",
    "\n",
    "print(\"x_train.shape\t\t:\",x_train.shape)\n",
    "print(\"y_train.shape\t\t:\",y_train.shape)\n",
    "\n",
    "print(\"X_test.shape\t\t:\",x_test.shape)\n",
    "print(\"y_test.shape\t\t:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y,ypred):\n",
    "      #ACCURACY COMPUTE \n",
    "      print(\"Accuracy:\",accuracy_score(y, ypred)*100)\n",
    "      print(\"Number of mislabeled points out of a total %d points = %d\"\n",
    "            % (y.shape[0], (y != ypred).sum()))\n",
    "\n",
    "def print_model_summary():\n",
    "      # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n",
    "      yp_train = model.predict(x_train)\n",
    "      yp_test = model.predict(x_test)\n",
    "\n",
    "      print(\"ACCURACY CALCULATION\\n\")\n",
    "\n",
    "      print(\"TRAINING SET:\")\n",
    "      report(y_train,yp_train)\n",
    "\n",
    "      print(\"\\nTEST SET (UNTRAINED DATA):\")\n",
    "      report(y_test,yp_test)\n",
    "\n",
    "      print(\"\\nCHECK FIRST 20 PREDICTIONS\")\n",
    "      print(\"TRAINING SET:\")\n",
    "      print(y_train[0:20])\n",
    "      print(yp_train[0:20])\n",
    "      print(\"ERRORS:\",yp_train[0:20]-y_train[0:20])\n",
    "\n",
    "      print(\"\\nTEST SET (UNTRAINED DATA):\")\n",
    "      print(y_test[0:20])\n",
    "      print(yp_test[0:20])\n",
    "      print(\"ERRORS:\",yp_test[0:20]-y_test[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# INITIALIZE MODEL \n",
    "model = MultinomialNB()\n",
    "\n",
    "# TRAIN MODEL \n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "# PRINT REPORT USING UTILITY FUNCTION ABOVE\n",
    "print_model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "#OUTPUT FOLDER: START FRESH (DELETE OLD ONE IF EXISTS)\n",
    "output_dir = \"output\"\n",
    "if os.path.exists(output_dir) and os.path.isdir(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=[]\n",
    "y=[]\n",
    "#ITERATE OVER ROWS\n",
    "# for i in range(0,10):  \n",
    "for i in range(0,df.shape[0]):\n",
    "    # QUICKLY CLEAN TEXT\n",
    "    keep=\"abcdefghijklmnopqrstuvwxyz \"\n",
    "    replace=\".,!;\"\n",
    "    tmp=\"\"\n",
    "    for char in df[\"text\"][i].replace(\"<br />\",\"\").lower():\n",
    "        if char in replace:\n",
    "            tmp+=\" \"\n",
    "        if char in keep:\n",
    "            tmp+=char\n",
    "    tmp=\" \".join(tmp.split())\n",
    "    reviews.append(tmp)\n",
    "    # CONVERT STRINGS TO INT TAGS\n",
    "    if(df[\"label\"][i]==\"positive\"):\n",
    "        y.append(1)\n",
    "    if(df[\"label\"][i]==\"negative\"):\n",
    "        y.append(-1)\n",
    "    if(df[\"label\"][i]==\"neutral\"):\n",
    "        y.append(0)\n",
    "\n",
    "    #PRINT FIRST COUPLE REVIEWS\n",
    "    if(i<3):\n",
    "        print(i)\n",
    "        print(df[\"text\"][i].replace(\"<br />\",\"\"),'\\n')\n",
    "        print(tmp)\n",
    "        print(df[\"label\"][i],y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reviews),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def vectorize(corpus,MAX_FEATURES):\n",
    "    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n",
    "    # RUN COUNT VECTORIZER ON OUR COURPUS \n",
    "    Xs  =  vectorizer.fit_transform(corpus)   \n",
    "    X=np.array(Xs.todense())\n",
    "    #CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n",
    "    maxs=np.max(X,axis=0)\n",
    "    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n",
    "\n",
    "(x,vocab0)=vectorize(reviews,MAX_FEATURES=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swap keys and values (value --> ley)\n",
    "vocab1 = dict([(value, key) for key, value in vocab0.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK VOCAB KEY-VALUE PAIRS\n",
    "print(list(vocab1.keys())[0:10])\n",
    "print(list(vocab1.values())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK TO SEE IF COUNT-VECT COLUMNS ARE SORTED BY OCCURRENCE \n",
    "print(x.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RE-ORDER COLUMN SO IT IS SORTED FROM HIGH FREQ TERMS TO LOW \n",
    "# https://stackoverflow.com/questions/60758625/sort-pandas-dataframe-by-sum-of-columns\n",
    "df2=pd.DataFrame(x)\n",
    "s = df2.sum(axis=0)\n",
    "df2=df2[s.sort_values(ascending=False).index[:]]\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMAP DICTIONARY TO CORRESPOND TO NEW COLUMN NUMBERS\n",
    "print()\n",
    "i1=0\n",
    "vocab2={}\n",
    "for i2 in list(df2.columns):\n",
    "    # print(i2)\n",
    "    vocab2[i1]=vocab1[int(i2)]\n",
    "    i1+=1\n",
    "\n",
    "#DOUBLE CHECK \n",
    "print(vocab2[0],vocab1[94])\n",
    "print(vocab2[1],vocab1[186])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAME COLUMNS 0,1,2,3 .. \n",
    "df2.columns = range(df2.columns.size)\n",
    "print(df2.head())\n",
    "print(df2.sum(axis=0))\n",
    "x=df2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOUBLE CHECK \n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "N=x.shape[0]\n",
    "l = [*range(N)]     # indices\n",
    "cut = int(0.8 * N) #80% of the list\n",
    "random.shuffle(l)   # randomize\n",
    "train_index = l[:cut] # first 80% of shuffled list\n",
    "test_index = l[cut:] # last 20% of shuffled list\n",
    "\n",
    "print(train_index[0:10])\n",
    "print(test_index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "def train_MNB_model(X,Y,i_print=False):\n",
    "\n",
    "    if(i_print):\n",
    "        print(X.shape,Y.shape)\n",
    "\n",
    "    #SPLIT\n",
    "    x_train=X[train_index]\n",
    "    y_train=Y[train_index].flatten()\n",
    "\n",
    "    x_test=X[test_index]\n",
    "    y_test=Y[test_index].flatten()\n",
    "\n",
    "    # INITIALIZE MODEL \n",
    "    model = MultinomialNB()\n",
    "\n",
    "    # TRAIN MODEL \n",
    "    start = time.process_time()\n",
    "    model.fit(x_train,y_train)\n",
    "    time_train=time.process_time() - start\n",
    "\n",
    "    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n",
    "    start = time.process_time()\n",
    "    yp_train = model.predict(x_train)\n",
    "    yp_test = model.predict(x_test)\n",
    "    time_eval=time.process_time() - start\n",
    "\n",
    "    acc_train= accuracy_score(y_train, yp_train)*100\n",
    "    acc_test= accuracy_score(y_test, yp_test)*100\n",
    "\n",
    "    if(i_print):\n",
    "        print(acc_train,acc_test,time_train,time_eval)\n",
    "\n",
    "    return (acc_train,acc_test,time_train,time_eval)\n",
    "\n",
    "\n",
    "#TEST\n",
    "print(type(x),type(y))\n",
    "print(x.shape,y.shape)\n",
    "(acc_train,acc_test,time_train,time_eval)=train_MNB_model(x,y,i_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_arrays():\n",
    "    global num_features,train_accuracies\n",
    "    global test_accuracies,train_time,eval_time\n",
    "    num_features=[]\n",
    "    train_accuracies=[]\n",
    "    test_accuracies=[]\n",
    "    train_time=[]\n",
    "    eval_time=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE ARRAYS\n",
    "initialize_arrays()\n",
    "\n",
    "# DEFINE SEARCH FUNCTION\n",
    "def partial_grid_search(num_runs, min_index, max_index):\n",
    "    for i in range(1, num_runs+1):\n",
    "        # SUBSET FEATURES \n",
    "        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n",
    "        xtmp=x[:,0:upper_index]\n",
    "\n",
    "        #TRAIN \n",
    "        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n",
    "\n",
    "        if(i%5==0):\n",
    "            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n",
    "            \n",
    "        #RECORD \n",
    "        num_features.append(xtmp.shape[1])\n",
    "        train_accuracies.append(acc_train)\n",
    "        test_accuracies.append(acc_test)\n",
    "        train_time.append(time_train)\n",
    "        eval_time.append(time_eval)\n",
    "\n",
    "# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\n",
    "partial_grid_search(num_runs=100, min_index=0, max_index=1000)\n",
    "\n",
    "# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\n",
    "partial_grid_search(num_runs=20, min_index=1000, max_index=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(path_root):\n",
    "    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n",
    "    out=pd.DataFrame(out)\n",
    "    out.to_csv(path_root+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILITY FUNCTION TO PLOT RESULTS\n",
    "def plot_results(path_root):\n",
    "\n",
    "    #PLOT-1\n",
    "    plt.plot(num_features,train_accuracies,'-or')\n",
    "    plt.plot(num_features,test_accuracies,'-ob')\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n",
    "    plt.savefig(path_root+'-1.png')\n",
    "    plt.show()\n",
    "\n",
    "    # #PLOT-2\n",
    "    plt.plot(num_features,train_time,'-or')\n",
    "    plt.plot(num_features,eval_time,'-ob')\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n",
    "    plt.savefig(path_root+'-2.png')\n",
    "    plt.show()\n",
    "\n",
    "    # #PLOT-3\n",
    "    plt.plot(np.array(test_accuracies),train_time,'-or')\n",
    "    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n",
    "    plt.xlabel('test_accuracies')\n",
    "    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n",
    "    plt.savefig(path_root+'-3.png')\n",
    "    plt.show()\n",
    "\n",
    "    # #PLOT-3\n",
    "    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('train_accuracies-test_accuracies')\n",
    "    plt.savefig(path_root+'-4.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(output_dir+\"/partial_grid_search\")\n",
    "plot_results(output_dir+\"/partial_grid_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var=np.var(x,axis=0)\n",
    "print(np.min(x_var))\n",
    "print(np.max(x_var))\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# DEFINE GRID OF THRESHOLDS \n",
    "num_thresholds=30\n",
    "thresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n",
    "\n",
    "#DOESN\"T WORK WELL WITH EDGE VALUES \n",
    "thresholds=thresholds[1:-2]; #print(thresholds)\n",
    "\n",
    "# INITIALIZE ARRAYS\n",
    "initialize_arrays()\n",
    "\n",
    "# SEARCH FOR OPTIMAL THRESHOLD\n",
    "for THRESHOLD in thresholds:\n",
    "    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n",
    "    xtmp=feature_selector.fit_transform(x)\n",
    "    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n",
    "\n",
    "    (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n",
    "             \n",
    "    #RECORD \n",
    "    num_features.append(xtmp.shape[1])\n",
    "    train_accuracies.append(acc_train)\n",
    "    test_accuracies.append(acc_test)\n",
    "    train_time.append(time_train)\n",
    "    eval_time.append(time_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(output_dir+\"/variance_threshold\")\n",
    "plot_results(output_dir+\"/variance_threshold\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
