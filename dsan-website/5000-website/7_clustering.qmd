---
title: "Data Clustering"
format:
  html:
    embed-resources: true
bibliography: ../Personal/reference.bib
---

```{css, echo = FALSE}
.justify {
    text-align: justify !important;
    text-indent: 20px; 
}

.epigrafe {
    text-align: justify !important;
    text-indent: 20px; 
    border: 1.5px solid #87c8b5; 
    padding-top: 15px;
    padding-bottom: 5px;
    padding-right: 15px;
    padding-left: 15px;
    font-size: 14px;
    background-color: #f9f9f9; 
    margin: 20px 0px 30px 0px;
}

h2 {
  font-size: 28px; 
  color: #FFFFFF; 
  background-color: #87c8b5;
  padding: 10px; 
  border-radius: 8px;
}
```

``` {r}
#| echo: false
#| message: false
#| results: 'hide'
#| warning: false

x <- 1

```

## Introduction
::: {.justify}

The purpose of this section is to evaluate the financial information for the lithium companies and understand how they relate to each other. As mentioned in the previous sections, the data sets consist of three types of companies, with each group differing in their manufacturing process. The companies specialize in lithium production, lithium battery production, and electric vehicle production.

All companies in the lithium industry have grown actively in recent years. However, their performance is correlated with the type of production they are developing. 

Clustering analysis allows us to group similar companies together to evaluate their relationship and discover patterns over time. We also want to find the most important outliers and how they have changed in recent years to assess the main reasons for these changes.
:::

Below there is the list of all the packages and libraries that were imported to do the analysis:

```{python}
#| echo: true
#| message: false
#| results: 'hide'
#| warning: false
#| code-fold: true
#| code-summary: "Libraries"

# Import Data
import pandas as pd

# Data Cleaning
import numpy as np
from sklearn.preprocessing import StandardScaler

# PCA analysis
from sklearn.decomposition import PCA

# t-SNE analysis
from sklearn.manifold import TSNE

# Visualize Results
import matplotlib.pyplot as plt

# Clustering
import matplotlib.pyplot as plt
import sklearn
import sklearn.cluster
```

```{python}
#| echo: false
#| message: false
#| warning: false

# UTILITY PLOTTING FUNCTION
def plot(X,color_vector):
    fig, ax = plt.subplots()
    ax.scatter(X[:,0], X[:,1],c=color_vector, alpha=0.5) #, c=y
    ax.set(xlabel='Feature-1 (x_1)', ylabel='Feature-2 (x_2)',
    title='Cluster data')
    ax.grid()
    # fig.savefig("test.png")
    plt.show()
```


## Theory
::: {.justify}

The machine learning clustering algorithms that will be used in this section are K-Means, Density-based spatial clustering of applications with noise (DBSCAN) and Hierarchical or Agglomerative Clustering. These algorithms have a similar output but the method to get the result is different.

To create the k-means model that will be applied to the analysis, we will use the k-means clustering function from the Sklearn package. The most important parameter for creating this model is the number of clusters. The first centroids are randomly located in the resulting space, and the algorithms calculate the distance of each point to the nearest algorithm that assigns this new label. After a certain number of iterations, they will converge, resulting in no significant difference. To choose the optimal number of clusters for this method, we will use the silhoutte metric explained in the hyperparameter tuning section.

To create the DBSCAN model, we will use the dbscan function from the Sklearn package. DBSCAN model groups the data point by density in the space. Similarly to k-means, in every iteration the distance from the points to the centroids is calculated in order to assign the points to the clusters. The function has a different parameter, eps, which is the calculation of the number of clusters: $eps = 0.25$ x $(clusters - 1)$.
This model do not assign outlier points to the clusters which this is a disadvantage.

To create the hierarchical clustering, we will use the agglomerative function from the Sklearn package. This model focuses on creating a hierarchy of clusters and merging them on each iteration. 

There are two methods we can use to evaluate the performance of the models and make decisions about improvements through the hyperparameter tuning technique. For k-means, we can use the elbow method, which consists of determining the optimal number of clusters by observing the inflection point. This means that the marginal value is close to 0, since adding one more cluster does not make a significant difference in performance to justify the complexity. The second method is the silhouette metric, which evaluates the quality of the clusters and the goal is to find the number of clusters that maximizes this value while minimizing the complexity.
:::


## Methods
::: {.justify}

There are several steps to the methodology for this section. The first is to select the data to be used for the clustering analysis. This data should not contain any labels for this analysis, as we want to find a relationship between the companies without knowing the actual relationship.

The next step is to find the optimal number of clusters per algorithm in the hyperparameter tuning. For this analysis we use a function developed during the course, where the input parameters are the data itself, the algorithm to be evaluated, the maximum number of clusters, and a boolean parameter to show the output or not. This step is explained in detail in the Hyperparameter Tuning section. With the optimal number of clusters, we will apply the final model to the data and plot the results to find insights and draw conclusions.
:::

## Data selection
::: {.justify}

For this analysis, we will use the financial data of lithium companies. Considering the high number of features in this dataset, we decide to use the output dataset obtained from the dimensionality reduction analysis. This is very helpful because we have reduced the variability while maintaining the quality of the data.

:::

**Companies Financial Data**

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

# Read csv
df = pd.read_csv('../../data/01-modified-data/clean_companies_finance.csv')

df_2019 = df[(df['Year'] == 2019) & (df['Quarter'] == 4)]

df_2019 = df_2019.pivot(index=['Company'], columns='KPI', values='Value')

df_2019.reset_index(inplace=True)

df_2019 = df_2019.fillna(0)

# df_2019.head(15)

numeric_cols = df_2019.columns[df_2019.dtypes != 'object']
X_2019 = df_2019[numeric_cols]

target_col = df_2019.columns[df_2019.dtypes == 'object']
Y = df_2019[target_col]
Y , _ = pd.factorize(Y.Company)
Y_2019 = Y.astype(int)

# Standardize the data
scaler = StandardScaler()
X_2019 = scaler.fit_transform(X_2019)

n_components = 9

pca = PCA(n_components = n_components)
X1_2019 = pca.fit_transform(X_2019)

results_2019 = pd.DataFrame({'X1': X1_2019[:, 0], 'X2': X1_2019[:, 1], 'Y': Y_2019})

results_2019.head(5).to_csv('../../data/02-visualization-data/results_2019.csv', index=False)
```

``` {r}
#| echo: false

# Read csv file
results_2019 <- read.csv('../../data/02-visualization-data/results_2019.csv')

knitr::kable(head(results_2019, 5))
```

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

df_2022 = df[(df['Year'] == 2022) & (df['Quarter'] == 4)]

df_2022 = df_2022.pivot(index=['Company'], columns='KPI', values='Value')

df_2022.reset_index(inplace=True)

df_2022 = df_2022.fillna(0)

#df_2022.head(15)

numeric_cols = df_2022.columns[df_2022.dtypes != 'object']
X_2022 = df_2022[numeric_cols]

target_col = df_2022.columns[df_2022.dtypes == 'object']
Y = df_2022[target_col]
Y , _ = pd.factorize(Y.Company)
Y_2022 = Y.astype(int)

# Standardize the data
scaler = StandardScaler()
X_2022 = scaler.fit_transform(X_2022)

n_components = 9

pca = PCA(n_components = n_components)
X1_2022 = pca.fit_transform(X_2022)

results_2022 = pd.DataFrame({'X1': X1_2022[:, 0], 'X2': X1_2022[:, 1], 'Y': Y_2022})

results_2022.head(5).to_csv('../../data/02-visualization-data/results_2022.csv', index=False)
```

``` {r}
#| echo: false

# Read csv file
results_2022 <- read.csv('../../data/02-visualization-data/results_2022.csv')

knitr::kable(head(results_2022, 5))
```

## Hyper-parameter tuning
::: {.justify}

As mentioned earlier, hyperparameter tuning is a technique in some machine learning algorithms, such as clustering, where the goal is to optimize the parameters to improve performance. This can be done by iterating different parameters and comparing the results in terms of performance metrics. The hyperparameter we are tuning for clustering is the number of clusters used to group the data points.

For this analysis, we created a function that aims to maximize the silhouette metric. After running the function on the 2019 and 2022 data and the k-means model, we can see that the optimal number of clusters is 2. When running the function for the DBSCAN, there is a warning, and the output is not displayed. After some research, we find that this is happening because this type of model does not assign outlier values to clusters, causing that the number of clusters identified is equal to 1. The function allows to assess the maximum silhouette for a minimum of two clusters. After running the function using the Hierarchical clustering model, we observe that for both 2019 and 2022 the optimal number of clusters is 2. 

Therefore, based on the results we obtained from evaluating the silhouette function for each clustering method we can proceed to run the final models.
:::


```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "Function maximize_silhouette()"

def maximize_silhouette(X,algo="birch",nmax=20,i_plot=False):

    # PARAM
    i_print=False

    #FORCE CONTIGUOUS
    X=np.ascontiguousarray(X)

    # LOOP OVER HYPER-PARAM
    params=[]; 
    sil_scores=[]
    sil_max=-10

    for param in range(2,nmax+1):
        if(algo=="birch"):
            model = sklearn.cluster.Birch(n_clusters=param).fit(X)
            labels=model.predict(X)

        if(algo=="ag"):
            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)
            labels=model.labels_

        if(algo=="dbscan"):
            param=0.25*(param-1)
            model = sklearn.cluster.DBSCAN(eps=param).fit(X)
            labels=model.labels_

        if(algo=="kmeans"):
            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)
            labels=model.predict(X)

        try:
            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))
            params.append(param)
        except:
            continue

        if(i_print): print(param,sil_scores[-1])

        if(sil_scores[-1]>sil_max):
             opt_param=param
             sil_max=sil_scores[-1]
             opt_labels=labels

    print("OPTIMAL PARAMETER =",opt_param)

    if(i_plot):
        fig, ax = plt.subplots()
        ax.plot(params, sil_scores, "-o")
        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')
        plt.show()

    return opt_labels
```


**K-MEANS**

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

opt_labels_kmeans = maximize_silhouette(X1_2019, algo="kmeans", nmax=9, i_plot=True)
```

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

opt_labels_kmeans = maximize_silhouette(X1_2022, algo="kmeans", nmax=9, i_plot=True)
```

**DBSCAN**

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

#opt_labels_dbscan = maximize_silhouette(X1_2019, algo="dbscan", nmax=9, i_plot=True)
```

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

#opt_labels_dbscan = maximize_silhouette(X1_2022, algo="dbscan", nmax=9, i_plot=True)
```

**Hierarchical**

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

opt_labels_ag = maximize_silhouette(X1_2019, algo="ag", nmax=9, i_plot=True)
```

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

opt_labels_ag = maximize_silhouette(X1_2022, algo="ag", nmax=9, i_plot=True)
```

## Final Models
::: {.justify}

Below, we can find the plots for each clustering method and for each dataset being analyzed. In the results section we will analyze the visualizations and insights we can observe.
::: 


**K-MEANS**

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

clusters_kmeans = 2

model_kmeans = sklearn.cluster.KMeans(n_clusters=clusters_kmeans).fit(X1_2019)

labels_kmeans = model_kmeans.predict(X1_2019)

plot(X1_2019,labels_kmeans)
```

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

clusters_kmeans = 2

model_kmeans = sklearn.cluster.KMeans(n_clusters=clusters_kmeans).fit(X1_2022)

labels_kmeans = model_kmeans.predict(X1_2022)

plot(X1_2022, labels_kmeans)
```

**DBSCAN**

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

model_DBSCAN = sklearn.cluster.DBSCAN(eps=2).fit(X1_2019)

labels_DBSCAN = model_DBSCAN.labels_

plot(X1_2019, labels_DBSCAN)
```

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

model_DBSCAN = sklearn.cluster.DBSCAN(eps=2).fit(X1_2022)

labels_DBSCAN = model_DBSCAN.labels_

plot(X1_2022, labels_DBSCAN)
```

**HIERARCHICAL**

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

clusters_ag = 2

model_ag = sklearn.cluster.AgglomerativeClustering(n_clusters=clusters_ag).fit(X1_2019)

labels_ag = model_ag.labels_

plot(X1_2019, labels_ag)
```

```{python}
#| echo: true
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "{python}"

clusters_ag = 2

model_ag = sklearn.cluster.AgglomerativeClustering(n_clusters=clusters_ag).fit(X1_2022)

labels_ag = model_ag.labels_

plot(X1_2022, labels_ag)
```

## Results
::: {.justify}
The first clustering method was k-means for 2019 data. Here we can observe the two clusters that were identified. The yellow points are closer together compared to the purple ones which are far from the cluster and far from each other. Comparing this plot to the 2022 data plot we can observe there is one point that was classified to the other cluster. In addition, we can observe that the location of the clusters was shifted down. 

The second clustering method was DBSCAN applied for 2019 and 2022 data. As it was explained earlier, this clustering method is identifying only one cluster because there are two outlier points that cannot be classified as to a cluster. This approach is very useful for datasets that contain more that 2 clusters because outliers can be easily identified instead to be classified to a cluster. On the other hand, for this specific case, it was a disadvantage because there was waste of time while trying to troubleshoot the code.

The third method is Hierarchical clustering. When we apply the selected model to the two datasets, we can observe that the results are very similar to the k-means model, therefore the conclusions are the same. 

In gerenal terms, both k-means and Hierarchical clustering suggests that there is a clear difference in the financial performance of the companies during this years.
:::


## Conclusions
::: {.justify}
As a summary for the analysis done, we first selected the data we wanted to use, then we used the function to maximize the silhouette metric to find the optimal number of clusters for each model and finally plotted the results and found insights. 

Clustering can be very useful to identify similar data points without having existing labels. For this analysis we took an additional step increasing the complexity of the analysis as we did two different data sets with the goal to understand how the clusters have changed over time. This allows to get additional insights of the data analyzed.

In future analysis it would be useful to create a function that automatically builds visualizations that allow to compare the clusters over a certain period of time in order to understand how the relationship between the data changes.
:::

<!-- 
<br>

***


### References:

::: {#refs}
::: -->