{"title":"Deep Learning for TS","markdown":{"yaml":{"title":"Deep Learning for TS","format":{"html":{"embed-resources":true}}},"headingText":"Overview","containsRefs":false,"markdown":"\n\n```{css, echo = FALSE}\n.justify {\ntext-align: justify !important;\ntext-indent: 20px;\n}\n\n\n.epigrafe {\ntext-align: justify !important;\ntext-indent: 20px;\nborder: 1.5px solid #87c8b5;\npadding: 15px;\nfont-size: 14px;\nbackground-color: #f9f9f9;\nmargin: 20px 0px 30px 0px;\n}\n\nh2 {\n  font-size: 28px; \n  color: #FFFFFF; \n  background-color: #87c8b5;\n  padding: 10px; \n  border-radius: 8px;\n}\n```\n\n``` {r}\n#| echo: false\n#| message: false\n#| results: 'hide'\n#| warning: false\n\nx<-1\n\n```\n\n\n::: {.justify}\n\nThe goal of this section is to apply deep learning algorithms to the oil price monthly data in order to make predictions of what the price of oil will be in the future.\n\nThe libraries that will be used in this section are in the following list:\n:::\n\n``` {python}\n#| echo: true\n#| message: false\n#| code-fold: true\n#| code-summary: \"Libraries\"\n#| results: 'hide'\n#| warning: false\n\n# Import Data\nimport pandas as pd\n\n# Plotting\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# Keras\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom keras.layers import Dense, SimpleRNN, LSTM ,GRU\n```\n\n### Data\n\n::: {.epigrafe}\n\nFor this analysis we will use the monthly adjusted oil price for the last 22 years which we can observe in the next table. The will be later transformed to be saved as a np.array() with a similar format to the time series object used in past sections.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n# Read csv\ndf = pd.read_csv('./data/df_oil_price_monthly.csv')\n\ndf\n\ndf = df.rename(columns={\"date\": \"t\", \"adjusted\": \"y\"})\n\ndf = df[[\"t\",\"y\"]]\n\nprint(\"CHECK NA:\\n\",df.isna().sum())\n\nt=np.array([*range(0,df.shape[0])])\n\nx=np.array(df['y']).reshape(t.shape[0],1)\n\nfeature_columns=[0] # columns to use as features\n\ntarget_columns=[0]  # columns to use as targets\n```\n\n### Visualize Data\n\n::: {.epigrafe}\n\nWhen we plot the raw data, as we evaluated in previous sections, we can observe there is no clear trend over all the years and there is a lot of fluctuation.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o',alpha = 0.5)\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.show()\n```\n\n### Normalize Data\n\n::: {.epigrafe}\n\nIn order to apply the data points to the deep learning algorithms we transformed the data to be normalized. For this procedure we used the mean and standard deviation of the points. After normalizing the data, the output values are between -3 and 3.\n\nWe plotted the transformed data and we can observe that the shape of the time series is very similar.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\nprint(np.mean(x,axis=0).shape,np.std(x,axis=0).shape)\nx=(x-np.mean(x,axis=0))/np.std(x,axis=0)\nprint(x.shape)\n```\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n# visualize normalized data \nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o')\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.show()\n```\n\n### Split Data\n\n::: {.epigrafe}\n\nThe next step is to split the data into train and validation sets. To perform this split we use an 80% of the data points for the train set and a 20% of the data points for the validation set.\n\nIn the following plot we can observe the train set colored in orange and the validation set in blue.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\nsplit_fraction=0.8\ncut=int(split_fraction*x.shape[0]) \ntt=t[0:cut]; \nxt=x[0:cut]\ntv=t[cut:]; \nxv=x[cut:]\n```\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n# visualize normalized data \nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(tt, xt[:,i],'ro',alpha=0.25)\n    ax.plot(tt, xt[:,i],\"g-\")\nfor i in range(0,x.shape[1]):\n    ax.plot(tv, xv[:,i],'bo',alpha=0.25)\n    ax.plot(tv, xv[:,i],\"g-\")\nplt.show()\n```\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n    # verbose=True --> report and plot for debugging\n    # unique=True --> don't re-sample: \n    # x1,x2,x3 --> x4 then x4,x5,x6 --> x7 instead of x2,x3,x4 --> x5\n\n    # initialize \n    i_start=0; count=0; \n    \n    # initialize output arrays with samples \n    x_out=[]\n    y_out=[]\n    \n    # sequentially build mini-batch samples\n    while i_start+lookback+delay< x.shape[0]:\n        \n        # define index bounds\n        i_stop=i_start+lookback\n        i_pred=i_stop+delay\n        \n        # report if desired \n        if verbose and count<2: print(\"indice range:\",i_start,i_stop,\"-->\",i_pred)\n\n        # define arrays: \n        # method-1: buggy due to indexing from left \n        # numpy's slicing --> start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n        \n        # method-2: non-vectorized but cleaner\n        indices_to_keep=[]; j=i_stop\n        while  j>=i_start:\n            indices_to_keep.append(j)\n            j=j-step\n\n        # create mini-batch sample\n        xtmp=x[indices_to_keep,:]    # isolate relevant indices\n        xtmp=xtmp[:,feature_columns] # isolate desire features\n        ytmp=x[i_pred,target_columns]\n        x_out.append(xtmp); y_out.append(ytmp); \n        \n        # report if desired \n        if verbose and count<2: print(xtmp, \"-->\",ytmp)\n        if verbose and count<2: print(\"shape:\",xtmp.shape, \"-->\",ytmp.shape)\n\n        # PLOT FIRST SAMPLE IF DESIRED FOR DEBUGGING    \n        if verbose and count<2:\n            fig, ax = plt.subplots()\n            ax.plot(x,'b-')\n            ax.plot(x,'bx')\n            ax.plot(indices_to_keep,xtmp,'go')\n            ax.plot(i_pred*np.ones(len(target_columns)),ytmp,'ro')\n            plt.show()\n            \n        # UPDATE START POINT \n        if unique: i_start+=lookback \n        i_start+=1; count+=1\n        \n    return np.array(x_out),np.array(y_out)\n```\n\n### Form Mini Batches\n\n::: {.epigrafe}\n\nFor this approach we will use mini batches. This allows to look at an especific number of data points at the same time for every iteration. In this case, considering the large amount of data points, we selected a lookback value of 25.\n:::\n\n```{python}\n#| echo: true\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n# training\nL=25; S=1; D=1\nXt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n# validation\nXv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n```\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\nprint(\"training:\",Xt.shape,Yt.shape)\nprint(\"validation:\",Xv.shape,Yv.shape)\n```\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\nfrom sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\n\n# UTILITY FUNCTION\ndef regression_report(yt,ytp,yv,yvp):\n    print(\"---------- Regression report ----------\")\n    \n    print(\"TRAINING:\")\n    print(\" RMSE:\",mean_squared_error(yt,ytp)**(1/2))\n    print(\" MSE:\",mean_squared_error(yt,ytp))\n    print(\" MAE:\",mean_absolute_error(yt,ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt,ytp,'ro')\n    ax.plot(yt,yt,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Training data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot=1.0\n    upper=int(frac_plot*yt.shape[0]); \n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper],'b-')\n    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t (blue=actual & red=prediction)', title='Training: Time-series prediction')\n    plt.show()\n\n      \n    print(\"VALIDATION:\")\n    print(\" RMSE:\",mean_squared_error(yv,yvp)**(1/2))\n    print(\" MSE:\",mean_squared_error(yv,yvp))\n    print(\" MAE:\",mean_absolute_error(yv,yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT \n    fig, ax = plt.subplots()\n    ax.plot(yv,yvp,'ro')\n    ax.plot(yv,yv,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Validation data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper=int(frac_plot*yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper],'b-')\n    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n    plt.show()\n\n```\n\n<!-- ### Utility Function -->\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS \n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\", color=\"green\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\", color=\"orange\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n```\n\n### Keras - RNN\n\n::: {.epigrafe}\n\nThe first model we fitted is RNN using the functions from Keras in the tensorflow package. \n\nIn the **Training and validation loss** plot we can observe that the training errors / loss reduces when the number of Epochs increases. In the case of the validation errors, they fluctuate a lot. This suggests that the model is not performing correctly and is probably overfitting.\n\nWhen we compare the RMSE for the Training and Validation sets, these are not very different but it does not mean that the model is predicting correctly.\n\nWe can compare the Validation data plot and the traning data plot and clearly see that the training values correlate better with the line, while the validation values do not.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n#batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n```\n\n\n### Keras - GRU\n\n::: {.epigrafe}\n\nIn the case of the GRU model created from the Keras function from the Tensorflow package, we obtained very different results for each analysis.\n\nThe training and validation loss plot shows that both the training loss and validation loss decreases to very similar values as the number of Epochs increases.\n\nConsidering that the trend of the validation loss values is very similar to the training trend, we can conclude that the RMSE of both sets is very low. This suggests that the model has an optimal fit.\n\nThe validation data plot and predictions, compared to the training plots, these look very similar compared to the real data values.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n```\n\n### Keras - LSTM\n\n::: {.epigrafe}\n\nThe last model created was LSTM, also obtained from the Keras package within Tensorflow.\n\nBy assessing the traning and validation loss plot, we can observe that this model is a midpoint between the two previously assessed. There is a los of fluctuation of the loss values but the value is still low. If we see more in detail we can clearly see that the trend is increasing. This suggests that with an increasing number of Epochs, for the higher values, the loss value in high.\n\nWhen we compare the RMSE for the training set and the test set, we can see there is a high difference and the trend suggests it will increase.\n\nThe predictions observed in the Validation plots are kind of good, but these can change in the future forests. This model may tend to overfitting if we increase the number of points being forecasted.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n```\n\n\n### Conclusions\n\n::: {.epigrafe}\n\nAs a conclusion, we need to evaluate the three models from different perspectives to undestand which model is performing better.\n\nWhen we compare the 3 models we can see that 2 of them are overfitting, RNN and LSTM, while the GRU is not. Therefore, from this point of view we conclude that the **train-test sets** are good for the model fitting.\n\nIn order to assess which model is better in terms of **predictive power,** we use the RMSE metric. The model that has the lower RMSE value is the GRU.\n\nWe modified the **regularization** parameter when training the models. This value was set by default in 0, which means there is no regularization at all. When we increase this value, the RMSE values are being higher compared to the default value. For that main reason, we decided to keep the regularization parameter as 0.\n\nTo assess how far into the future can the deep learning model accurately predict the future depends on the model that is being used. In terms of computationally intensive, with a lot of data points this predictions can take very long which that would not be very efficient. Also, as we have evaluated previously, for the LSTM model, the loss function, RMSE metric, was increasing over time, this suggests that the model was not accurately predicting the future values.\n\nIn the previous section \"ARIMA Model - Crude Oil Price by Month\", we obtained a fitted ARIMA model. The RMSE value of the final model obtained was approximately 0.1911. Considering the best deep learning model obtained was the GRU, we compare the approximate RMSE value of 0.4094. We can see the values are very different but these are very low. Therefore, we can conclude that the models are very similar and both have good predicion power.\n\nThe deep learning models are very different to the ARIMA models. Tuning the deep learning models can be harder and more computationally intensive. On the other hand, the ARIMA model may take more steps to obtain the final model, but the analysis that has to be done is very similar to other models.\n:::","srcMarkdownNoYaml":"\n\n```{css, echo = FALSE}\n.justify {\ntext-align: justify !important;\ntext-indent: 20px;\n}\n\n\n.epigrafe {\ntext-align: justify !important;\ntext-indent: 20px;\nborder: 1.5px solid #87c8b5;\npadding: 15px;\nfont-size: 14px;\nbackground-color: #f9f9f9;\nmargin: 20px 0px 30px 0px;\n}\n\nh2 {\n  font-size: 28px; \n  color: #FFFFFF; \n  background-color: #87c8b5;\n  padding: 10px; \n  border-radius: 8px;\n}\n```\n\n``` {r}\n#| echo: false\n#| message: false\n#| results: 'hide'\n#| warning: false\n\nx<-1\n\n```\n\n### Overview\n\n::: {.justify}\n\nThe goal of this section is to apply deep learning algorithms to the oil price monthly data in order to make predictions of what the price of oil will be in the future.\n\nThe libraries that will be used in this section are in the following list:\n:::\n\n``` {python}\n#| echo: true\n#| message: false\n#| code-fold: true\n#| code-summary: \"Libraries\"\n#| results: 'hide'\n#| warning: false\n\n# Import Data\nimport pandas as pd\n\n# Plotting\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# Keras\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom keras.layers import Dense, SimpleRNN, LSTM ,GRU\n```\n\n### Data\n\n::: {.epigrafe}\n\nFor this analysis we will use the monthly adjusted oil price for the last 22 years which we can observe in the next table. The will be later transformed to be saved as a np.array() with a similar format to the time series object used in past sections.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n# Read csv\ndf = pd.read_csv('./data/df_oil_price_monthly.csv')\n\ndf\n\ndf = df.rename(columns={\"date\": \"t\", \"adjusted\": \"y\"})\n\ndf = df[[\"t\",\"y\"]]\n\nprint(\"CHECK NA:\\n\",df.isna().sum())\n\nt=np.array([*range(0,df.shape[0])])\n\nx=np.array(df['y']).reshape(t.shape[0],1)\n\nfeature_columns=[0] # columns to use as features\n\ntarget_columns=[0]  # columns to use as targets\n```\n\n### Visualize Data\n\n::: {.epigrafe}\n\nWhen we plot the raw data, as we evaluated in previous sections, we can observe there is no clear trend over all the years and there is a lot of fluctuation.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o',alpha = 0.5)\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.show()\n```\n\n### Normalize Data\n\n::: {.epigrafe}\n\nIn order to apply the data points to the deep learning algorithms we transformed the data to be normalized. For this procedure we used the mean and standard deviation of the points. After normalizing the data, the output values are between -3 and 3.\n\nWe plotted the transformed data and we can observe that the shape of the time series is very similar.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\nprint(np.mean(x,axis=0).shape,np.std(x,axis=0).shape)\nx=(x-np.mean(x,axis=0))/np.std(x,axis=0)\nprint(x.shape)\n```\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n# visualize normalized data \nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o')\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.show()\n```\n\n### Split Data\n\n::: {.epigrafe}\n\nThe next step is to split the data into train and validation sets. To perform this split we use an 80% of the data points for the train set and a 20% of the data points for the validation set.\n\nIn the following plot we can observe the train set colored in orange and the validation set in blue.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\nsplit_fraction=0.8\ncut=int(split_fraction*x.shape[0]) \ntt=t[0:cut]; \nxt=x[0:cut]\ntv=t[cut:]; \nxv=x[cut:]\n```\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n# visualize normalized data \nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(tt, xt[:,i],'ro',alpha=0.25)\n    ax.plot(tt, xt[:,i],\"g-\")\nfor i in range(0,x.shape[1]):\n    ax.plot(tv, xv[:,i],'bo',alpha=0.25)\n    ax.plot(tv, xv[:,i],\"g-\")\nplt.show()\n```\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n    # verbose=True --> report and plot for debugging\n    # unique=True --> don't re-sample: \n    # x1,x2,x3 --> x4 then x4,x5,x6 --> x7 instead of x2,x3,x4 --> x5\n\n    # initialize \n    i_start=0; count=0; \n    \n    # initialize output arrays with samples \n    x_out=[]\n    y_out=[]\n    \n    # sequentially build mini-batch samples\n    while i_start+lookback+delay< x.shape[0]:\n        \n        # define index bounds\n        i_stop=i_start+lookback\n        i_pred=i_stop+delay\n        \n        # report if desired \n        if verbose and count<2: print(\"indice range:\",i_start,i_stop,\"-->\",i_pred)\n\n        # define arrays: \n        # method-1: buggy due to indexing from left \n        # numpy's slicing --> start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n        \n        # method-2: non-vectorized but cleaner\n        indices_to_keep=[]; j=i_stop\n        while  j>=i_start:\n            indices_to_keep.append(j)\n            j=j-step\n\n        # create mini-batch sample\n        xtmp=x[indices_to_keep,:]    # isolate relevant indices\n        xtmp=xtmp[:,feature_columns] # isolate desire features\n        ytmp=x[i_pred,target_columns]\n        x_out.append(xtmp); y_out.append(ytmp); \n        \n        # report if desired \n        if verbose and count<2: print(xtmp, \"-->\",ytmp)\n        if verbose and count<2: print(\"shape:\",xtmp.shape, \"-->\",ytmp.shape)\n\n        # PLOT FIRST SAMPLE IF DESIRED FOR DEBUGGING    \n        if verbose and count<2:\n            fig, ax = plt.subplots()\n            ax.plot(x,'b-')\n            ax.plot(x,'bx')\n            ax.plot(indices_to_keep,xtmp,'go')\n            ax.plot(i_pred*np.ones(len(target_columns)),ytmp,'ro')\n            plt.show()\n            \n        # UPDATE START POINT \n        if unique: i_start+=lookback \n        i_start+=1; count+=1\n        \n    return np.array(x_out),np.array(y_out)\n```\n\n### Form Mini Batches\n\n::: {.epigrafe}\n\nFor this approach we will use mini batches. This allows to look at an especific number of data points at the same time for every iteration. In this case, considering the large amount of data points, we selected a lookback value of 25.\n:::\n\n```{python}\n#| echo: true\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n# training\nL=25; S=1; D=1\nXt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n\n# validation\nXv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=False)\n```\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\nprint(\"training:\",Xt.shape,Yt.shape)\nprint(\"validation:\",Xv.shape,Yv.shape)\n```\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\nfrom sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\n\n# UTILITY FUNCTION\ndef regression_report(yt,ytp,yv,yvp):\n    print(\"---------- Regression report ----------\")\n    \n    print(\"TRAINING:\")\n    print(\" RMSE:\",mean_squared_error(yt,ytp)**(1/2))\n    print(\" MSE:\",mean_squared_error(yt,ytp))\n    print(\" MAE:\",mean_absolute_error(yt,ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt,ytp,'ro')\n    ax.plot(yt,yt,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Training data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot=1.0\n    upper=int(frac_plot*yt.shape[0]); \n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper],'b-')\n    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t (blue=actual & red=prediction)', title='Training: Time-series prediction')\n    plt.show()\n\n      \n    print(\"VALIDATION:\")\n    print(\" RMSE:\",mean_squared_error(yv,yvp)**(1/2))\n    print(\" MSE:\",mean_squared_error(yv,yvp))\n    print(\" MAE:\",mean_absolute_error(yv,yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT \n    fig, ax = plt.subplots()\n    ax.plot(yv,yvp,'ro')\n    ax.plot(yv,yv,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Validation data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper=int(frac_plot*yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper],'b-')\n    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n    plt.show()\n\n```\n\n<!-- ### Utility Function -->\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS \n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\", color=\"green\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\", color=\"orange\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n```\n\n### Keras - RNN\n\n::: {.epigrafe}\n\nThe first model we fitted is RNN using the functions from Keras in the tensorflow package. \n\nIn the **Training and validation loss** plot we can observe that the training errors / loss reduces when the number of Epochs increases. In the case of the validation errors, they fluctuate a lot. This suggests that the model is not performing correctly and is probably overfitting.\n\nWhen we compare the RMSE for the Training and Validation sets, these are not very different but it does not mean that the model is predicting correctly.\n\nWe can compare the Validation data plot and the traning data plot and clearly see that the training values correlate better with the line, while the validation values do not.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n#batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n```\n\n\n### Keras - GRU\n\n::: {.epigrafe}\n\nIn the case of the GRU model created from the Keras function from the Tensorflow package, we obtained very different results for each analysis.\n\nThe training and validation loss plot shows that both the training loss and validation loss decreases to very similar values as the number of Epochs increases.\n\nConsidering that the trend of the validation loss values is very similar to the training trend, we can conclude that the RMSE of both sets is very low. This suggests that the model has an optimal fit.\n\nThe validation data plot and predictions, compared to the training plots, these look very similar compared to the real data values.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n```\n\n### Keras - LSTM\n\n::: {.epigrafe}\n\nThe last model created was LSTM, also obtained from the Keras package within Tensorflow.\n\nBy assessing the traning and validation loss plot, we can observe that this model is a midpoint between the two previously assessed. There is a los of fluctuation of the loss values but the value is still low. If we see more in detail we can clearly see that the trend is increasing. This suggests that with an increasing number of Epochs, for the higher values, the loss value in high.\n\nWhen we compare the RMSE for the training set and the test set, we can see there is a high difference and the trend suggests it will increase.\n\nThe predictions observed in the Validation plots are kind of good, but these can change in the future forests. This model may tend to overfitting if we increase the number of points being forecasted.\n:::\n\n```{python}\n#| echo: false\n#| message: false\n#| warning: false\n#| code-fold: true\n#| code-summary: \"{python}\"\n\n\nprint(Xt.shape,\"-->\",Yt.shape)\nprint(Xv.shape,\"-->\",Yv.shape)\n\n# HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\nbatch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\n# batch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)\n```\n\n\n### Conclusions\n\n::: {.epigrafe}\n\nAs a conclusion, we need to evaluate the three models from different perspectives to undestand which model is performing better.\n\nWhen we compare the 3 models we can see that 2 of them are overfitting, RNN and LSTM, while the GRU is not. Therefore, from this point of view we conclude that the **train-test sets** are good for the model fitting.\n\nIn order to assess which model is better in terms of **predictive power,** we use the RMSE metric. The model that has the lower RMSE value is the GRU.\n\nWe modified the **regularization** parameter when training the models. This value was set by default in 0, which means there is no regularization at all. When we increase this value, the RMSE values are being higher compared to the default value. For that main reason, we decided to keep the regularization parameter as 0.\n\nTo assess how far into the future can the deep learning model accurately predict the future depends on the model that is being used. In terms of computationally intensive, with a lot of data points this predictions can take very long which that would not be very efficient. Also, as we have evaluated previously, for the LSTM model, the loss function, RMSE metric, was increasing over time, this suggests that the model was not accurately predicting the future values.\n\nIn the previous section \"ARIMA Model - Crude Oil Price by Month\", we obtained a fitted ARIMA model. The RMSE value of the final model obtained was approximately 0.1911. Considering the best deep learning model obtained was the GRU, we compare the approximate RMSE value of 0.4094. We can see the values are very different but these are very low. Therefore, we can conclude that the models are very similar and both have good predicion power.\n\nThe deep learning models are very different to the ARIMA models. Tuning the deep learning models can be harder and more computationally intensive. On the other hand, the ARIMA model may take more steps to obtain the final model, but the analysis that has to be done is very similar to other models.\n:::"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"embed-resources":true,"output-file":"10_Deep_Learning_TS.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"minty","title":"Deep Learning for TS"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}